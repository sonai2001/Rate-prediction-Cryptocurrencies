{"cells":[{"cell_type":"markdown","metadata":{"id":"9XEj2_DcSJ19"},"source":["## Import Library"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QTcH_x1fTByd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668518760684,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}},"outputId":"0b340795-8b4e-43f4-d943-7dbf32f3327e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}],"source":["%tensorflow_version 2.x\n","import json\n","import requests\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.metrics import mean_absolute_error , r2_score , mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"yTN5vSQ7UAds"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Kwm7_TrqT_SU","executionInfo":{"status":"ok","timestamp":1668518766432,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["endpoint = 'https://min-api.cryptocompare.com/data/histoday'\n","res = requests.get(endpoint + '?fsym=BTC&tsym=CAD&limit=1000')\n","hist = pd.DataFrame(json.loads(res.content)['Data'])\n","hist = hist.set_index('time')\n","hist.index = pd.to_datetime(hist.index, unit='s')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"oA5X0YowV3aS","outputId":"cbafdc63-fb82-4127-a542-05af42a4705c","executionInfo":{"status":"ok","timestamp":1667225615525,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sanjoy Sarkar","userId":"06752626805357965620"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                high       low      open  volumefrom    volumeto     close  \\\n","time                                                                         \n","2020-02-04  12299.80  12000.00  12234.30       29.93   363396.83  12164.70   \n","2020-02-05  12762.60  12110.20  12164.70       85.37  1068653.94  12635.70   \n","2020-02-06  12935.00  12477.40  12635.70       62.95   802245.42  12912.20   \n","2020-02-07  13000.00  12842.70  12912.20       54.37   703314.44  12994.50   \n","2020-02-08  13088.10  12759.60  12994.50       22.38   289608.42  13085.90   \n","...              ...       ...       ...         ...         ...       ...   \n","2022-10-27  28240.26  27423.92  28129.98      105.92  2958314.22  27529.62   \n","2022-10-28  28249.54  27268.07  27529.62       81.60  2263533.08  28091.08   \n","2022-10-29  28643.57  28026.38  28091.08      111.80  3168835.79  28327.15   \n","2022-10-30  28485.93  27959.49  28327.15       77.56  2189599.13  28086.61   \n","2022-10-31  28379.43  27750.15  28086.61       53.32  1497077.14  27771.58   \n","\n","           conversionType conversionSymbol  \n","time                                        \n","2020-02-04         direct                   \n","2020-02-05         direct                   \n","2020-02-06         direct                   \n","2020-02-07         direct                   \n","2020-02-08         direct                   \n","...                   ...              ...  \n","2022-10-27         direct                   \n","2022-10-28         direct                   \n","2022-10-29         direct                   \n","2022-10-30         direct                   \n","2022-10-31         direct                   \n","\n","[1001 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-28d2b222-a7d8-42ac-af14-0bd530e06a5f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>open</th>\n","      <th>volumefrom</th>\n","      <th>volumeto</th>\n","      <th>close</th>\n","      <th>conversionType</th>\n","      <th>conversionSymbol</th>\n","    </tr>\n","    <tr>\n","      <th>time</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2020-02-04</th>\n","      <td>12299.80</td>\n","      <td>12000.00</td>\n","      <td>12234.30</td>\n","      <td>29.93</td>\n","      <td>363396.83</td>\n","      <td>12164.70</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2020-02-05</th>\n","      <td>12762.60</td>\n","      <td>12110.20</td>\n","      <td>12164.70</td>\n","      <td>85.37</td>\n","      <td>1068653.94</td>\n","      <td>12635.70</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2020-02-06</th>\n","      <td>12935.00</td>\n","      <td>12477.40</td>\n","      <td>12635.70</td>\n","      <td>62.95</td>\n","      <td>802245.42</td>\n","      <td>12912.20</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2020-02-07</th>\n","      <td>13000.00</td>\n","      <td>12842.70</td>\n","      <td>12912.20</td>\n","      <td>54.37</td>\n","      <td>703314.44</td>\n","      <td>12994.50</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2020-02-08</th>\n","      <td>13088.10</td>\n","      <td>12759.60</td>\n","      <td>12994.50</td>\n","      <td>22.38</td>\n","      <td>289608.42</td>\n","      <td>13085.90</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2022-10-27</th>\n","      <td>28240.26</td>\n","      <td>27423.92</td>\n","      <td>28129.98</td>\n","      <td>105.92</td>\n","      <td>2958314.22</td>\n","      <td>27529.62</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2022-10-28</th>\n","      <td>28249.54</td>\n","      <td>27268.07</td>\n","      <td>27529.62</td>\n","      <td>81.60</td>\n","      <td>2263533.08</td>\n","      <td>28091.08</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2022-10-29</th>\n","      <td>28643.57</td>\n","      <td>28026.38</td>\n","      <td>28091.08</td>\n","      <td>111.80</td>\n","      <td>3168835.79</td>\n","      <td>28327.15</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2022-10-30</th>\n","      <td>28485.93</td>\n","      <td>27959.49</td>\n","      <td>28327.15</td>\n","      <td>77.56</td>\n","      <td>2189599.13</td>\n","      <td>28086.61</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2022-10-31</th>\n","      <td>28379.43</td>\n","      <td>27750.15</td>\n","      <td>28086.61</td>\n","      <td>53.32</td>\n","      <td>1497077.14</td>\n","      <td>27771.58</td>\n","      <td>direct</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1001 rows Ã— 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28d2b222-a7d8-42ac-af14-0bd530e06a5f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28d2b222-a7d8-42ac-af14-0bd530e06a5f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28d2b222-a7d8-42ac-af14-0bd530e06a5f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}],"source":["hist"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1qGCFwUWfzU","outputId":"cc6f2ee4-6153-47b3-c0b5-f0808cae77cb","executionInfo":{"status":"ok","timestamp":1668518777129,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","DatetimeIndex: 1001 entries, 2020-02-19 to 2022-11-15\n","Data columns (total 8 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   high              1001 non-null   float64\n"," 1   low               1001 non-null   float64\n"," 2   open              1001 non-null   float64\n"," 3   volumefrom        1001 non-null   float64\n"," 4   volumeto          1001 non-null   float64\n"," 5   close             1001 non-null   float64\n"," 6   conversionType    1001 non-null   object \n"," 7   conversionSymbol  1001 non-null   object \n","dtypes: float64(6), object(2)\n","memory usage: 70.4+ KB\n"]}],"source":["hist.info()"]},{"cell_type":"markdown","metadata":{"id":"0fwnghqSVy39"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"syCN598dXLcR"},"source":["Remove Unwanted coloums"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YYlMSBhOVTeC","executionInfo":{"status":"ok","timestamp":1668518784198,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["hist.drop([\"conversionType\", \"conversionSymbol\"], axis = 'columns', inplace = True)"]},{"cell_type":"markdown","metadata":{"id":"fwWT8enIaU5q"},"source":["check null values"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PMN8pzgbQ1y","outputId":"974213c6-2c20-41ba-c0af-61680ad1dc96","executionInfo":{"status":"ok","timestamp":1668518788476,"user_tz":-330,"elapsed":1168,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["high          0\n","low           0\n","open          0\n","volumefrom    0\n","volumeto      0\n","close         0\n","dtype: int64"]},"metadata":{},"execution_count":5}],"source":["hist.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"S8CHkik_bvEz"},"source":["Normalize The Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QbWevh5Kbzyl","executionInfo":{"status":"ok","timestamp":1668518793663,"user_tz":-330,"elapsed":1785,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","hist_columns = hist.columns\n","scaler = MinMaxScaler()\n","hist = scaler.fit_transform(hist)\n","hist = pd.DataFrame(hist)\n","hist.columns = hist_columns"]},{"cell_type":"markdown","metadata":{"id":"gk_cJEQicTln"},"source":["Describe the Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"nhHE6V3TcbvX","outputId":"578bad19-23c1-46ce-9625-96ce972b7d12","executionInfo":{"status":"ok","timestamp":1668518799068,"user_tz":-330,"elapsed":26,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              high          low         open   volumefrom     volumeto  \\\n","count  1001.000000  1001.000000  1001.000000  1001.000000  1001.000000   \n","mean      0.412818     0.416746     0.414608     0.063341     0.076320   \n","std       0.274984     0.264350     0.272952     0.122727     0.146978   \n","min       0.000000     0.000000     0.000000     0.000000     0.000000   \n","25%       0.128813     0.149060     0.134329     0.003096     0.001989   \n","50%       0.413179     0.412352     0.415406     0.005440     0.003712   \n","75%       0.638019     0.636080     0.636631     0.017527     0.013971   \n","max       1.000000     1.000000     1.000000     1.000000     1.000000   \n","\n","             close  \n","count  1001.000000  \n","mean      0.414715  \n","std       0.272829  \n","min       0.000000  \n","25%       0.135242  \n","50%       0.415406  \n","75%       0.636631  \n","max       1.000000  "],"text/html":["\n","  <div id=\"df-39e9714d-8388-4cb9-af4c-9415ec02346a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>open</th>\n","      <th>volumefrom</th>\n","      <th>volumeto</th>\n","      <th>close</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1001.000000</td>\n","      <td>1001.000000</td>\n","      <td>1001.000000</td>\n","      <td>1001.000000</td>\n","      <td>1001.000000</td>\n","      <td>1001.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.412818</td>\n","      <td>0.416746</td>\n","      <td>0.414608</td>\n","      <td>0.063341</td>\n","      <td>0.076320</td>\n","      <td>0.414715</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.274984</td>\n","      <td>0.264350</td>\n","      <td>0.272952</td>\n","      <td>0.122727</td>\n","      <td>0.146978</td>\n","      <td>0.272829</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.128813</td>\n","      <td>0.149060</td>\n","      <td>0.134329</td>\n","      <td>0.003096</td>\n","      <td>0.001989</td>\n","      <td>0.135242</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.413179</td>\n","      <td>0.412352</td>\n","      <td>0.415406</td>\n","      <td>0.005440</td>\n","      <td>0.003712</td>\n","      <td>0.415406</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.638019</td>\n","      <td>0.636080</td>\n","      <td>0.636631</td>\n","      <td>0.017527</td>\n","      <td>0.013971</td>\n","      <td>0.636631</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39e9714d-8388-4cb9-af4c-9415ec02346a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-39e9714d-8388-4cb9-af4c-9415ec02346a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-39e9714d-8388-4cb9-af4c-9415ec02346a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["hist.describe()"]},{"cell_type":"markdown","metadata":{"id":"tB2VeHLmf20i"},"source":["## Divided the Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9O1rUOgPgBEJ","outputId":"12d1394e-b06f-458f-a37d-f194c82f1c6f","executionInfo":{"status":"ok","timestamp":1668518811178,"user_tz":-330,"elapsed":1397,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["          high       low      open  volumefrom  volumeto\n","0     0.078469  0.089767  0.086043    0.005805  0.001636\n","1     0.069948  0.091426  0.076662    0.002381  0.000561\n","2     0.072518  0.093835  0.077077    0.002551  0.000623\n","3     0.070364  0.094276  0.078165    0.000445  0.000000\n","4     0.074626  0.094997  0.077618    0.001198  0.000231\n","...        ...       ...       ...         ...       ...\n","996   0.206387  0.211429  0.215571    0.007305  0.003775\n","997   0.193845  0.210670  0.204863    0.006213  0.003112\n","998   0.187130  0.199389  0.193818    0.005267  0.002565\n","999   0.195388  0.201231  0.191718    0.006347  0.003157\n","1000  0.191796  0.215130  0.198927    0.001528  0.000748\n","\n","[1001 rows x 5 columns]\n","0       0.076662\n","1       0.077077\n","2       0.078165\n","3       0.077618\n","4       0.082977\n","          ...   \n","996     0.204863\n","997     0.193818\n","998     0.191718\n","999     0.198927\n","1000    0.202047\n","Name: close, Length: 1001, dtype: float64\n"]}],"source":["X = hist.drop(columns = \"close\")\n","y = hist.close\n","print(X)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"XMWifZwQgbst"},"source":["Train_test_split"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MGEprdFOggPp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668518825574,"user_tz":-330,"elapsed":8450,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}},"outputId":"895ea4fe-f521-4dc5-f361-6f77f80ee651"},"outputs":[{"name":"stdout","output_type":"stream","text":["test size= 0.4\n"]}],"source":["from sklearn.model_selection import train_test_split\n","test_size = eval(input(\"test size= \"))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"RxtRiQu8hyGZ"},"source":["# Using Random forest Regressor Model"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Uwcc8Qrjq7I","outputId":"c882a863-3cd5-4bca-ec3c-8f07a5995ef0","executionInfo":{"status":"ok","timestamp":1668518833562,"user_tz":-330,"elapsed":2623,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestRegressor()"]},"metadata":{},"execution_count":10}],"source":["from sklearn.ensemble import RandomForestRegressor\n","RF = RandomForestRegressor()\n","RF.fit(X_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"69GSRmw7llWo"},"source":["predict from model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGDSm1wJlkW0","outputId":"7054d221-76d9-4dc0-ea37-7f49ac4b9ab8","executionInfo":{"status":"ok","timestamp":1668518837926,"user_tz":-330,"elapsed":541,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.46194242, 0.25637845, 0.64021369, 0.27383493, 0.85170561,\n","       0.68879523, 0.75012046, 0.42905574, 0.658755  , 0.07328275,\n","       0.27008186, 0.07595649, 0.55561398, 0.53534652, 0.25478761,\n","       0.30227897, 0.23838581, 0.30235127, 0.64383755, 0.43582569,\n","       0.72751158, 0.10985103, 0.71580893, 0.57980365, 0.09684857,\n","       0.10482863, 0.07989304, 0.27369991, 0.30207721, 0.65441803,\n","       0.47564208, 0.07667553, 0.9358529 , 0.45163674, 0.74030974,\n","       0.09003176, 0.85564128, 0.59653814, 0.25148355, 0.03984437,\n","       0.50317755, 0.07417141, 0.35676164, 0.250071  , 0.22866435,\n","       0.2224497 , 0.25323961, 0.81933444, 0.83913271, 0.26348914,\n","       0.60476762, 0.26742998, 0.1760128 , 0.75846086, 0.00629542,\n","       0.02664769, 0.90081431, 0.06366079, 0.09660703, 0.30498993,\n","       0.21507587, 0.03223876, 0.67352937, 0.09681484, 0.90327368,\n","       0.536764  , 0.39815847, 0.25405444, 0.05803916, 0.653254  ,\n","       0.08562188, 0.41393509, 0.69672416, 0.3983599 , 0.23339783,\n","       0.23690926, 0.81722633, 0.02395656, 0.68229585, 0.30204322,\n","       0.69350042, 0.04881853, 0.22052111, 0.05284725, 0.42612841,\n","       0.71842169, 0.19452535, 0.52360265, 0.55780986, 0.48712702,\n","       0.75020428, 0.68234776, 0.46910447, 0.09218302, 0.56078918,\n","       0.81245087, 0.96155645, 0.08840458, 0.03954568, 0.39793666,\n","       0.60624471, 0.53387176, 0.81358623, 0.15048137, 0.92001057,\n","       0.57646694, 0.70163303, 0.26982947, 0.11500387, 0.2652216 ,\n","       0.5733016 , 0.59394365, 0.27194879, 0.41261558, 0.08381795,\n","       0.75367613, 0.1154911 , 0.25939871, 0.58206618, 0.21535325,\n","       0.39920065, 0.84009375, 0.52985486, 0.15783385, 0.10544182,\n","       0.69495063, 0.60893491, 0.81816097, 0.25678473, 0.54270204,\n","       0.26778044, 0.24352244, 0.17230335, 0.20980558, 0.57414171,\n","       0.03718415, 0.07723355, 0.09619053, 0.00692747, 0.07377763,\n","       0.03723253, 0.13485072, 0.64041101, 0.20168103, 0.3012837 ,\n","       0.27029282, 0.44167145, 0.11482567, 0.6998585 , 0.44658396,\n","       0.84948708, 0.08449869, 0.66230694, 0.71729272, 0.25438183,\n","       0.53398656, 0.30193981, 0.58669398, 0.04311688, 0.6231141 ,\n","       0.80403902, 0.71694869, 0.86480708, 0.59747254, 0.70178857,\n","       0.7175353 , 0.22977281, 0.39976002, 0.35748285, 0.30856264,\n","       0.7176668 , 0.47872413, 0.08284606, 0.28808627, 0.48437589,\n","       0.71631132, 0.14310953, 0.14923062, 0.69981362, 0.72564921,\n","       0.4425773 , 0.9136793 , 0.70530676, 0.56033463, 0.63181675,\n","       0.07360596, 0.60950821, 0.78405593, 0.27253484, 0.56628137,\n","       0.24285018, 0.55927891, 0.59059933, 0.08784914, 0.09958999,\n","       0.87087344, 0.51893188, 0.09137173, 0.27482551, 0.08618338,\n","       0.02094852, 0.62141909, 0.61468651, 0.71779032, 0.85053847,\n","       0.80572673, 0.11154424, 0.26529872, 0.46177286, 0.40307738,\n","       0.04437152, 0.07524985, 0.25412788, 0.38926072, 0.72067141,\n","       0.52346375, 0.70979261, 0.13585067, 0.31188932, 0.24215492,\n","       0.28007248, 0.57361514, 0.24639346, 0.04261022, 0.27497278,\n","       0.07940302, 0.67556742, 0.51492294, 0.30591872, 0.63569064,\n","       0.44324755, 0.20466453, 0.53971122, 0.69774101, 0.02448516,\n","       0.02418786, 0.09654477, 0.41859741, 0.1114089 , 0.50152556,\n","       0.07863163, 0.08985582, 0.41949991, 0.41117996, 0.29823329,\n","       0.08229794, 0.72514875, 0.07982878, 0.07704695, 0.23815568,\n","       0.07060006, 0.90722955, 0.58235106, 0.55988165, 0.80188022,\n","       0.53357529, 0.44418977, 0.27506284, 0.2626581 , 0.0803258 ,\n","       0.55670276, 0.4771937 , 0.07421005, 0.17622383, 0.65638943,\n","       0.26184313, 0.2401541 , 0.71030561, 0.24887675, 0.71941553,\n","       0.43789301, 0.07975401, 0.41710982, 0.13584632, 0.54379822,\n","       0.06733259, 0.1111279 , 0.87238951, 0.0739791 , 0.57573819,\n","       0.59016985, 0.89926186, 0.10919922, 0.22773093, 0.09810823,\n","       0.60742052, 0.69575542, 0.71290153, 0.09489782, 0.26547103,\n","       0.24873935, 0.57324858, 0.46936428, 0.50157369, 0.70830915,\n","       0.48461959, 0.28725182, 0.77867925, 0.06997187, 0.08006377,\n","       0.6602373 , 0.80470131, 0.55502686, 0.93638121, 0.25485437,\n","       0.24864942, 0.51859031, 0.55300504, 0.27514802, 0.66492502,\n","       0.78186116, 0.55672511, 0.63718645, 0.50791816, 0.76217212,\n","       0.59265794, 0.09590039, 0.5365421 , 0.61059318, 0.09983852,\n","       0.09473389, 0.07473526, 0.27831429, 0.97149797, 0.61882121,\n","       0.45641762, 0.4326787 , 0.5045527 , 0.70632226, 0.07396587,\n","       0.4926978 , 0.3182047 , 0.63259733, 0.96517013, 0.07627145,\n","       0.39095351, 0.52830765, 0.66381489, 0.25370907, 0.54229515,\n","       0.50026657, 0.84741744, 0.11917152, 0.08128518, 0.85104374,\n","       0.93654577, 0.70519768, 0.30836103, 0.08730712, 0.24938918,\n","       0.12185384, 0.21950179, 0.30205396, 0.0539076 , 0.23066469,\n","       0.07404845, 0.23334704, 0.81959755, 0.52961905, 0.0743046 ,\n","       0.90708521, 0.60985656, 0.07373967, 0.7153186 , 0.1153248 ,\n","       0.00686912, 0.30754961, 0.11247543, 0.85663558, 0.87671315,\n","       0.11870965, 0.5075472 , 0.0784807 , 0.11923216, 0.07373885,\n","       0.39752209, 0.48783701, 0.07747189, 0.67644367, 0.25479634,\n","       0.10672211, 0.07816324, 0.57116804, 0.1021255 , 0.09762559,\n","       0.25713579, 0.45582027, 0.07398874, 0.28646436, 0.25607242,\n","       0.11479367, 0.03446983, 0.9178441 , 0.08371185, 0.61342131,\n","       0.03534295, 0.86420052, 0.8223041 , 0.58388644, 0.24678756,\n","       0.01051025])"]},"metadata":{},"execution_count":11}],"source":["rf_y_pred = RF.predict(X_test)\n","rf_y_pred"]},{"cell_type":"markdown","metadata":{"id":"7kjzsd8Cl6yb"},"source":["##Evalute the Model\n","\n","\n","1.   Mean Absolute error\n","2.   Mean Squared error\n","3.   r2 score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6fIVncLmjxn","outputId":"e937bea0-861c-4de3-f4b7-ea2f0ca5f52a"},"outputs":[{"name":"stdout","output_type":"stream","text":["mean absolute error value is =  0.007213494848629944\n","mean squared error value is =  0.0001244783578933231\n","R2 score value is =  0.9985874638969592\n"]}],"source":["MAE = mean_absolute_error(rf_y_pred, y_test)\n","MSE=mean_squared_error(rf_y_pred, y_test)\n","R2=r2_score(rf_y_pred, y_test)\n","print(\"mean absolute error value is = \",MAE)\n","print(\"mean squared error value is = \",MSE)\n","print(\"R2 score value is = \",R2)"]},{"cell_type":"markdown","metadata":{"id":"SauzpHkIm6vx"},"source":["#GridSearchcv of Rfregressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDCXC3XknP9m"},"outputs":[],"source":["param_grid = {\n","    'bootstrap': [True],\n","    'max_depth': [80, 90, 100, 110],\n","    'max_features': [2, 3],\n","    'min_samples_leaf': [3, 4, 5],\n","    'min_samples_split': [8, 10, 12],\n","    'n_estimators': [100, 200, 300, 1000]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4JdxMGDnYWd"},"outputs":[],"source":["RF_grid_search = GridSearchCV(estimator = RF, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"]},{"cell_type":"markdown","metadata":{"id":"fsm5qOcinr5z"},"source":["with train part of dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsczOuernykY","outputId":"e719e32b-4026-457e-ce96-b76d59e3831c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"]},{"data":{"text/plain":["GridSearchCV(cv=3, estimator=RandomForestRegressor(), n_jobs=-1,\n","             param_grid={'bootstrap': [True], 'max_depth': [80, 90, 100, 110],\n","                         'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5],\n","                         'min_samples_split': [8, 10, 12],\n","                         'n_estimators': [100, 200, 300, 1000]},\n","             verbose=2)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["RF_grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2ziZn21oTm3","outputId":"437ade7f-383b-4f2e-d3cf-7ebc22f1b5f3"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best estimator across ALL searched params:\n"," RandomForestRegressor(max_depth=90, max_features=3, min_samples_leaf=3,\n","                      min_samples_split=8, n_estimators=300)\n","\n"," The best score across ALL searched params:\n"," 0.9981244787253604\n","\n"," The best parameters across ALL searched params:\n"," {'bootstrap': True, 'max_depth': 90, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 300}\n"]}],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",RF_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",RF_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",RF_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"lExBj4OvsD_F"},"source":["With test part of Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmxku8EusIPR","outputId":"f8438c79-4638-4662-f42a-f75403ee790e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"]},{"data":{"text/plain":["GridSearchCV(cv=3, estimator=RandomForestRegressor(), n_jobs=-1,\n","             param_grid={'bootstrap': [True], 'max_depth': [80, 90, 100, 110],\n","                         'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5],\n","                         'min_samples_split': [8, 10, 12],\n","                         'n_estimators': [100, 200, 300, 1000]},\n","             verbose=2)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["RF_grid_search.fit(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I69eqM_4sPDv","outputId":"957d726e-c60a-4c47-d566-f1aa20c0412b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best estimator across ALL searched params:\n"," RandomForestRegressor(max_depth=100, max_features=3, min_samples_leaf=3,\n","                      min_samples_split=8)\n","\n"," The best score across ALL searched params:\n"," 0.997140403992587\n","\n"," The best parameters across ALL searched params:\n"," {'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 100}\n"]}],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",RF_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",RF_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",RF_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"e4Wi2e2fuvJN"},"source":["#Using Linear regression Lasso Part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6OSyKSUu1-i","outputId":"f3f1f0c1-ef09-49ad-96b5-a44f37195ae8"},"outputs":[{"data":{"text/plain":["Lasso()"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import linear_model\n","LR = linear_model.Lasso()\n","LR.fit(X_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"wj6xupU7vhyl"},"source":["Predict from the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNS5-g9KvlVJ","outputId":"f58c3f55-77f4-4746-9337-ed810d012a6d"},"outputs":[{"data":{"text/plain":["array([0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806, 0.39529806, 0.39529806, 0.39529806, 0.39529806,\n","       0.39529806])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["LR_y_Pred = LR.predict(X_test)\n","LR_y_Pred"]},{"cell_type":"markdown","metadata":{"id":"maZ9R2xOvyUF"},"source":["##Evalute the Model\n","\n","\n","1.   Mean Absolute error\n","2.   Mean Squared error\n","3.   r2 score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IbMF-rtv53R","outputId":"00167ea2-03df-48a1-e0bf-9f8fea055566"},"outputs":[{"name":"stdout","output_type":"stream","text":["mean absolute error value is =  0.27149388601393387\n","mean squared error value is =  0.08899932101314266\n","R2 score value is =  -2.888193093176708e+31\n"]}],"source":["MAE = mean_absolute_error(LR_y_Pred, y_test)\n","MSE=mean_squared_error(LR_y_Pred, y_test)\n","R2=r2_score(LR_y_Pred, y_test)\n","print(\"mean absolute error value is = \",MAE)\n","print(\"mean squared error value is = \",MSE)\n","print(\"R2 score value is = \",R2)"]},{"cell_type":"markdown","metadata":{"id":"KeJfpjpGw2dl"},"source":["# Gridsearch of Lasso"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-0AQejhyD2K"},"outputs":[],"source":["parameters = {\n","    \"alpha\" : [0,0.1,0.01,0.05,0.5,1],\n","    \"normalize\" : [True, False],\n","    \"selection\" : [\"cyclic\", \"random\"]\n","    \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74syN69qyiqO"},"outputs":[],"source":["LR_grid_search = GridSearchCV(LR, parameters, cv = 5)"]},{"cell_type":"markdown","metadata":{"id":"T2GDHGAjzVF9"},"source":["with train part of dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2SrsEkkzW_A","outputId":"1ae13f88-c6da-438d-8839-f1a1771a39e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.359e-02, tolerance: 4.854e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e-02, tolerance: 4.871e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.564e-02, tolerance: 4.839e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e-02, tolerance: 4.829e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.438e-02, tolerance: 4.795e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.734e-02, tolerance: 4.854e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e-02, tolerance: 4.871e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.621e-02, tolerance: 4.839e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e-02, tolerance: 4.829e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.450e-02, tolerance: 4.795e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.359e-02, tolerance: 4.854e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e-02, tolerance: 4.871e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.564e-02, tolerance: 4.839e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e-02, tolerance: 4.829e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.438e-02, tolerance: 4.795e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.398e-02, tolerance: 4.854e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.957e-02, tolerance: 4.871e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e-02, tolerance: 4.839e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e-02, tolerance: 4.829e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e-02, tolerance: 4.795e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:926: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  self.best_estimator_.fit(X, y, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.091e-02, tolerance: 6.052e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"]},{"data":{"text/plain":["GridSearchCV(cv=5, estimator=Lasso(),\n","             param_grid={'alpha': [0, 0.1, 0.01, 0.05, 0.5, 1],\n","                         'normalize': [True, False],\n","                         'selection': ['cyclic', 'random']})"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["LR_grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqBvDFzQ1dgV","outputId":"6b8c7338-0c4d-49b8-ff3d-a94debbd1192"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best estimator across ALL searched params:\n"," Lasso(alpha=0, normalize=True)\n","\n"," The best score across ALL searched params:\n"," 0.9989083242326222\n","\n"," The best parameters across ALL searched params:\n"," {'alpha': 0, 'normalize': True, 'selection': 'cyclic'}\n"]}],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",LR_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",LR_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",LR_grid_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrz7cgAu1vGR","outputId":"f76f8fbd-f2ee-4b1b-c842-3f6693c63df4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.453e-03, tolerance: 2.149e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.761e-03, tolerance: 2.081e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.772e-03, tolerance: 2.169e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.782e-03, tolerance: 2.143e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.321e-03, tolerance: 2.179e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.791e-03, tolerance: 2.149e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e-02, tolerance: 2.081e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e-02, tolerance: 2.169e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e-02, tolerance: 2.143e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.573e-03, tolerance: 2.179e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.453e-03, tolerance: 2.149e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.761e-03, tolerance: 2.081e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.772e-03, tolerance: 2.169e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.782e-03, tolerance: 2.143e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.321e-03, tolerance: 2.179e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.316e-02, tolerance: 2.149e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e-02, tolerance: 2.081e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.080e-02, tolerance: 2.169e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.041e-02, tolerance: 2.143e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.545e-03, tolerance: 2.179e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:926: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n","  self.best_estimator_.fit(X, y, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e-02, tolerance: 2.681e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"]},{"data":{"text/plain":["GridSearchCV(cv=5, estimator=Lasso(),\n","             param_grid={'alpha': [0, 0.1, 0.01, 0.05, 0.5, 1],\n","                         'normalize': [True, False],\n","                         'selection': ['cyclic', 'random']})"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["LR_grid_search.fit(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zqgl75NM12Vc","outputId":"17e72dcd-a724-44eb-da50-485296559dc1"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best estimator across ALL searched params:\n"," Lasso(alpha=0, normalize=True)\n","\n"," The best score across ALL searched params:\n"," 0.9990399738094506\n","\n"," The best parameters across ALL searched params:\n"," {'alpha': 0, 'normalize': True, 'selection': 'cyclic'}\n"]}],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",LR_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",LR_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",LR_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"1v_N_MBcnZba"},"source":["#Using Gradientboost Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QScayilZnicb","outputId":"a9f3fa97-fb43-4215-8b7d-123b258568bf"},"outputs":[{"data":{"text/plain":["GradientBoostingRegressor()"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import GradientBoostingRegressor\n","GB = GradientBoostingRegressor()\n","GB.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"ZmbJEg06oBbV"},"source":["predict the values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koFP_EmboEi_","outputId":"c5984aac-1bd6-4653-8695-d6a5aa7d4eb1"},"outputs":[{"data":{"text/plain":["array([0.83987579, 0.45089069, 0.9515632 , 0.25907537, 0.23450454,\n","       0.71666034, 0.66901579, 0.7475785 , 0.85330052, 0.08248817,\n","       0.58975217, 0.03578433, 0.93794473, 0.75788472, 0.49271482,\n","       0.67142835, 0.07373334, 0.55185235, 0.82126517, 0.11107752,\n","       0.88245757, 0.03823898, 0.10806232, 0.85070253, 0.09607396,\n","       0.08625846, 0.05705371, 0.26751685, 0.6408381 , 0.92308563,\n","       0.10056892, 0.06389244, 0.4422714 , 0.85364307, 0.10918448,\n","       0.04801879, 0.46595751, 0.60103569, 0.55591528, 0.04254551,\n","       0.10399153, 0.0578033 , 0.11966794, 0.36211052, 0.11309792,\n","       0.07530285, 0.55998713, 0.51340921, 0.3123229 , 0.39976313,\n","       0.72837137, 0.59490502, 0.07735006, 0.55402112, 0.069897  ,\n","       0.06417247, 0.47961603, 0.04801879, 0.08873726, 0.59898492,\n","       0.11349038, 0.03823898, 0.85135967, 0.0877811 , 0.45410931,\n","       0.86935594, 0.5023527 , 0.52727027, 0.03578433, 0.8515265 ,\n","       0.05373076, 0.69488188, 0.50872049, 0.58187656, 0.10806232,\n","       0.08435488, 0.46902033, 0.04793388, 0.4726798 , 0.11219968,\n","       0.73631077, 0.03578433, 0.07545993, 0.03108015, 0.11977987,\n","       0.86730423, 0.25464803, 0.61658821, 0.92497648, 0.11107752,\n","       0.13251875, 0.40281138, 0.82163576, 0.08435488, 0.74663337,\n","       0.29139498, 0.45494508, 0.04793388, 0.03578433, 0.60924979,\n","       0.63261976, 0.0969604 , 0.51448454, 0.082655  , 0.45166559,\n","       0.68857458, 0.89850175, 0.39680302, 0.03416628, 0.63932738,\n","       0.83318346, 0.95492111, 0.25907537, 0.65297059, 0.03578433,\n","       0.25442624, 0.0427642 , 0.28372617, 0.81539834, 0.10806232,\n","       0.5402709 , 0.21383479, 0.63382589, 0.07505172, 0.08353742,\n","       0.76701941, 0.59219975, 0.36211052, 0.55670098, 0.68554571,\n","       0.5323028 , 0.62883689, 0.07552897, 0.2349064 , 0.79070665,\n","       0.03591465, 0.0827781 , 0.07445257, 0.069897  , 0.03591465,\n","       0.03591465, 0.07545993, 0.69861068, 0.07406938, 0.59898492,\n","       0.57000259, 0.68857458, 0.07749979, 0.82994699, 0.10806232,\n","       0.42537706, 0.00881097, 0.85592544, 0.2321885 , 0.44860769,\n","       0.52959328, 0.11478256, 0.90622177, 0.03591465, 0.93146539,\n","       0.4646601 , 0.56302098, 0.43529658, 0.72103566, 0.5255894 ,\n","       0.65700768, 0.10918448, 0.60991803, 0.11388432, 0.11461573,\n","       0.09798341, 0.460619  , 0.0698121 , 0.26791871, 0.62908939,\n","       0.0969604 , 0.07530285, 0.07530285, 0.79389457, 0.66555359,\n","       0.70241085, 0.46222611, 0.13251875, 0.87175589, 0.82082201,\n","       0.07538776, 0.10125013, 0.10466978, 0.55998713, 0.8985361 ,\n","       0.4070502 , 0.68612588, 0.70419628, 0.05364585, 0.07207746,\n","       0.22121018, 0.08857043, 0.0815666 , 0.24845215, 0.03578433,\n","       0.06012782, 0.83826191, 0.75941404, 0.83042126, 0.45074134,\n","       0.45426213, 0.03591465, 0.55998713, 0.74991041, 0.60895987,\n","       0.03578433, 0.06012782, 0.55998713, 0.61227017, 0.5681558 ,\n","       0.08873726, 0.59512668, 0.08045806, 0.63587535, 0.37606218,\n","       0.11197576, 0.82838524, 0.30575253, 0.04353406, 0.52220175,\n","       0.05373076, 0.85042378, 0.0969604 , 0.67060983, 0.86708953,\n","       0.10154006, 0.23825083, 0.5187234 , 0.80978422, 0.06012782,\n","       0.06417247, 0.082655  , 0.68433289, 0.0424606 , 0.08873726,\n","       0.05373076, 0.07670934, 0.74787428, 0.60924979, 0.10918448,\n","       0.06408756, 0.22890456, 0.03578433, 0.03416628, 0.08045806,\n","       0.03578433, 0.5289273 , 0.72199712, 0.89442694, 0.17898459,\n","       0.10929641, 0.8676081 , 0.55998713, 0.54089205, 0.04801879,\n","       0.82648795, 0.51259957, 0.0569688 , 0.07373334, 0.09214745,\n","       0.55756865, 0.42304771, 0.13251875, 0.40734013, 0.13251875,\n","       0.70055073, 0.07545993, 0.67267353, 0.07568605, 0.83318346,\n","       0.04801879, 0.04452575, 0.46902033, 0.03578433, 0.83147617,\n","       0.51281298, 0.41503873, 0.08435488, 0.10077302, 0.08873726,\n","       0.599879  , 0.09736226, 0.4172817 , 0.07373334, 0.60924979,\n","       0.41850223, 0.72145428, 0.72676367, 0.49808153, 0.09282866,\n","       0.46047828, 0.55814792, 0.22161203, 0.04801879, 0.00755809,\n","       0.85396283, 0.3123229 , 0.538568  , 0.2349064 , 0.6408381 ,\n","       0.40135412, 0.08873726, 0.73534931, 0.53974493, 0.82039453,\n","       0.49133155, 0.83319238, 0.6839563 , 0.70275056, 0.38731599,\n","       0.45341971, 0.07373334, 0.0969604 , 0.88350325, 0.0760369 ,\n","       0.03842399, 0.069897  , 0.11648517, 0.45006701, 0.82443493,\n","       0.65782142, 0.1011382 , 0.55551342, 0.58157932, 0.08051735,\n","       0.67331894, 0.11966794, 0.95613758, 0.45630742, 0.0578033 ,\n","       0.11309792, 0.69534956, 0.88130508, 0.6408381 , 0.78944941,\n","       0.48334644, 0.21038982, 0.03340303, 0.08873726, 0.17060394,\n","       0.23450454, 0.92220331, 0.6600585 , 0.04793388, 0.49288165,\n","       0.04793388, 0.07545993, 0.6385012 , 0.03591465, 0.10736964,\n","       0.07838222, 0.11966794, 0.22850271, 0.11219968, 0.06012782,\n","       0.23450454, 0.70345807, 0.08080727, 0.43588508, 0.02735509,\n","       0.06887242, 0.61851225, 0.04103428, 0.15068395, 0.45537125,\n","       0.03108015, 0.69788999, 0.06867731, 0.03416628, 0.04801879,\n","       0.51638497, 0.460619  , 0.0424606 , 0.71522232, 0.39753163,\n","       0.08873726, 0.06012782, 0.74090011, 0.08353742, 0.08873726,\n","       0.55969721, 0.7555943 , 0.07662443, 0.25907537, 0.58187656,\n","       0.03780857, 0.04353406, 0.51764623, 0.06012782, 0.49940574,\n","       0.04103428, 0.22121018, 0.34479881, 0.90313188, 0.56920015,\n","       0.07287621])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["GB_y_pred = GB.predict(X_test)\n","GB_y_pred"]},{"cell_type":"markdown","metadata":{"id":"llgXWROgonsx"},"source":["##Evalute the Model\n","\n","\n","1.   Mean Absolute error\n","2.   Mean Squared error\n","3.   r2 score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2xtgy6Mom8v","outputId":"78147d79-aa17-4a22-9b0b-8c8670c86548"},"outputs":[{"name":"stdout","output_type":"stream","text":["mean absolute error value is =  0.00783163138563432\n","mean squared error value is =  0.00013900348501789565\n","R2 score value is =  0.9984211551860152\n"]}],"source":["MAE = mean_absolute_error(GB_y_pred, y_test)\n","MSE=mean_squared_error(GB_y_pred, y_test)\n","R2=r2_score(GB_y_pred, y_test)\n","print(\"mean absolute error value is = \",MAE)\n","print(\"mean squared error value is = \",MSE)\n","print(\"R2 score value is = \",R2)"]},{"cell_type":"markdown","metadata":{"id":"dHRc6DDto3Hd"},"source":["#Gridsearch of Gradient Boost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qe3_Tnpvo9Lh"},"outputs":[],"source":["parameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n","                  'subsample'    : [0.9, 0.5, 0.2, 0.1],\n","                  'n_estimators' : [100,500,1000, 1500],\n","                  'max_depth'    : [4,6,8,10]\n","                 }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiCU3PNApNC4"},"outputs":[],"source":["GB_grid_search = GridSearchCV(estimator=GB, param_grid = parameters, cv = 3, n_jobs=-1)"]},{"cell_type":"markdown","metadata":{"id":"Yky9YZ_Nposg"},"source":["with train part of dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"X-Iiq8o3pwD-","outputId":"9ebfdb4e-f378-4ca8-c57d-5e1693e9ae4d"},"outputs":[{"data":{"text/plain":["GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n","             param_grid={'learning_rate': [0.01, 0.02, 0.03, 0.04],\n","                         'max_depth': [4, 6, 8, 10],\n","                         'n_estimators': [100, 500, 1000, 1500],\n","                         'subsample': [0.9, 0.5, 0.2, 0.1]})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["GB_grid_search.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vmm3HkXQqubO"},"outputs":[],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",GB_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",GB_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",GB_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"Naa8zkldq7FZ"},"source":["With the test part "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEt9X7tKrBn_","outputId":"8848ed04-d999-41cc-cacf-85e045e06ad5"},"outputs":[{"data":{"text/plain":["GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n","             param_grid={'learning_rate': [0.01, 0.02, 0.03, 0.04],\n","                         'max_depth': [4, 6, 8, 10],\n","                         'n_estimators': [100, 500, 1000, 1500],\n","                         'subsample': [0.9, 0.5, 0.2, 0.1]})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["GB_grid_search.fit(X_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d71-81twrK5z","outputId":"f7808648-5686-450e-a2ba-dfaa4ac8bb99"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best estimator across ALL searched params:\n"," GradientBoostingRegressor(learning_rate=0.02, max_depth=8, n_estimators=1000,\n","                          subsample=0.5)\n","\n"," The best score across ALL searched params:\n"," 0.9977618302279385\n","\n"," The best parameters across ALL searched params:\n"," {'learning_rate': 0.02, 'max_depth': 8, 'n_estimators': 1000, 'subsample': 0.5}\n"]}],"source":["print(\" Results from Grid Search \" )\n","print(\"\\n The best estimator across ALL searched params:\\n\",GB_grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",GB_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",GB_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"Uxnfdx0ctHgc"},"source":["#Using LSTM model"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XX_xmnt_tX92","executionInfo":{"status":"ok","timestamp":1668518852469,"user_tz":-330,"elapsed":2321,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Activation, Dense, Dropout, LSTM"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzhFUwwCtsVj","outputId":"9d9765d0-d502-4208-caad-1c3d69314fe6","executionInfo":{"status":"ok","timestamp":1668518870764,"user_tz":-330,"elapsed":14325,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","19/19 [==============================] - 6s 13ms/step - loss: 0.1360\n","Epoch 2/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0381\n","Epoch 3/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0122\n","Epoch 4/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0055\n","Epoch 5/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0050\n","Epoch 6/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0040\n","Epoch 7/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0039\n","Epoch 8/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0036\n","Epoch 9/30\n","19/19 [==============================] - 0s 11ms/step - loss: 0.0036\n","Epoch 10/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0032\n","Epoch 11/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0029\n","Epoch 12/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0030\n","Epoch 13/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0026\n","Epoch 14/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0026\n","Epoch 15/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0028\n","Epoch 16/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0025\n","Epoch 17/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0027\n","Epoch 18/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0023\n","Epoch 19/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0025\n","Epoch 20/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0025\n","Epoch 21/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0025\n","Epoch 22/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0026\n","Epoch 23/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0023\n","Epoch 24/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0021\n","Epoch 25/30\n","19/19 [==============================] - 0s 13ms/step - loss: 0.0021\n","Epoch 26/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0022\n","Epoch 27/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0019\n","Epoch 28/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0022\n","Epoch 29/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0022\n","Epoch 30/30\n","19/19 [==============================] - 0s 12ms/step - loss: 0.0020\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f11ed98c050>"]},"metadata":{},"execution_count":13}],"source":["# The LSTM architecture\n","regressor = Sequential()\n","# First LSTM layer with Dropout regularisation\n","regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n","regressor.add(Dropout(0.2))\n","# Second LSTM layer\n","regressor.add(LSTM(units=50, return_sequences=True))\n","regressor.add(Dropout(0.2))\n","# Third LSTM layer\n","regressor.add(LSTM(units=50, return_sequences=True))\n","regressor.add(Dropout(0.2))\n","# Fourth LSTM layer\n","regressor.add(LSTM(units=50))\n","regressor.add(Dropout(0.2))\n","# The output layer\n","regressor.add(Dense(units=1))\n","\n","# Compiling the RNN\n","regressor.compile(optimizer='adam',loss='mean_squared_error')\n","# Fitting to the training set\n","regressor.fit(X_train,y_train,epochs=30,batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"jEiX51oBtyl9"},"source":["predict the values"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JYHIxOCt1w0","outputId":"f3d775c6-4fe1-4fa0-db51-7d86a44a06aa","executionInfo":{"status":"ok","timestamp":1668518884580,"user_tz":-330,"elapsed":912,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["13/13 [==============================] - 3s 7ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.5047215 ],\n","       [0.263136  ],\n","       [0.65032053],\n","       [0.28190738],\n","       [0.86534375],\n","       [0.73001903],\n","       [0.77308947],\n","       [0.47881988],\n","       [0.681001  ],\n","       [0.08362322],\n","       [0.28887305],\n","       [0.0875506 ],\n","       [0.56698143],\n","       [0.5629197 ],\n","       [0.26229483],\n","       [0.30853093],\n","       [0.24355632],\n","       [0.30955988],\n","       [0.6972864 ],\n","       [0.4339044 ],\n","       [0.7620395 ],\n","       [0.12160921],\n","       [0.73589206],\n","       [0.6205299 ],\n","       [0.10918671],\n","       [0.11462279],\n","       [0.09296147],\n","       [0.2818298 ],\n","       [0.31305674],\n","       [0.6668853 ],\n","       [0.49300003],\n","       [0.08786376],\n","       [0.92862797],\n","       [0.492146  ],\n","       [0.76259583],\n","       [0.10342421],\n","       [0.8740099 ],\n","       [0.65591764],\n","       [0.25461802],\n","       [0.05524725],\n","       [0.5085626 ],\n","       [0.08427741],\n","       [0.37717575],\n","       [0.25850302],\n","       [0.23795213],\n","       [0.232187  ],\n","       [0.26030436],\n","       [0.854224  ],\n","       [0.8405233 ],\n","       [0.26980618],\n","       [0.619     ],\n","       [0.28602576],\n","       [0.18601644],\n","       [0.78337616],\n","       [0.02186428],\n","       [0.04471581],\n","       [0.9177937 ],\n","       [0.07417562],\n","       [0.10689999],\n","       [0.31256834],\n","       [0.21960042],\n","       [0.06577092],\n","       [0.71293   ],\n","       [0.10762152],\n","       [0.92321056],\n","       [0.54720193],\n","       [0.4076771 ],\n","       [0.25987682],\n","       [0.07576966],\n","       [0.701319  ],\n","       [0.09758608],\n","       [0.46213767],\n","       [0.7359099 ],\n","       [0.4102869 ],\n","       [0.24042498],\n","       [0.24127008],\n","       [0.8581036 ],\n","       [0.03774036],\n","       [0.7121837 ],\n","       [0.3086312 ],\n","       [0.7218137 ],\n","       [0.07515939],\n","       [0.23064199],\n","       [0.06425348],\n","       [0.42016667],\n","       [0.74325734],\n","       [0.2048992 ],\n","       [0.5330011 ],\n","       [0.57044095],\n","       [0.50606465],\n","       [0.75072014],\n","       [0.70036954],\n","       [0.51048017],\n","       [0.10399692],\n","       [0.5828558 ],\n","       [0.81078696],\n","       [0.96613127],\n","       [0.09904778],\n","       [0.05382325],\n","       [0.40852675],\n","       [0.614387  ],\n","       [0.54771507],\n","       [0.8552271 ],\n","       [0.15872066],\n","       [0.928637  ],\n","       [0.586099  ],\n","       [0.7371737 ],\n","       [0.2770358 ],\n","       [0.12291206],\n","       [0.27761063],\n","       [0.62187624],\n","       [0.6160505 ],\n","       [0.2813063 ],\n","       [0.4340265 ],\n","       [0.09910411],\n","       [0.7624439 ],\n","       [0.12388648],\n","       [0.27034998],\n","       [0.62261397],\n","       [0.225106  ],\n","       [0.41424516],\n","       [0.8358813 ],\n","       [0.5533819 ],\n","       [0.17498527],\n","       [0.11671603],\n","       [0.7202729 ],\n","       [0.6543374 ],\n","       [0.8353977 ],\n","       [0.26211947],\n","       [0.5645304 ],\n","       [0.2886318 ],\n","       [0.25242212],\n","       [0.17917512],\n","       [0.21777624],\n","       [0.615361  ],\n","       [0.04971422],\n","       [0.08809403],\n","       [0.10789384],\n","       [0.0255255 ],\n","       [0.0845093 ],\n","       [0.05087755],\n","       [0.14172064],\n","       [0.6620445 ],\n","       [0.22537535],\n","       [0.30831605],\n","       [0.27972803],\n","       [0.45223382],\n","       [0.12398133],\n","       [0.7291778 ],\n","       [0.448407  ],\n","       [0.86947024],\n","       [0.09652224],\n","       [0.692413  ],\n","       [0.7461231 ],\n","       [0.26144227],\n","       [0.56609833],\n","       [0.3112833 ],\n","       [0.6071597 ],\n","       [0.07181709],\n","       [0.63553756],\n","       [0.8129621 ],\n","       [0.7348372 ],\n","       [0.88594973],\n","       [0.6256697 ],\n","       [0.72312343],\n","       [0.7349352 ],\n","       [0.23704052],\n","       [0.42040378],\n","       [0.36973473],\n","       [0.3248675 ],\n","       [0.72656494],\n","       [0.537163  ],\n","       [0.09607255],\n","       [0.2924161 ],\n","       [0.5715176 ],\n","       [0.7292176 ],\n","       [0.1533085 ],\n","       [0.1588328 ],\n","       [0.7349835 ],\n","       [0.76442784],\n","       [0.48598734],\n","       [0.926901  ],\n","       [0.7326579 ],\n","       [0.6244674 ],\n","       [0.6560161 ],\n","       [0.08410585],\n","       [0.6316003 ],\n","       [0.7820065 ],\n","       [0.28180924],\n","       [0.60645294],\n","       [0.2537628 ],\n","       [0.5707055 ],\n","       [0.64404565],\n","       [0.1000326 ],\n","       [0.11110125],\n","       [0.8797448 ],\n","       [0.5363699 ],\n","       [0.10298584],\n","       [0.28498405],\n","       [0.09894796],\n","       [0.03777954],\n","       [0.6534249 ],\n","       [0.63127136],\n","       [0.74291533],\n","       [0.8612551 ],\n","       [0.8325387 ],\n","       [0.1267255 ],\n","       [0.2774951 ],\n","       [0.52042633],\n","       [0.40898108],\n","       [0.07227997],\n","       [0.08785127],\n","       [0.262089  ],\n","       [0.40162826],\n","       [0.76399374],\n","       [0.5478275 ],\n","       [0.74989295],\n","       [0.1438932 ],\n","       [0.32970828],\n","       [0.25265232],\n","       [0.3025111 ],\n","       [0.59270066],\n","       [0.25893235],\n","       [0.07316968],\n","       [0.2930411 ],\n","       [0.08981307],\n","       [0.71240205],\n","       [0.5316281 ],\n","       [0.3199927 ],\n","       [0.6663612 ],\n","       [0.4562527 ],\n","       [0.21703449],\n","       [0.5807655 ],\n","       [0.731432  ],\n","       [0.04118342],\n","       [0.04118851],\n","       [0.10513192],\n","       [0.4266272 ],\n","       [0.11977641],\n","       [0.5182937 ],\n","       [0.08886121],\n","       [0.10082322],\n","       [0.46077353],\n","       [0.41924557],\n","       [0.30539617],\n","       [0.09558064],\n","       [0.7436778 ],\n","       [0.09180248],\n","       [0.09075347],\n","       [0.24495113],\n","       [0.08410728],\n","       [0.9270242 ],\n","       [0.59806114],\n","       [0.5715557 ],\n","       [0.813495  ],\n","       [0.53141767],\n","       [0.46041182],\n","       [0.2828613 ],\n","       [0.26685762],\n","       [0.09161733],\n","       [0.5778692 ],\n","       [0.49996406],\n","       [0.08382326],\n","       [0.18581678],\n","       [0.6840503 ],\n","       [0.27571175],\n","       [0.25007   ],\n","       [0.7237667 ],\n","       [0.258295  ],\n","       [0.72584695],\n","       [0.52266705],\n","       [0.0889051 ],\n","       [0.432721  ],\n","       [0.14458974],\n","       [0.5805626 ],\n","       [0.07795507],\n","       [0.12228853],\n","       [0.90087885],\n","       [0.08907068],\n","       [0.59076566],\n","       [0.6414434 ],\n","       [0.9147075 ],\n","       [0.11799224],\n","       [0.23468567],\n","       [0.1105182 ],\n","       [0.62114376],\n","       [0.71702474],\n","       [0.7040603 ],\n","       [0.10448162],\n","       [0.27576753],\n","       [0.25594422],\n","       [0.5844963 ],\n","       [0.51357484],\n","       [0.53196025],\n","       [0.7191161 ],\n","       [0.51865   ],\n","       [0.29242063],\n","       [0.792085  ],\n","       [0.08076472],\n","       [0.09080447],\n","       [0.6922998 ],\n","       [0.8132922 ],\n","       [0.61600053],\n","       [0.92532146],\n","       [0.26477084],\n","       [0.25632396],\n","       [0.52947044],\n","       [0.57189727],\n","       [0.28569448],\n","       [0.700554  ],\n","       [0.787835  ],\n","       [0.5881675 ],\n","       [0.6822647 ],\n","       [0.52905256],\n","       [0.7818775 ],\n","       [0.62123615],\n","       [0.10519136],\n","       [0.5488299 ],\n","       [0.6375065 ],\n","       [0.10965542],\n","       [0.10470507],\n","       [0.08543089],\n","       [0.29455075],\n","       [0.99859345],\n","       [0.63389724],\n","       [0.47726005],\n","       [0.4544172 ],\n","       [0.5880325 ],\n","       [0.75055975],\n","       [0.08559911],\n","       [0.505872  ],\n","       [0.35281956],\n","       [0.6484    ],\n","       [0.9883889 ],\n","       [0.09322794],\n","       [0.3968845 ],\n","       [0.5431821 ],\n","       [0.6981544 ],\n","       [0.26547834],\n","       [0.57640356],\n","       [0.5319608 ],\n","       [0.848287  ],\n","       [0.12739159],\n","       [0.09224282],\n","       [0.861979  ],\n","       [0.93740004],\n","       [0.7479872 ],\n","       [0.3248002 ],\n","       [0.09920694],\n","       [0.2584884 ],\n","       [0.13152179],\n","       [0.22473119],\n","       [0.30741942],\n","       [0.065834  ],\n","       [0.23916462],\n","       [0.08555026],\n","       [0.24004464],\n","       [0.8569243 ],\n","       [0.56229776],\n","       [0.0859227 ],\n","       [0.9113328 ],\n","       [0.65428376],\n","       [0.08415665],\n","       [0.7194388 ],\n","       [0.1261658 ],\n","       [0.02535649],\n","       [0.32263792],\n","       [0.12067048],\n","       [0.8710709 ],\n","       [0.8968207 ],\n","       [0.12832007],\n","       [0.5284466 ],\n","       [0.08837044],\n","       [0.129142  ],\n","       [0.08687834],\n","       [0.4104064 ],\n","       [0.52529997],\n","       [0.08735499],\n","       [0.7175476 ],\n","       [0.26351136],\n","       [0.11661784],\n","       [0.08832116],\n","       [0.5851383 ],\n","       [0.11314528],\n","       [0.10709725],\n","       [0.2614792 ],\n","       [0.50245285],\n","       [0.0858217 ],\n","       [0.29049316],\n","       [0.26131216],\n","       [0.12353942],\n","       [0.04412939],\n","       [0.92515725],\n","       [0.09450135],\n","       [0.6572462 ],\n","       [0.04901079],\n","       [0.8693957 ],\n","       [0.828292  ],\n","       [0.60437995],\n","       [0.2569909 ],\n","       [0.02428434]], dtype=float32)"]},"metadata":{},"execution_count":14}],"source":["LSTM_y_pred = regressor.predict(X_test)\n","LSTM_y_pred"]},{"cell_type":"markdown","metadata":{"id":"sx4JotTKuV0Y"},"source":["##Evalute the Model\n","\n","\n","1.   Mean Absolute error\n","2.   Mean Squared error\n","3.   r2 score"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJD7kVfnualY","outputId":"407790b8-1f08-4394-8d26-396c8869e1f1","executionInfo":{"status":"ok","timestamp":1668518895090,"user_tz":-330,"elapsed":1362,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["mean absolute error value is =  0.01822307294246395\n","mean squared error value is =  0.000576345052974757\n","R2 score value is =  0.992302081335751\n"]}],"source":["MAE = mean_absolute_error(LSTM_y_pred, y_test)\n","MSE=mean_squared_error(LSTM_y_pred, y_test)\n","R2=r2_score(LSTM_y_pred, y_test)\n","print(\"mean absolute error value is = \",MAE)\n","print(\"mean squared error value is = \",MSE)\n","print(\"R2 score value is = \",R2)"]},{"cell_type":"markdown","metadata":{"id":"IenN1-_DuwD9"},"source":["#Gridsearch with LSTM"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Jd0wYa0hu8FL","executionInfo":{"status":"ok","timestamp":1668518906389,"user_tz":-330,"elapsed":1505,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["from keras.wrappers.scikit_learn import KerasClassifier"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"360RHFNWvNNF","executionInfo":{"status":"ok","timestamp":1668518910800,"user_tz":-330,"elapsed":519,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["parameters = {    'batch_size' : [20,32,40],\n","              'epochs' : [8,30,50],\n","              'optimizer' : ['adam','Adadelta','rmsprop']\n","                 }"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"hBpuGOsTvaz5","executionInfo":{"status":"ok","timestamp":1668518917454,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[],"source":["def build_lstm(optimizer):\n","    regressor = Sequential()\n","    # First LSTM layer with Dropout regularisation\n","    regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n","    regressor.add(Dropout(0.2))\n","    # Second LSTM layer\n","    regressor.add(LSTM(units=50, return_sequences=True))\n","    regressor.add(Dropout(0.2))\n","    # Third LSTM layer\n","    regressor.add(LSTM(units=50, return_sequences=True))\n","    regressor.add(Dropout(0.2))\n","    # Fourth LSTM layer\n","    regressor.add(LSTM(units=50))\n","    regressor.add(Dropout(0.2))\n","    # The output layer\n","    regressor.add(Dense(units=1))\n","    # Compiling the RNN\n","    regressor.compile(loss='mean_squared_error', optimizer = optimizer,metrics=[\"accuracy\"])\n","    return regressor"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4AN5VaLwXdT","outputId":"0c152deb-aafa-4361-f6a6-e1dba27f1a82","executionInfo":{"status":"ok","timestamp":1668518924244,"user_tz":-330,"elapsed":26,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["grid_model = KerasClassifier(build_fn=build_lstm)\n","LSTM_grid_search  = GridSearchCV(estimator = grid_model,\n","                            param_grid = parameters,\n","                            cv = 3)"]},{"cell_type":"markdown","metadata":{"id":"8gL21zTaxadG"},"source":["With the train part of dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDQWB8lg3IZB","outputId":"e88db888-ae59-4468-e679-b74d3c935b7f","executionInfo":{"status":"ok","timestamp":1668520049873,"user_tz":-330,"elapsed":1114263,"user":{"displayName":"Anubrata Sarkar","userId":"00989716644763908460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n","20/20 [==============================] - 5s 9ms/step - loss: 53009.3906 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 51150.0586 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 49413.5000 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48850.8984 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 10ms/step - loss: 48368.5352 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 8ms/step - loss: 47909.8594 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47438.7734 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47070.0664 - accuracy: 0.0025\n","10/10 [==============================] - 2s 3ms/step - loss: 48974.6133 - accuracy: 0.0000e+00\n","Epoch 1/8\n","20/20 [==============================] - 5s 8ms/step - loss: 53009.6406 - accuracy: 0.0050\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 50906.4141 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48902.2344 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48345.1914 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47893.5703 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47405.8750 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47002.6797 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46574.8750 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 47085.9492 - accuracy: 0.0050 \n","Epoch 1/8\n","20/20 [==============================] - 5s 8ms/step - loss: 52983.5469 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 10ms/step - loss: 50646.6836 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48483.2500 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47857.2812 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47384.0938 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46951.3516 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 10ms/step - loss: 46446.2695 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 10ms/step - loss: 46181.9883 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 43790.5391 - accuracy: 0.0000e+00\n","Epoch 1/8\n","20/20 [==============================] - 5s 8ms/step - loss: 53133.0156 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.8711 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.7344 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.5391 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.3789 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.2188 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.0508 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.8984 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 55354.3008 - accuracy: 0.0050 \n","Epoch 1/8\n","20/20 [==============================] - 5s 8ms/step - loss: 53133.9062 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 8ms/step - loss: 53133.7695 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.6250 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.5156 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.3906 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.2266 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.0742 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9219 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 53923.4336 - accuracy: 0.0000e+00\n","Epoch 1/8\n","20/20 [==============================] - 6s 9ms/step - loss: 53133.5938 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.5117 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 8ms/step - loss: 53133.3281 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.1562 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9766 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9062 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 8ms/step - loss: 53132.7383 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.5312 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 50858.8594 - accuracy: 0.0000e+00\n","Epoch 1/8\n","20/20 [==============================] - 6s 9ms/step - loss: 51563.2500 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48974.2500 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48471.0195 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48104.9805 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47686.6484 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47314.9141 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46980.8008 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46571.4102 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 48504.6992 - accuracy: 0.0000e+00\n","Epoch 1/8\n","20/20 [==============================] - 5s 9ms/step - loss: 51725.6953 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48933.5703 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48476.3008 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48073.1758 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47654.2344 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47319.5312 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46987.2969 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 8ms/step - loss: 46613.8750 - accuracy: 0.0025\n","10/10 [==============================] - 2s 3ms/step - loss: 47091.9688 - accuracy: 0.0050 \n","Epoch 1/8\n","20/20 [==============================] - 5s 9ms/step - loss: 51577.2656 - accuracy: 0.0025\n","Epoch 2/8\n","20/20 [==============================] - 0s 9ms/step - loss: 49105.7617 - accuracy: 0.0025\n","Epoch 3/8\n","20/20 [==============================] - 0s 10ms/step - loss: 48653.9688 - accuracy: 0.0025\n","Epoch 4/8\n","20/20 [==============================] - 0s 9ms/step - loss: 48207.3984 - accuracy: 0.0025\n","Epoch 5/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47837.5391 - accuracy: 0.0025\n","Epoch 6/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47436.7734 - accuracy: 0.0025\n","Epoch 7/8\n","20/20 [==============================] - 0s 9ms/step - loss: 47046.6094 - accuracy: 0.0025\n","Epoch 8/8\n","20/20 [==============================] - 0s 9ms/step - loss: 46728.3203 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 44382.9336 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 6s 9ms/step - loss: 53017.8516 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 50860.0117 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 10ms/step - loss: 48713.0703 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48089.0195 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47630.6250 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47191.9492 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46744.0234 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46268.6836 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45890.8047 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45569.8008 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45206.0312 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44750.2617 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44408.2188 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44021.4805 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43659.0859 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43317.3086 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43009.1914 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42678.5938 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 10ms/step - loss: 42250.7344 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41913.6133 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41685.2344 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41277.2812 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41043.7461 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40674.3633 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40261.6836 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40106.4766 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39661.7812 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39339.1250 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39151.5430 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 9ms/step - loss: 38804.7461 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 40602.6484 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 5s 9ms/step - loss: 53017.0508 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 50970.9766 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 10ms/step - loss: 48886.2656 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48243.2617 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47768.9688 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47307.0391 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46878.2852 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46484.2734 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46089.4492 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 10ms/step - loss: 45651.5156 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45289.3008 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44930.8633 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44565.3867 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44235.5898 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43721.5195 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43418.4258 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43192.3789 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42791.6445 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42480.2461 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 10ms/step - loss: 42122.3008 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41855.9766 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41385.6367 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 10ms/step - loss: 41140.1289 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40794.5508 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40467.1836 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40129.7461 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39817.9883 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39518.5742 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39156.5742 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 10ms/step - loss: 38895.5938 - accuracy: 0.0025\n","10/10 [==============================] - 2s 3ms/step - loss: 39339.1602 - accuracy: 0.0050 \n","Epoch 1/30\n","20/20 [==============================] - 5s 9ms/step - loss: 53014.1211 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 50960.7031 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 10ms/step - loss: 48951.8906 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 10ms/step - loss: 48370.4336 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47890.1250 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 10ms/step - loss: 47446.0117 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 10ms/step - loss: 47006.5547 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46680.7969 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46187.2500 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45839.5039 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 10ms/step - loss: 45421.5234 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45008.4453 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44678.7617 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44331.7266 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43966.5039 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43601.4844 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43229.0742 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42965.7852 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42555.1641 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42198.0156 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41875.2461 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41485.4688 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 10ms/step - loss: 41134.7617 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40997.5938 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40566.5664 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40285.8320 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39875.2695 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39552.4609 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39253.2031 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39141.3359 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 36904.5508 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 6s 9ms/step - loss: 53133.2500 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.0703 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9258 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.7695 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.5156 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.3516 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.1953 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.0234 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.8438 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.5742 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.3203 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.1797 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.9648 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.8086 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.5664 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.3086 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.0703 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.8945 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.7109 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.4609 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.3008 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.9297 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.7461 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.4336 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.3516 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.9609 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.7812 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.3906 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.2695 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53126.9609 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 55349.1445 - accuracy: 0.0050 \n","Epoch 1/30\n","20/20 [==============================] - 5s 8ms/step - loss: 53134.1250 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53134.0156 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.7539 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 8ms/step - loss: 53133.6719 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53133.3945 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.2500 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.0898 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9219 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.8047 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.5703 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.4336 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.2383 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.2148 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.9062 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.7148 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.5234 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.3086 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.1445 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.9336 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.7383 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 8ms/step - loss: 53130.5234 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.2188 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.0742 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.7812 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.6719 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.5234 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.1211 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.9805 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.7109 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.3945 - accuracy: 0.0025\n","10/10 [==============================] - 2s 3ms/step - loss: 53918.8867 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 5s 9ms/step - loss: 53132.1094 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.9258 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.8555 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 8ms/step - loss: 53131.7188 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.5156 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.3906 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53131.2695 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.0547 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.9766 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.8047 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.5859 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.5195 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.3945 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.1836 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.9648 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.7539 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.5195 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.4805 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.1953 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.0156 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 8ms/step - loss: 53128.8359 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.5195 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.5391 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.2812 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.0156 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.8008 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.5469 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.3984 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.1055 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.8750 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 50853.4531 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 6s 9ms/step - loss: 51822.7109 - accuracy: 0.0025\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 49531.5664 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 10ms/step - loss: 49043.0156 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48652.4961 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 10ms/step - loss: 48287.3906 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47894.7500 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47516.4258 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47142.1562 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46796.0938 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46462.9805 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46052.7891 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45711.3789 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45362.9609 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44990.4688 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44670.5938 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44297.5547 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43923.8438 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43600.1445 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43281.6367 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42884.1211 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 10ms/step - loss: 42556.1055 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42131.8008 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41872.0156 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41543.1758 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41240.1016 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40812.8438 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40521.6562 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40298.7500 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39906.9414 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 8ms/step - loss: 39637.9414 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 41378.2188 - accuracy: 0.0000e+00\n","Epoch 1/30\n","20/20 [==============================] - 6s 8ms/step - loss: 51602.9961 - accuracy: 0.0050\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 49021.8203 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48472.6953 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48100.7109 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47702.5391 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47330.5703 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46960.7305 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46592.1289 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46228.4766 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45891.2891 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45490.5469 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 10ms/step - loss: 45145.8711 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44813.1289 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44439.2539 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44126.7852 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43710.5312 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 10ms/step - loss: 43416.9062 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43098.2617 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42757.1094 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42358.7383 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42103.3867 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41748.3555 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 8ms/step - loss: 41343.1055 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40991.8711 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40761.4883 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40365.6445 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40069.8359 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 10ms/step - loss: 39752.6406 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39525.7617 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39217.6523 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 39524.5508 - accuracy: 0.0050 \n","Epoch 1/30\n","20/20 [==============================] - 5s 9ms/step - loss: 51485.0391 - accuracy: 0.0050\n","Epoch 2/30\n","20/20 [==============================] - 0s 9ms/step - loss: 49140.2148 - accuracy: 0.0025\n","Epoch 3/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48677.9961 - accuracy: 0.0025\n","Epoch 4/30\n","20/20 [==============================] - 0s 9ms/step - loss: 48261.1562 - accuracy: 0.0025\n","Epoch 5/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47836.7148 - accuracy: 0.0025\n","Epoch 6/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47522.6289 - accuracy: 0.0025\n","Epoch 7/30\n","20/20 [==============================] - 0s 9ms/step - loss: 47104.2891 - accuracy: 0.0025\n","Epoch 8/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46773.2344 - accuracy: 0.0025\n","Epoch 9/30\n","20/20 [==============================] - 0s 9ms/step - loss: 46366.1211 - accuracy: 0.0025\n","Epoch 10/30\n","20/20 [==============================] - 0s 10ms/step - loss: 46019.0586 - accuracy: 0.0025\n","Epoch 11/30\n","20/20 [==============================] - 0s 10ms/step - loss: 45669.6562 - accuracy: 0.0025\n","Epoch 12/30\n","20/20 [==============================] - 0s 9ms/step - loss: 45305.6992 - accuracy: 0.0025\n","Epoch 13/30\n","20/20 [==============================] - 0s 9ms/step - loss: 44948.6758 - accuracy: 0.0025\n","Epoch 14/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44664.7305 - accuracy: 0.0025\n","Epoch 15/30\n","20/20 [==============================] - 0s 10ms/step - loss: 44186.4844 - accuracy: 0.0025\n","Epoch 16/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43953.9883 - accuracy: 0.0025\n","Epoch 17/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43596.1641 - accuracy: 0.0025\n","Epoch 18/30\n","20/20 [==============================] - 0s 9ms/step - loss: 43258.3281 - accuracy: 0.0025\n","Epoch 19/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42920.5938 - accuracy: 0.0025\n","Epoch 20/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42598.7812 - accuracy: 0.0025\n","Epoch 21/30\n","20/20 [==============================] - 0s 9ms/step - loss: 42194.0664 - accuracy: 0.0025\n","Epoch 22/30\n","20/20 [==============================] - 0s 10ms/step - loss: 41839.9141 - accuracy: 0.0025\n","Epoch 23/30\n","20/20 [==============================] - 0s 9ms/step - loss: 41553.4688 - accuracy: 0.0025\n","Epoch 24/30\n","20/20 [==============================] - 0s 10ms/step - loss: 41299.5352 - accuracy: 0.0025\n","Epoch 25/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40900.6133 - accuracy: 0.0025\n","Epoch 26/30\n","20/20 [==============================] - 0s 9ms/step - loss: 40567.1094 - accuracy: 0.0025\n","Epoch 27/30\n","20/20 [==============================] - 0s 10ms/step - loss: 40261.0898 - accuracy: 0.0025\n","Epoch 28/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39950.8555 - accuracy: 0.0025\n","Epoch 29/30\n","20/20 [==============================] - 0s 9ms/step - loss: 39509.6016 - accuracy: 0.0025\n","Epoch 30/30\n","20/20 [==============================] - 0s 8ms/step - loss: 39210.8086 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 37110.7695 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 6s 9ms/step - loss: 53020.2383 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 9ms/step - loss: 51092.3633 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 9ms/step - loss: 48891.5859 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48354.4453 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47836.1797 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47384.3281 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46951.6836 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46536.3281 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46207.3516 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45835.4883 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45416.3398 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45018.5000 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44626.3711 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 9ms/step - loss: 44308.9414 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43914.3359 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43601.9844 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43258.4961 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42864.6602 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42535.0586 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42196.4805 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41855.1211 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41592.7266 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 11ms/step - loss: 41112.4258 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 11ms/step - loss: 40834.6133 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40497.8750 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40179.8008 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39859.3047 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39544.3633 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39262.5898 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39003.2812 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38633.5117 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38422.4453 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 11ms/step - loss: 38086.9688 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37883.3008 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37508.7969 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37199.0703 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36967.9453 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36664.9883 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36326.0469 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 9ms/step - loss: 36179.1367 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35828.1836 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35450.4375 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35394.2344 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35033.8906 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34745.6367 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34442.1367 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34277.4766 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34188.1133 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33669.6719 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 9ms/step - loss: 33415.2891 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 35158.1016 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 5s 9ms/step - loss: 53026.3164 - accuracy: 0.0050\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 50840.3008 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48802.7148 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48227.8438 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47779.8164 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47294.1836 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46917.9766 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46520.0117 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 11ms/step - loss: 46064.2109 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45693.9336 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45314.9297 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44950.6016 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44596.5156 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44274.2109 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43832.8281 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43473.7812 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43207.8203 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42782.8008 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42494.0195 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42127.0508 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41855.5938 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41529.8516 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41123.4883 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40702.8711 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40498.9844 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40292.9219 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39861.8008 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39619.8516 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39173.7422 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38894.0664 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38616.9844 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38379.4258 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38091.9609 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37759.7891 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37403.8711 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37236.8008 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36827.9844 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 11ms/step - loss: 36726.0586 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36234.0117 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 9ms/step - loss: 36061.0508 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35765.2344 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35646.5312 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35251.5586 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35054.2266 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34695.6055 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34380.4688 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34160.9297 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 11ms/step - loss: 33846.6797 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33722.1992 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 9ms/step - loss: 33373.8008 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 33768.6094 - accuracy: 0.0050 \n","Epoch 1/50\n","20/20 [==============================] - 5s 11ms/step - loss: 52991.5117 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 50730.2617 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48855.6289 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48248.3945 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47757.3633 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47304.6133 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46922.5469 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46588.3555 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 9ms/step - loss: 46111.0312 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 9ms/step - loss: 45705.1289 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45363.5508 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 11ms/step - loss: 44975.4414 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 9ms/step - loss: 44612.2812 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44239.6094 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43877.5352 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43505.1953 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43214.4531 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42895.3594 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42506.4336 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42174.5898 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41733.6797 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 11ms/step - loss: 41532.7812 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41189.1836 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 9ms/step - loss: 40947.9492 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40528.6250 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40214.8359 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39971.2344 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39556.5547 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39307.0664 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38957.1133 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38656.8438 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38393.6758 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38145.6289 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37831.4805 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37393.1875 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37167.9492 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 9ms/step - loss: 36900.5703 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36647.3516 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36375.4844 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35999.9844 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35724.3906 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 9ms/step - loss: 35461.6875 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35382.0469 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35059.6836 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 11ms/step - loss: 34693.9805 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34570.8125 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34261.1719 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33917.9258 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33763.3633 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33389.1953 - accuracy: 0.0025\n","10/10 [==============================] - 2s 5ms/step - loss: 31489.2207 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 5s 9ms/step - loss: 53133.3047 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.1367 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53133.0234 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.8516 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.7266 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.5000 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.3633 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.1602 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.0312 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.8281 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.6484 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.5117 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.3242 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.0781 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.8594 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.7148 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.5039 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.2891 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.1016 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.9531 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.7188 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.4688 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.2734 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.0508 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.8555 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.5312 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.3438 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.1133 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.8242 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.5742 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.3164 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.1094 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53126.8086 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.6094 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.3633 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.9805 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.5938 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.3867 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.1211 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.7344 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53124.5859 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.1992 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.9102 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.4961 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.1406 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53122.8359 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53122.4688 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.9219 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.7031 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.3203 - accuracy: 0.0025\n","10/10 [==============================] - 1s 3ms/step - loss: 55343.2383 - accuracy: 0.0050 \n","Epoch 1/50\n","20/20 [==============================] - 6s 9ms/step - loss: 53133.8945 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53133.6367 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53133.5117 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53133.3008 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53133.1914 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.9492 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.7188 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.5742 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53132.3398 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.2617 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.9102 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.8203 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.5352 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.4219 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.1445 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.8867 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.5742 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.4688 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53130.2344 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.0156 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.6992 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.4648 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.2305 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53128.9648 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.7617 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.4531 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 11ms/step - loss: 53128.1914 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.9805 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.5703 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.3281 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.0117 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53126.7344 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.4609 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.1211 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.6641 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.4648 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53125.1562 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.7734 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.5156 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.0938 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 11ms/step - loss: 53123.7266 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.4453 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.0117 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53122.5664 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53122.2188 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.8867 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.3633 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53121.0117 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53120.4688 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53120.0703 - accuracy: 0.0025\n","10/10 [==============================] - 1s 5ms/step - loss: 53910.1953 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 6s 10ms/step - loss: 53133.0312 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.8164 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.7148 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.6367 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.3711 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.1914 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53132.0859 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.8086 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.6797 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.5156 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.3398 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53131.1602 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.9062 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.7266 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.5469 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.3555 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53130.1719 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.9609 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.7188 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53129.4297 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.2188 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53129.0195 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.8281 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.6133 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.4141 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53128.1367 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53127.9062 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.7500 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.4492 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53127.1797 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.9141 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53126.6836 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.5039 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53126.1094 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.7344 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53125.4297 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53125.2812 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.9805 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53124.7344 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.4297 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53124.1367 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.8516 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.3555 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53123.0703 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53122.7305 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 11ms/step - loss: 53122.5664 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53122.1289 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.6016 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 9ms/step - loss: 53121.4102 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 10ms/step - loss: 53121.0391 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 50847.4883 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 5s 10ms/step - loss: 51212.8203 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48401.5664 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47914.4648 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47468.9688 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47078.5312 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46694.1094 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46308.0781 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45932.1133 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45563.2812 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45338.3086 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44872.9648 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 11ms/step - loss: 44593.7148 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44263.4453 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43861.2734 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43588.6484 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 11ms/step - loss: 43191.2695 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42756.1211 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 9ms/step - loss: 42478.4453 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 9ms/step - loss: 42127.3164 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41791.8555 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41490.1602 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41079.4844 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40742.4141 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40481.7383 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40034.0352 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39893.5312 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 9ms/step - loss: 39541.8438 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 9ms/step - loss: 39149.8594 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38867.1562 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38614.8242 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38259.0703 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37916.1680 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 9ms/step - loss: 37583.7422 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37180.8633 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36939.5352 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36709.4219 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36481.3789 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36051.0117 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35748.1523 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35554.4141 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35279.3750 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34965.2031 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34532.7656 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 11ms/step - loss: 34276.7617 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34020.0234 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33737.4727 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33420.2188 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33188.5078 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 32921.9414 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 10ms/step - loss: 32765.8574 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 34347.8984 - accuracy: 0.0000e+00\n","Epoch 1/50\n","20/20 [==============================] - 6s 9ms/step - loss: 51520.2305 - accuracy: 0.0025\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 49275.1797 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48820.2656 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48425.9844 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48024.0742 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47627.0898 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47256.6797 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46902.2148 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46541.5664 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46174.3086 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45878.3945 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45442.0391 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45135.8047 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44759.4883 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44367.6641 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44097.6211 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43649.0898 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 9ms/step - loss: 43348.8555 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43048.8203 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42737.5781 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42280.0664 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41924.1367 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41685.7734 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 10ms/step - loss: 41299.7070 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40967.8750 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40604.2188 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40294.3867 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39982.3711 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39772.7500 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39364.0469 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38968.4453 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38636.6875 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38346.7891 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38104.5547 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37852.2383 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37489.8984 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37200.9062 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36832.0547 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36536.6836 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36138.9258 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36015.1758 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35736.7031 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35248.0078 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 11ms/step - loss: 34991.8242 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34729.4961 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34575.3320 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34150.5352 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33915.2656 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33719.4805 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 9ms/step - loss: 33328.1562 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 33692.8242 - accuracy: 0.0050 \n","Epoch 1/50\n","20/20 [==============================] - 6s 10ms/step - loss: 51546.8242 - accuracy: 0.0050\n","Epoch 2/50\n","20/20 [==============================] - 0s 10ms/step - loss: 49128.4844 - accuracy: 0.0025\n","Epoch 3/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48563.7188 - accuracy: 0.0025\n","Epoch 4/50\n","20/20 [==============================] - 0s 10ms/step - loss: 48225.9141 - accuracy: 0.0025\n","Epoch 5/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47860.6602 - accuracy: 0.0025\n","Epoch 6/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47436.3945 - accuracy: 0.0025\n","Epoch 7/50\n","20/20 [==============================] - 0s 10ms/step - loss: 47038.5195 - accuracy: 0.0025\n","Epoch 8/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46711.2188 - accuracy: 0.0025\n","Epoch 9/50\n","20/20 [==============================] - 0s 10ms/step - loss: 46307.3438 - accuracy: 0.0025\n","Epoch 10/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45922.5117 - accuracy: 0.0025\n","Epoch 11/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45563.0000 - accuracy: 0.0025\n","Epoch 12/50\n","20/20 [==============================] - 0s 10ms/step - loss: 45219.7539 - accuracy: 0.0025\n","Epoch 13/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44940.2617 - accuracy: 0.0025\n","Epoch 14/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44521.4258 - accuracy: 0.0025\n","Epoch 15/50\n","20/20 [==============================] - 0s 10ms/step - loss: 44208.0234 - accuracy: 0.0025\n","Epoch 16/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43879.7031 - accuracy: 0.0025\n","Epoch 17/50\n","20/20 [==============================] - 0s 11ms/step - loss: 43456.0000 - accuracy: 0.0025\n","Epoch 18/50\n","20/20 [==============================] - 0s 10ms/step - loss: 43096.6133 - accuracy: 0.0025\n","Epoch 19/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42838.5703 - accuracy: 0.0025\n","Epoch 20/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42391.5898 - accuracy: 0.0025\n","Epoch 21/50\n","20/20 [==============================] - 0s 10ms/step - loss: 42163.4883 - accuracy: 0.0025\n","Epoch 22/50\n","20/20 [==============================] - 0s 11ms/step - loss: 41805.2969 - accuracy: 0.0025\n","Epoch 23/50\n","20/20 [==============================] - 0s 9ms/step - loss: 41394.0703 - accuracy: 0.0025\n","Epoch 24/50\n","20/20 [==============================] - 0s 11ms/step - loss: 41083.8945 - accuracy: 0.0025\n","Epoch 25/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40763.3203 - accuracy: 0.0025\n","Epoch 26/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40564.1797 - accuracy: 0.0025\n","Epoch 27/50\n","20/20 [==============================] - 0s 10ms/step - loss: 40153.6016 - accuracy: 0.0025\n","Epoch 28/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39812.8438 - accuracy: 0.0025\n","Epoch 29/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39472.8320 - accuracy: 0.0025\n","Epoch 30/50\n","20/20 [==============================] - 0s 10ms/step - loss: 39245.3984 - accuracy: 0.0025\n","Epoch 31/50\n","20/20 [==============================] - 0s 11ms/step - loss: 38836.2461 - accuracy: 0.0025\n","Epoch 32/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38486.6289 - accuracy: 0.0025\n","Epoch 33/50\n","20/20 [==============================] - 0s 10ms/step - loss: 38191.5781 - accuracy: 0.0025\n","Epoch 34/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37863.5586 - accuracy: 0.0025\n","Epoch 35/50\n","20/20 [==============================] - 0s 10ms/step - loss: 37563.1016 - accuracy: 0.0025\n","Epoch 36/50\n","20/20 [==============================] - 0s 11ms/step - loss: 37293.7969 - accuracy: 0.0025\n","Epoch 37/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36988.0820 - accuracy: 0.0025\n","Epoch 38/50\n","20/20 [==============================] - 0s 11ms/step - loss: 36635.5117 - accuracy: 0.0025\n","Epoch 39/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36312.1016 - accuracy: 0.0025\n","Epoch 40/50\n","20/20 [==============================] - 0s 10ms/step - loss: 36067.3398 - accuracy: 0.0025\n","Epoch 41/50\n","20/20 [==============================] - 0s 11ms/step - loss: 35802.5742 - accuracy: 0.0025\n","Epoch 42/50\n","20/20 [==============================] - 0s 11ms/step - loss: 35356.2734 - accuracy: 0.0025\n","Epoch 43/50\n","20/20 [==============================] - 0s 10ms/step - loss: 35205.3242 - accuracy: 0.0025\n","Epoch 44/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34932.9023 - accuracy: 0.0025\n","Epoch 45/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34677.5117 - accuracy: 0.0025\n","Epoch 46/50\n","20/20 [==============================] - 0s 10ms/step - loss: 34400.3359 - accuracy: 0.0025\n","Epoch 47/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33954.0781 - accuracy: 0.0025\n","Epoch 48/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33692.5586 - accuracy: 0.0025\n","Epoch 49/50\n","20/20 [==============================] - 0s 10ms/step - loss: 33439.3164 - accuracy: 0.0025\n","Epoch 50/50\n","20/20 [==============================] - 0s 11ms/step - loss: 33260.2344 - accuracy: 0.0025\n","10/10 [==============================] - 1s 4ms/step - loss: 31228.3691 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 7s 13ms/step - loss: 53088.2031 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 52648.6094 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 50359.0664 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48978.8203 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48531.0039 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48169.9648 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47856.5352 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47602.4453 - accuracy: 0.0025\n","7/7 [==============================] - 1s 4ms/step - loss: 49543.5391 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 6s 13ms/step - loss: 53093.5586 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 52647.8516 - accuracy: 0.0000e+00\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 50484.3867 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 14ms/step - loss: 49112.6250 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48613.2734 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48263.7617 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47932.9609 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47676.0039 - accuracy: 0.0025\n","7/7 [==============================] - 1s 4ms/step - loss: 48188.7812 - accuracy: 0.0050\n","Epoch 1/8\n","13/13 [==============================] - 6s 14ms/step - loss: 53086.0352 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 14ms/step - loss: 52618.3281 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 50296.5742 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48771.4258 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48345.3750 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47955.6250 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47652.0195 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47342.7656 - accuracy: 0.0025\n","7/7 [==============================] - 1s 4ms/step - loss: 45049.6016 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 7s 13ms/step - loss: 53133.1719 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.1055 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.0508 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.8984 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.7695 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.6250 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.5938 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.4336 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 55354.8398 - accuracy: 0.0050\n","Epoch 1/8\n","13/13 [==============================] - 7s 14ms/step - loss: 53133.1758 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.1562 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.9531 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.8594 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.7812 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.6641 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.5586 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.4648 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 53922.9766 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 7s 14ms/step - loss: 53133.8594 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.8008 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.6797 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.5703 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.4102 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.2734 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.1406 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.0508 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 50859.3711 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 6s 13ms/step - loss: 52615.6562 - accuracy: 0.0000e+00\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 49597.9844 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 15ms/step - loss: 49044.5469 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 13ms/step - loss: 48731.7852 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48454.1836 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48227.9297 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47968.0547 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47674.7539 - accuracy: 0.0025\n","7/7 [==============================] - 2s 4ms/step - loss: 49686.9219 - accuracy: 0.0000e+00\n","Epoch 1/8\n","13/13 [==============================] - 6s 14ms/step - loss: 52464.7031 - accuracy: 0.0000e+00\n","Epoch 2/8\n","13/13 [==============================] - 0s 14ms/step - loss: 49537.8359 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48925.9062 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48622.1953 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48348.0898 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48091.4297 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47819.5781 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47584.6797 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 48156.0742 - accuracy: 0.0050\n","Epoch 1/8\n","13/13 [==============================] - 7s 13ms/step - loss: 52660.9883 - accuracy: 0.0025\n","Epoch 2/8\n","13/13 [==============================] - 0s 13ms/step - loss: 49948.3164 - accuracy: 0.0025\n","Epoch 3/8\n","13/13 [==============================] - 0s 14ms/step - loss: 49335.0312 - accuracy: 0.0025\n","Epoch 4/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48981.3789 - accuracy: 0.0025\n","Epoch 5/8\n","13/13 [==============================] - 0s 14ms/step - loss: 48712.1016 - accuracy: 0.0025\n","Epoch 6/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48434.7695 - accuracy: 0.0025\n","Epoch 7/8\n","13/13 [==============================] - 0s 15ms/step - loss: 48250.6914 - accuracy: 0.0025\n","Epoch 8/8\n","13/13 [==============================] - 0s 14ms/step - loss: 47911.2305 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 45653.1953 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 7s 15ms/step - loss: 53093.0312 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 14ms/step - loss: 52627.1953 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 13ms/step - loss: 50374.1562 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 15ms/step - loss: 49033.4414 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48592.5508 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48297.4766 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47900.4609 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47640.9219 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 16ms/step - loss: 47321.3086 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47022.8789 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46805.7031 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46473.2812 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46219.8867 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45999.8906 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45687.0781 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45481.7031 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45267.6836 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45040.8047 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 13ms/step - loss: 44747.9336 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44541.4688 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44278.7383 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44047.4336 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 16ms/step - loss: 43828.6133 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43520.8984 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43392.5195 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43187.2109 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 16ms/step - loss: 42881.1250 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 16ms/step - loss: 42711.1914 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 13ms/step - loss: 42436.7539 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42268.1758 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 44134.6992 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 7s 18ms/step - loss: 53089.1445 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 16ms/step - loss: 52621.7539 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 17ms/step - loss: 50345.4961 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 15ms/step - loss: 48990.1484 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 17ms/step - loss: 48522.9336 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 18ms/step - loss: 48196.8047 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 17ms/step - loss: 47907.8984 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 16ms/step - loss: 47585.2695 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 16ms/step - loss: 47280.0000 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 19ms/step - loss: 47047.9062 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 19ms/step - loss: 46774.3008 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 16ms/step - loss: 46518.4297 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46178.9609 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45946.0664 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45704.3359 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45506.8945 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 16ms/step - loss: 45158.1484 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44977.7383 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44712.7695 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44461.0195 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44353.4297 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 16ms/step - loss: 43967.9141 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43789.7695 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43533.8047 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43340.4062 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43125.0508 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42895.7852 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 16ms/step - loss: 42633.3086 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42409.6562 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42218.3242 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 42719.0859 - accuracy: 0.0050\n","Epoch 1/30\n","13/13 [==============================] - 7s 14ms/step - loss: 53092.0938 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 15ms/step - loss: 52682.8008 - accuracy: 0.0050\n","Epoch 3/30\n","13/13 [==============================] - 0s 15ms/step - loss: 50502.7617 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 15ms/step - loss: 49043.6133 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 15ms/step - loss: 48624.4844 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48250.3984 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47926.2266 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47609.9961 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47317.7188 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47077.4258 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46786.0781 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46528.5117 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46231.2695 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45933.9531 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 16ms/step - loss: 45715.5234 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45460.0742 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45233.3633 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44967.8906 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44794.4102 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44539.4844 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44236.1211 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 16ms/step - loss: 44030.0000 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43777.7031 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43572.0703 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43281.6016 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43105.8203 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 16ms/step - loss: 42914.5391 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42663.1641 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42389.4961 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42229.8164 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 40061.1992 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 6s 14ms/step - loss: 53134.3242 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.2539 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53134.1484 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.0547 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.0000 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.8242 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.7305 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.7188 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.5547 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.4961 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.4453 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.2852 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.1836 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.0547 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.9492 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.8789 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.7656 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.6289 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.5586 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 17ms/step - loss: 53132.4766 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53132.3516 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.2266 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.1406 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.9883 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.8867 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53131.7969 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.6484 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.4961 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.3906 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.2539 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 55353.6836 - accuracy: 0.0050\n","Epoch 1/30\n","13/13 [==============================] - 7s 13ms/step - loss: 53135.2656 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53135.1484 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53134.9531 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.8242 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53134.6562 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.5000 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.4336 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53134.3516 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53134.2109 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53134.1094 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 16ms/step - loss: 53133.9102 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.8086 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.6562 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.5352 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.3633 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.3164 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.2031 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.9805 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.7969 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.7266 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.5898 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.4297 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.2617 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.2305 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.9219 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.8867 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.6250 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53131.5117 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.3398 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.2031 - accuracy: 0.0025\n","7/7 [==============================] - 1s 4ms/step - loss: 53921.6367 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 7s 14ms/step - loss: 53133.8008 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.8438 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.6602 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.5781 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.4453 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.3555 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.2461 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.1250 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.0352 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.8281 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.7812 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.6797 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.5469 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.3906 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.2891 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.2188 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.0508 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.9805 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.7656 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.6289 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.5156 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53131.4961 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.3555 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.1953 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 16ms/step - loss: 53131.0664 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.8984 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.7891 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.6133 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 13ms/step - loss: 53130.5391 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.2891 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 50856.7383 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 7s 14ms/step - loss: 52551.7031 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 15ms/step - loss: 49563.6055 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48979.5508 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48682.6094 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 15ms/step - loss: 48392.5195 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48124.5117 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47868.3906 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 13ms/step - loss: 47607.1602 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47373.6406 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47161.5195 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46885.3945 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46682.0391 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46455.8281 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46193.1992 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45945.3047 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45759.1367 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45506.3242 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45287.6914 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44994.4297 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 16ms/step - loss: 44826.0312 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44630.1719 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 16ms/step - loss: 44325.7695 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44131.8047 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43864.2266 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43593.2695 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43380.4141 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 16ms/step - loss: 43201.4219 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42942.5664 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42725.3555 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42610.7305 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 44433.8047 - accuracy: 0.0000e+00\n","Epoch 1/30\n","13/13 [==============================] - 7s 14ms/step - loss: 52432.7188 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 14ms/step - loss: 49435.7266 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 13ms/step - loss: 48837.5547 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48495.2266 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 15ms/step - loss: 48264.9609 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 16ms/step - loss: 47984.0508 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47705.2969 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 15ms/step - loss: 47487.7852 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47218.6992 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 15ms/step - loss: 46996.0703 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46748.6055 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 16ms/step - loss: 46580.1484 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46326.1289 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46067.2734 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45837.0039 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45594.5508 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45325.5742 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 16ms/step - loss: 45094.5469 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44880.4961 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44638.2500 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44389.0000 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 16ms/step - loss: 44127.5547 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43945.9688 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43658.9453 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43503.0391 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43232.0664 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43072.4414 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42812.9102 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 15ms/step - loss: 42578.8281 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42372.2031 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 42889.6406 - accuracy: 0.0050\n","Epoch 1/30\n","13/13 [==============================] - 6s 14ms/step - loss: 52482.9961 - accuracy: 0.0025\n","Epoch 2/30\n","13/13 [==============================] - 0s 14ms/step - loss: 49548.0156 - accuracy: 0.0025\n","Epoch 3/30\n","13/13 [==============================] - 0s 13ms/step - loss: 48974.1094 - accuracy: 0.0025\n","Epoch 4/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48667.7812 - accuracy: 0.0025\n","Epoch 5/30\n","13/13 [==============================] - 0s 14ms/step - loss: 48437.2109 - accuracy: 0.0025\n","Epoch 6/30\n","13/13 [==============================] - 0s 16ms/step - loss: 48132.2031 - accuracy: 0.0025\n","Epoch 7/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47920.0195 - accuracy: 0.0025\n","Epoch 8/30\n","13/13 [==============================] - 0s 13ms/step - loss: 47675.5781 - accuracy: 0.0025\n","Epoch 9/30\n","13/13 [==============================] - 0s 14ms/step - loss: 47394.6836 - accuracy: 0.0025\n","Epoch 10/30\n","13/13 [==============================] - 0s 16ms/step - loss: 47187.2188 - accuracy: 0.0025\n","Epoch 11/30\n","13/13 [==============================] - 0s 13ms/step - loss: 46891.6094 - accuracy: 0.0025\n","Epoch 12/30\n","13/13 [==============================] - 0s 16ms/step - loss: 46658.1016 - accuracy: 0.0025\n","Epoch 13/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46471.7539 - accuracy: 0.0025\n","Epoch 14/30\n","13/13 [==============================] - 0s 14ms/step - loss: 46196.4805 - accuracy: 0.0025\n","Epoch 15/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45961.8047 - accuracy: 0.0025\n","Epoch 16/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45755.1211 - accuracy: 0.0025\n","Epoch 17/30\n","13/13 [==============================] - 0s 15ms/step - loss: 45472.2266 - accuracy: 0.0025\n","Epoch 18/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45293.7109 - accuracy: 0.0025\n","Epoch 19/30\n","13/13 [==============================] - 0s 14ms/step - loss: 45045.3203 - accuracy: 0.0025\n","Epoch 20/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44760.5547 - accuracy: 0.0025\n","Epoch 21/30\n","13/13 [==============================] - 0s 15ms/step - loss: 44631.1836 - accuracy: 0.0025\n","Epoch 22/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44355.1992 - accuracy: 0.0025\n","Epoch 23/30\n","13/13 [==============================] - 0s 14ms/step - loss: 44109.6289 - accuracy: 0.0025\n","Epoch 24/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43853.3047 - accuracy: 0.0025\n","Epoch 25/30\n","13/13 [==============================] - 0s 14ms/step - loss: 43728.5859 - accuracy: 0.0025\n","Epoch 26/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43470.3984 - accuracy: 0.0025\n","Epoch 27/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43167.3359 - accuracy: 0.0025\n","Epoch 28/30\n","13/13 [==============================] - 0s 15ms/step - loss: 43015.5352 - accuracy: 0.0025\n","Epoch 29/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42720.3750 - accuracy: 0.0025\n","Epoch 30/30\n","13/13 [==============================] - 0s 14ms/step - loss: 42587.9297 - accuracy: 0.0025\n","7/7 [==============================] - 2s 5ms/step - loss: 40364.0352 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 6s 14ms/step - loss: 53098.7539 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 52755.1094 - accuracy: 0.0050\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 50816.1211 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 15ms/step - loss: 49399.1211 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48980.3047 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 16ms/step - loss: 48606.0859 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48268.1055 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47977.4219 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47698.6914 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47368.7109 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 16ms/step - loss: 47118.1445 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46838.2812 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46621.8086 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46332.0664 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46109.7500 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45792.5742 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45590.2539 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45365.6445 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45074.7383 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44810.5117 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44586.7383 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44343.0508 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44130.0156 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 14ms/step - loss: 43871.0938 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43668.7461 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 14ms/step - loss: 43406.5000 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43194.2617 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42969.6289 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42779.6367 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42492.5234 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42392.9805 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42126.4258 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41800.0039 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41629.7812 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41499.2852 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41352.3555 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40970.6992 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 17ms/step - loss: 40829.2188 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40568.7109 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40412.7461 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40142.2969 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39938.8203 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39737.1289 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39544.6289 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39358.6133 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 16ms/step - loss: 39196.9297 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39024.8242 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38756.3438 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38580.8906 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38311.0938 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 40227.0742 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 7s 15ms/step - loss: 53086.1953 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 52624.5703 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 50570.1133 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 14ms/step - loss: 49387.2539 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48928.5234 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 16ms/step - loss: 48552.2305 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 15ms/step - loss: 48235.3242 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47943.4492 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47613.0742 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47332.3750 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47087.2109 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 16ms/step - loss: 46862.8164 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 16ms/step - loss: 46543.0859 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46295.4531 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46072.6992 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45797.6836 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45548.3633 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45264.9492 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45074.6719 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44816.8711 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44573.3008 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44362.9492 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44137.4336 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 14ms/step - loss: 43888.5859 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43616.0312 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43458.7891 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43278.1953 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42977.7344 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42701.5586 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42486.7617 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42253.1719 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42095.7383 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41882.9531 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 14ms/step - loss: 41609.5742 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41440.5039 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41221.9648 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40990.1016 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40783.4883 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 17ms/step - loss: 40617.0156 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40410.3594 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40208.4766 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39965.7734 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39780.6211 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39512.3242 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39389.1641 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39159.2031 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38945.9062 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38784.0117 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38652.3438 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38371.5547 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 38850.3789 - accuracy: 0.0050\n","Epoch 1/50\n","13/13 [==============================] - 7s 14ms/step - loss: 53097.9414 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 52713.9336 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 15ms/step - loss: 50483.2656 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 16ms/step - loss: 48994.9062 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 15ms/step - loss: 48556.9219 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48194.3555 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47887.6836 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47584.9883 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 16ms/step - loss: 47303.6797 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46969.6367 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46738.2891 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46470.5391 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46225.5312 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45929.2266 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45725.2617 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45461.2266 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45218.7969 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44964.5234 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44727.6758 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44441.6484 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44230.1719 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43999.5000 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43795.6602 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43555.8398 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43284.6914 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43103.5898 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42818.0898 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42684.3047 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42449.0508 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42141.6562 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42014.6211 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41794.6133 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41499.6953 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41338.2188 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41094.6094 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40921.3711 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40773.4648 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40565.5508 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40260.0664 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40042.5820 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39786.6836 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39720.9609 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39496.7461 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39304.9297 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38981.8086 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38952.5742 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 14ms/step - loss: 38738.7344 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38428.4531 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38212.6836 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 14ms/step - loss: 38045.1133 - accuracy: 0.0025\n","7/7 [==============================] - 1s 4ms/step - loss: 36027.2539 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 7s 15ms/step - loss: 53133.4141 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.3359 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.2969 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.1484 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.0195 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53132.9219 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.8281 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.7500 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53132.6836 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.5352 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.4492 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.3438 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.2188 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53132.1289 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53132.0469 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.9102 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.7266 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.7305 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.6094 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.4609 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.3633 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.2461 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.1602 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53130.9297 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.8516 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.7969 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.6133 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.4336 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.3984 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.2344 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.1016 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.9961 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.9102 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.6289 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53129.5508 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.4453 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.3398 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.1719 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.9844 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.8594 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.7266 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.4531 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.3047 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.2539 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.0703 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.9062 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.7305 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.5898 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.4531 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53127.2266 - accuracy: 0.0025\n","7/7 [==============================] - 1s 6ms/step - loss: 55349.5195 - accuracy: 0.0050\n","Epoch 1/50\n","13/13 [==============================] - 6s 14ms/step - loss: 53134.4297 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.2266 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.1133 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53134.0039 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53133.8789 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.7695 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.7148 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 13ms/step - loss: 53133.6094 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.5312 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.3594 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53133.3242 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.0664 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.8945 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.8555 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.6250 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.5898 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.4297 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.2891 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.1445 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.0703 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.9844 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.8203 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.6484 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.4688 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.3164 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.2812 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.0508 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53130.9648 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.7305 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.6016 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.5000 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.3555 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.1797 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.0352 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.8203 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.6914 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.5234 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53129.3750 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.2188 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.0898 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.8867 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.6133 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53128.4883 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.3008 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.1055 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53127.8633 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.7969 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.5781 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53127.3945 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.1758 - accuracy: 0.0025\n","7/7 [==============================] - 2s 4ms/step - loss: 53917.5898 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 6s 14ms/step - loss: 53133.1094 - accuracy: 0.0025\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53133.0039 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.9609 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.7969 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.6367 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.5898 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.4453 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.4102 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53132.2656 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53132.1641 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.9688 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.7383 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.8398 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.6836 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 16ms/step - loss: 53131.5508 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.4297 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53131.3281 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53131.1602 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.9805 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.8281 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53130.7461 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.5703 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 13ms/step - loss: 53130.5547 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.3789 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.2344 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53130.0781 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.9414 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.8008 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.6758 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.5352 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.2852 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53129.1445 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53129.0391 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.8750 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.7031 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53128.5312 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.3594 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.2812 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53128.1562 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.7695 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.8047 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53127.5508 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.3906 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53127.2148 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 13ms/step - loss: 53127.0391 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53126.8008 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53126.5312 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53126.3242 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 15ms/step - loss: 53126.2500 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 14ms/step - loss: 53126.0898 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 50852.5352 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 7s 14ms/step - loss: 52460.9648 - accuracy: 0.0000e+00\n","Epoch 2/50\n","13/13 [==============================] - 0s 16ms/step - loss: 49170.8516 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 15ms/step - loss: 48567.3438 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48268.3203 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47975.3047 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 16ms/step - loss: 47734.7969 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47452.7383 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47235.7891 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47000.0117 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 16ms/step - loss: 46711.7305 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 16ms/step - loss: 46463.2266 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46233.1914 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46019.7812 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45840.6953 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45560.8438 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45355.4258 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45088.1836 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44845.3398 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44621.5781 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44396.7500 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44152.0195 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43935.8164 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 14ms/step - loss: 43650.4609 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43491.1719 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43244.0469 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43071.0508 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42811.9688 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42606.7539 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42365.9492 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42157.6367 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41868.2578 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41706.1055 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41417.1367 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41272.2734 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41009.8750 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40823.8984 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40638.8281 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40306.0000 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40312.0352 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39924.8711 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39638.4141 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 16ms/step - loss: 39612.0938 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39418.7656 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39120.9609 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 14ms/step - loss: 38833.0547 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38805.4844 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38595.9766 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 14ms/step - loss: 38317.6719 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38080.8633 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 15ms/step - loss: 37863.4531 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 39720.4688 - accuracy: 0.0000e+00\n","Epoch 1/50\n","13/13 [==============================] - 7s 15ms/step - loss: 52405.8711 - accuracy: 0.0050\n","Epoch 2/50\n","13/13 [==============================] - 0s 16ms/step - loss: 49295.7852 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48695.8242 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48370.1484 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 15ms/step - loss: 48134.1445 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47881.8789 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47652.5195 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47382.5312 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47120.3984 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46889.4531 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46661.9844 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46395.4219 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46128.2188 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45952.1641 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45694.3945 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45457.2852 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45183.2266 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44996.1602 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44792.7969 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44543.3789 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44278.9141 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44057.8633 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43844.7734 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43717.7266 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43441.4609 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43194.0117 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42914.5117 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42654.9609 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42465.7617 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42265.8867 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 15ms/step - loss: 42031.9648 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41934.9766 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 14ms/step - loss: 41574.9336 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41363.0938 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41263.0195 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40876.9141 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40783.0703 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40498.5391 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40257.1016 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 14ms/step - loss: 40134.9141 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39932.4141 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39655.7344 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39439.2344 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39336.6328 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 14ms/step - loss: 39081.4805 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38919.0938 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38651.6094 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38403.5469 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38223.6562 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 14ms/step - loss: 38075.8047 - accuracy: 0.0025\n","7/7 [==============================] - 1s 6ms/step - loss: 38477.8438 - accuracy: 0.0050\n","Epoch 1/50\n","13/13 [==============================] - 7s 14ms/step - loss: 52529.1602 - accuracy: 0.0000e+00\n","Epoch 2/50\n","13/13 [==============================] - 0s 15ms/step - loss: 49475.9531 - accuracy: 0.0025\n","Epoch 3/50\n","13/13 [==============================] - 0s 14ms/step - loss: 48846.5117 - accuracy: 0.0025\n","Epoch 4/50\n","13/13 [==============================] - 0s 16ms/step - loss: 48536.0000 - accuracy: 0.0025\n","Epoch 5/50\n","13/13 [==============================] - 0s 15ms/step - loss: 48237.7539 - accuracy: 0.0025\n","Epoch 6/50\n","13/13 [==============================] - 0s 16ms/step - loss: 48032.0469 - accuracy: 0.0025\n","Epoch 7/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47770.1953 - accuracy: 0.0025\n","Epoch 8/50\n","13/13 [==============================] - 0s 14ms/step - loss: 47538.0508 - accuracy: 0.0025\n","Epoch 9/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47328.9648 - accuracy: 0.0025\n","Epoch 10/50\n","13/13 [==============================] - 0s 15ms/step - loss: 47086.3867 - accuracy: 0.0025\n","Epoch 11/50\n","13/13 [==============================] - 0s 16ms/step - loss: 46869.9805 - accuracy: 0.0025\n","Epoch 12/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46577.0352 - accuracy: 0.0025\n","Epoch 13/50\n","13/13 [==============================] - 0s 15ms/step - loss: 46405.2344 - accuracy: 0.0025\n","Epoch 14/50\n","13/13 [==============================] - 0s 14ms/step - loss: 46156.0234 - accuracy: 0.0025\n","Epoch 15/50\n","13/13 [==============================] - 0s 14ms/step - loss: 45919.6289 - accuracy: 0.0025\n","Epoch 16/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45725.1953 - accuracy: 0.0025\n","Epoch 17/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45512.3555 - accuracy: 0.0025\n","Epoch 18/50\n","13/13 [==============================] - 0s 15ms/step - loss: 45233.7852 - accuracy: 0.0025\n","Epoch 19/50\n","13/13 [==============================] - 0s 16ms/step - loss: 45095.8086 - accuracy: 0.0025\n","Epoch 20/50\n","13/13 [==============================] - 0s 14ms/step - loss: 44783.8359 - accuracy: 0.0025\n","Epoch 21/50\n","13/13 [==============================] - 0s 16ms/step - loss: 44583.5586 - accuracy: 0.0025\n","Epoch 22/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44348.3711 - accuracy: 0.0025\n","Epoch 23/50\n","13/13 [==============================] - 0s 15ms/step - loss: 44077.6953 - accuracy: 0.0025\n","Epoch 24/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43836.4688 - accuracy: 0.0025\n","Epoch 25/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43612.3086 - accuracy: 0.0025\n","Epoch 26/50\n","13/13 [==============================] - 0s 15ms/step - loss: 43395.3008 - accuracy: 0.0025\n","Epoch 27/50\n","13/13 [==============================] - 0s 16ms/step - loss: 43129.3867 - accuracy: 0.0025\n","Epoch 28/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42934.7617 - accuracy: 0.0025\n","Epoch 29/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42727.4883 - accuracy: 0.0025\n","Epoch 30/50\n","13/13 [==============================] - 0s 14ms/step - loss: 42498.5508 - accuracy: 0.0025\n","Epoch 31/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42220.7695 - accuracy: 0.0025\n","Epoch 32/50\n","13/13 [==============================] - 0s 16ms/step - loss: 42048.2305 - accuracy: 0.0025\n","Epoch 33/50\n","13/13 [==============================] - 0s 16ms/step - loss: 41883.1719 - accuracy: 0.0025\n","Epoch 34/50\n","13/13 [==============================] - 0s 17ms/step - loss: 41525.1367 - accuracy: 0.0025\n","Epoch 35/50\n","13/13 [==============================] - 0s 14ms/step - loss: 41330.5469 - accuracy: 0.0025\n","Epoch 36/50\n","13/13 [==============================] - 0s 15ms/step - loss: 41200.0273 - accuracy: 0.0025\n","Epoch 37/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40981.6367 - accuracy: 0.0025\n","Epoch 38/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40794.6719 - accuracy: 0.0025\n","Epoch 39/50\n","13/13 [==============================] - 0s 17ms/step - loss: 40577.2891 - accuracy: 0.0025\n","Epoch 40/50\n","13/13 [==============================] - 0s 15ms/step - loss: 40300.2617 - accuracy: 0.0025\n","Epoch 41/50\n","13/13 [==============================] - 0s 16ms/step - loss: 40097.9844 - accuracy: 0.0025\n","Epoch 42/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39888.4453 - accuracy: 0.0025\n","Epoch 43/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39748.2695 - accuracy: 0.0025\n","Epoch 44/50\n","13/13 [==============================] - 0s 16ms/step - loss: 39419.6875 - accuracy: 0.0025\n","Epoch 45/50\n","13/13 [==============================] - 0s 15ms/step - loss: 39255.6367 - accuracy: 0.0025\n","Epoch 46/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38953.3633 - accuracy: 0.0025\n","Epoch 47/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38830.5469 - accuracy: 0.0025\n","Epoch 48/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38682.0859 - accuracy: 0.0025\n","Epoch 49/50\n","13/13 [==============================] - 0s 15ms/step - loss: 38375.2539 - accuracy: 0.0025\n","Epoch 50/50\n","13/13 [==============================] - 0s 16ms/step - loss: 38181.5039 - accuracy: 0.0025\n","7/7 [==============================] - 1s 5ms/step - loss: 36165.9531 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 5s 13ms/step - loss: 53098.0039 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 13ms/step - loss: 52871.3750 - accuracy: 0.0050\n","Epoch 3/8\n","10/10 [==============================] - 0s 13ms/step - loss: 51586.2109 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 14ms/step - loss: 49253.1289 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 15ms/step - loss: 48340.3516 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 14ms/step - loss: 47977.4844 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 13ms/step - loss: 47679.4297 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 14ms/step - loss: 47457.4336 - accuracy: 0.0025\n","5/5 [==============================] - 1s 6ms/step - loss: 49440.6016 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 5s 12ms/step - loss: 53100.4062 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 15ms/step - loss: 52895.3242 - accuracy: 0.0000e+00\n","Epoch 3/8\n","10/10 [==============================] - 0s 13ms/step - loss: 51765.4766 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 14ms/step - loss: 49636.1055 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48717.0352 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48318.8555 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 14ms/step - loss: 47976.0938 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 13ms/step - loss: 47733.8398 - accuracy: 0.0025\n","5/5 [==============================] - 1s 6ms/step - loss: 48285.8906 - accuracy: 0.0050\n","Epoch 1/8\n","10/10 [==============================] - 5s 12ms/step - loss: 53101.3398 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 14ms/step - loss: 52891.8086 - accuracy: 0.0050\n","Epoch 3/8\n","10/10 [==============================] - 0s 15ms/step - loss: 51827.0117 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 15ms/step - loss: 49797.9531 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48977.9219 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48605.9492 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 15ms/step - loss: 48349.7891 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48117.1289 - accuracy: 0.0025\n","5/5 [==============================] - 1s 6ms/step - loss: 45815.6289 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 5s 11ms/step - loss: 53133.5781 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.4844 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.4766 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.3516 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.2852 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.1992 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.1445 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.0117 - accuracy: 0.0025\n","5/5 [==============================] - 2s 7ms/step - loss: 55355.5156 - accuracy: 0.0050\n","Epoch 1/8\n","10/10 [==============================] - 5s 13ms/step - loss: 53134.1016 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 14ms/step - loss: 53134.0234 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.9414 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 14ms/step - loss: 53133.8047 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.7891 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.7031 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.6992 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.5195 - accuracy: 0.0025\n","5/5 [==============================] - 1s 7ms/step - loss: 53924.0469 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 6s 11ms/step - loss: 53133.0000 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.9453 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.8594 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.8359 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7031 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.6406 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.5938 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.4492 - accuracy: 0.0025\n","5/5 [==============================] - 1s 6ms/step - loss: 50858.8438 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 6s 13ms/step - loss: 52883.4531 - accuracy: 0.0050\n","Epoch 2/8\n","10/10 [==============================] - 0s 13ms/step - loss: 50093.4492 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 13ms/step - loss: 49110.0195 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48858.7461 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48653.9219 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 15ms/step - loss: 48392.6211 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48203.8945 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 14ms/step - loss: 47959.7109 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 50030.6914 - accuracy: 0.0000e+00\n","Epoch 1/8\n","10/10 [==============================] - 5s 12ms/step - loss: 52821.9062 - accuracy: 0.0025\n","Epoch 2/8\n","10/10 [==============================] - 0s 14ms/step - loss: 50180.7383 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 15ms/step - loss: 49294.6406 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 12ms/step - loss: 48930.2891 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48686.0938 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 14ms/step - loss: 48459.6211 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48296.6719 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 12ms/step - loss: 48073.5938 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 48697.6211 - accuracy: 0.0050\n","Epoch 1/8\n","10/10 [==============================] - 6s 11ms/step - loss: 52831.1016 - accuracy: 0.0050\n","Epoch 2/8\n","10/10 [==============================] - 0s 13ms/step - loss: 50119.5547 - accuracy: 0.0025\n","Epoch 3/8\n","10/10 [==============================] - 0s 13ms/step - loss: 49204.4688 - accuracy: 0.0025\n","Epoch 4/8\n","10/10 [==============================] - 0s 12ms/step - loss: 48905.8008 - accuracy: 0.0025\n","Epoch 5/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48647.6094 - accuracy: 0.0025\n","Epoch 6/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48445.1016 - accuracy: 0.0025\n","Epoch 7/8\n","10/10 [==============================] - 0s 13ms/step - loss: 48263.1836 - accuracy: 0.0025\n","Epoch 8/8\n","10/10 [==============================] - 0s 12ms/step - loss: 48071.0195 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 45784.3906 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 5s 14ms/step - loss: 53107.5117 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 12ms/step - loss: 52951.5898 - accuracy: 0.0000e+00\n","Epoch 3/30\n","10/10 [==============================] - 0s 13ms/step - loss: 52047.1055 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 50214.0508 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 14ms/step - loss: 49302.9609 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48920.0703 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 15ms/step - loss: 48632.1016 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 12ms/step - loss: 48425.6719 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48195.6719 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47943.2734 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47720.8984 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 15ms/step - loss: 47498.1406 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47270.3867 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47050.5117 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46823.0664 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46678.7617 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46440.5781 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46279.9609 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46040.3281 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45840.8203 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 15ms/step - loss: 45630.7383 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45496.6094 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45258.7695 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45140.1484 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44861.4609 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44722.6992 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44578.9219 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44340.0391 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44114.5156 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44024.7695 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 45952.7969 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 5s 13ms/step - loss: 53098.2461 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 14ms/step - loss: 52886.8789 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 13ms/step - loss: 51767.3047 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 14ms/step - loss: 49866.4414 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 13ms/step - loss: 49093.4102 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48736.6133 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48436.2031 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48228.4961 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47970.4609 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47686.8281 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47520.7305 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47305.6289 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47059.1953 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46890.7539 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46718.6914 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46417.7852 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46281.1055 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46126.0547 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45918.7109 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45714.9453 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45501.4766 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 15ms/step - loss: 45332.2266 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45120.4297 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44920.4961 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 16ms/step - loss: 44862.9805 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44615.6914 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44483.6914 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44286.8633 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44042.7812 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 43935.0117 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 44448.6836 - accuracy: 0.0050\n","Epoch 1/30\n","10/10 [==============================] - 5s 11ms/step - loss: 53103.0859 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 13ms/step - loss: 52906.6641 - accuracy: 0.0050\n","Epoch 3/30\n","10/10 [==============================] - 0s 14ms/step - loss: 51869.7734 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 49951.9219 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 14ms/step - loss: 49253.9609 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48879.6211 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48625.0117 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48372.9297 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 12ms/step - loss: 48143.7148 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47957.3164 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47640.1484 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 17ms/step - loss: 47439.4688 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47268.9648 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47027.5859 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46831.1406 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46619.8008 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 15ms/step - loss: 46427.4883 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46265.6289 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 15ms/step - loss: 46008.0039 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45843.6914 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 15ms/step - loss: 45630.4609 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45419.9805 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45360.5703 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45104.1992 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44930.7109 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 15ms/step - loss: 44741.1758 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44488.7188 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44344.5391 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44182.5508 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 15ms/step - loss: 44013.6758 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 41807.1641 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 6s 12ms/step - loss: 53133.7305 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.6211 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.5508 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.4492 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53133.4492 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.3086 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.2188 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.1055 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53133.0938 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.9141 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.8867 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.8242 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7852 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.6641 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.5586 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.4414 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.3867 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.2305 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.2461 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 15ms/step - loss: 53132.1094 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.0898 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.9961 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8438 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8086 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6836 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6133 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.4766 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.3164 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.2812 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.1758 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 55353.5859 - accuracy: 0.0050\n","Epoch 1/30\n","10/10 [==============================] - 5s 12ms/step - loss: 53133.3711 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.3438 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.2305 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.1562 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.1094 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53133.0312 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.9648 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.8359 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7812 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.5938 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.5781 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.4961 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.3438 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.3086 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.1758 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.1562 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.9609 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8242 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.7891 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.7031 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.6016 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.4453 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.4297 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.2656 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.1445 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.0195 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.9961 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.8789 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.7891 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.7305 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 53921.1641 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 5s 15ms/step - loss: 53132.5859 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.5234 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.4609 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.3438 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.3242 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.2305 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.1914 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.0898 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.0391 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.0508 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8281 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8438 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.7031 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6602 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6094 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.5664 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.4453 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.3086 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.2695 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.1992 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.1719 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.0469 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.9883 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.0547 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.8438 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.7305 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.6484 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.6797 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.4492 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.5547 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 50856.8789 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 6s 12ms/step - loss: 52889.1719 - accuracy: 0.0000e+00\n","Epoch 2/30\n","10/10 [==============================] - 0s 13ms/step - loss: 50067.1914 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 12ms/step - loss: 48960.8047 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48641.0898 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 12ms/step - loss: 48469.6602 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 12ms/step - loss: 48191.7383 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 12ms/step - loss: 47998.7148 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47811.5195 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47633.9492 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47427.2734 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 12ms/step - loss: 47248.6055 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47088.0312 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46861.3516 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46707.9492 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46476.8203 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 12ms/step - loss: 46313.6484 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46193.7031 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45974.2188 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45778.8047 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45667.1992 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45488.3711 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45318.6484 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45131.0586 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 12ms/step - loss: 44889.8359 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44683.6797 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44609.5859 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44369.3594 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44305.1133 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 12ms/step - loss: 44053.1562 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 43836.1992 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 45849.0312 - accuracy: 0.0000e+00\n","Epoch 1/30\n","10/10 [==============================] - 6s 13ms/step - loss: 52863.3945 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 15ms/step - loss: 49924.1250 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48877.6914 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48524.0664 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48330.4453 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48080.6719 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47982.2539 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47744.0352 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 15ms/step - loss: 47528.8398 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47396.9297 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47141.9102 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46995.2617 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46794.3438 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46619.8750 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46462.4688 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46251.4414 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46107.2891 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45936.7383 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45728.5742 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45597.0742 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45381.7266 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45239.3242 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 12ms/step - loss: 45052.2656 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44838.6445 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44658.4102 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44389.3203 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44297.8008 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44094.9844 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 13ms/step - loss: 43920.0781 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 12ms/step - loss: 43733.4531 - accuracy: 0.0025\n","5/5 [==============================] - 2s 6ms/step - loss: 44323.9805 - accuracy: 0.0050\n","Epoch 1/30\n","10/10 [==============================] - 5s 12ms/step - loss: 52900.6992 - accuracy: 0.0025\n","Epoch 2/30\n","10/10 [==============================] - 0s 14ms/step - loss: 50403.4141 - accuracy: 0.0025\n","Epoch 3/30\n","10/10 [==============================] - 0s 14ms/step - loss: 49519.0781 - accuracy: 0.0025\n","Epoch 4/30\n","10/10 [==============================] - 0s 14ms/step - loss: 49193.9219 - accuracy: 0.0025\n","Epoch 5/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48988.4961 - accuracy: 0.0025\n","Epoch 6/30\n","10/10 [==============================] - 0s 14ms/step - loss: 48763.8164 - accuracy: 0.0025\n","Epoch 7/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48537.7461 - accuracy: 0.0025\n","Epoch 8/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48337.6641 - accuracy: 0.0025\n","Epoch 9/30\n","10/10 [==============================] - 0s 13ms/step - loss: 48175.8203 - accuracy: 0.0025\n","Epoch 10/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47973.8906 - accuracy: 0.0025\n","Epoch 11/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47803.1055 - accuracy: 0.0025\n","Epoch 12/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47579.2969 - accuracy: 0.0025\n","Epoch 13/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47417.4688 - accuracy: 0.0025\n","Epoch 14/30\n","10/10 [==============================] - 0s 14ms/step - loss: 47207.0156 - accuracy: 0.0025\n","Epoch 15/30\n","10/10 [==============================] - 0s 13ms/step - loss: 47038.8633 - accuracy: 0.0025\n","Epoch 16/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46882.4648 - accuracy: 0.0025\n","Epoch 17/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46680.4258 - accuracy: 0.0025\n","Epoch 18/30\n","10/10 [==============================] - 0s 13ms/step - loss: 46500.9141 - accuracy: 0.0025\n","Epoch 19/30\n","10/10 [==============================] - 0s 15ms/step - loss: 46316.1602 - accuracy: 0.0025\n","Epoch 20/30\n","10/10 [==============================] - 0s 14ms/step - loss: 46113.3164 - accuracy: 0.0025\n","Epoch 21/30\n","10/10 [==============================] - 0s 13ms/step - loss: 45881.4844 - accuracy: 0.0025\n","Epoch 22/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45779.8086 - accuracy: 0.0025\n","Epoch 23/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45598.0703 - accuracy: 0.0025\n","Epoch 24/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45341.6211 - accuracy: 0.0025\n","Epoch 25/30\n","10/10 [==============================] - 0s 14ms/step - loss: 45245.6562 - accuracy: 0.0025\n","Epoch 26/30\n","10/10 [==============================] - 0s 15ms/step - loss: 45037.9844 - accuracy: 0.0025\n","Epoch 27/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44838.3086 - accuracy: 0.0025\n","Epoch 28/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44651.8398 - accuracy: 0.0025\n","Epoch 29/30\n","10/10 [==============================] - 0s 14ms/step - loss: 44442.4297 - accuracy: 0.0025\n","Epoch 30/30\n","10/10 [==============================] - 0s 13ms/step - loss: 44339.3789 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 42134.4492 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 5s 13ms/step - loss: 53101.0234 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 15ms/step - loss: 52892.5898 - accuracy: 0.0050\n","Epoch 3/50\n","10/10 [==============================] - 0s 14ms/step - loss: 51765.3945 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 14ms/step - loss: 49552.1094 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48720.1133 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48355.6289 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48089.5703 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 15ms/step - loss: 47776.8711 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 15ms/step - loss: 47566.1406 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47326.3711 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47120.7109 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46898.3750 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 15ms/step - loss: 46655.8711 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46494.2500 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46247.8086 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46093.3438 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45830.5000 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45702.4883 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45480.6367 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45302.1289 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45127.3789 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44911.8867 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44649.6992 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44484.8906 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44308.1836 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44133.8594 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44080.1055 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43786.2305 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43618.2031 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43388.8281 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 15ms/step - loss: 43257.8242 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43131.8516 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 15ms/step - loss: 42929.0586 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 15ms/step - loss: 42692.1914 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 15ms/step - loss: 42561.6914 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42326.5312 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42220.1289 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42040.3164 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41765.1641 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41734.2383 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41526.3164 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41357.5664 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 15ms/step - loss: 41151.9766 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41064.2109 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 15ms/step - loss: 40916.1094 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40676.7344 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40554.3906 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 15ms/step - loss: 40447.9297 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40271.5352 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40159.1758 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 41981.1562 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 6s 15ms/step - loss: 53097.5898 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 14ms/step - loss: 52875.1016 - accuracy: 0.0050\n","Epoch 3/50\n","10/10 [==============================] - 0s 15ms/step - loss: 51610.2344 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 13ms/step - loss: 49436.9688 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48583.5039 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48182.1133 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47929.4805 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47665.3633 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47395.3398 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47178.1953 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46995.0117 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46798.3789 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 12ms/step - loss: 46540.6797 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46360.9805 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46125.6133 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45946.8086 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45756.9609 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45606.0547 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45407.2539 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45167.2188 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44917.2031 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44806.1367 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44595.2109 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44414.8867 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44226.8203 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44119.5742 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43872.0898 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 12ms/step - loss: 43788.6641 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 15ms/step - loss: 43550.2461 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43331.5859 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43152.1641 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43018.5586 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42829.0469 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42679.6992 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42401.6758 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42240.0352 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42031.2109 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41955.6289 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41839.2617 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41634.6133 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41508.4883 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41225.3164 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41040.0859 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 15ms/step - loss: 41007.5547 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40790.8398 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40643.9297 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40525.3750 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40332.5547 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40192.1133 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 14ms/step - loss: 39957.5234 - accuracy: 0.0025\n","5/5 [==============================] - 2s 6ms/step - loss: 40498.1094 - accuracy: 0.0050\n","Epoch 1/50\n","10/10 [==============================] - 5s 13ms/step - loss: 53099.3984 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 14ms/step - loss: 52873.2734 - accuracy: 0.0000e+00\n","Epoch 3/50\n","10/10 [==============================] - 0s 15ms/step - loss: 51610.6484 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 14ms/step - loss: 49300.7617 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48582.0000 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48219.7109 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47947.3984 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47680.0000 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47465.6016 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47225.4766 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47013.3789 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46822.9805 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46600.9062 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 15ms/step - loss: 46421.9492 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46146.2188 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 15ms/step - loss: 45989.4102 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45799.7695 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45584.0000 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45363.5586 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45181.6406 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 15ms/step - loss: 45029.4297 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44777.3750 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44654.8164 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44404.5195 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44335.5508 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44092.5547 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43845.7617 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 15ms/step - loss: 43739.5586 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43521.1992 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43423.1797 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43251.8242 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42989.8711 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42752.2305 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 15ms/step - loss: 42647.3594 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 15ms/step - loss: 42499.6445 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42302.4531 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42228.0234 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41999.2969 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41755.7109 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41663.7539 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41519.0117 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41315.6641 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41160.7031 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40961.3281 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40817.9062 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40672.2891 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40491.1758 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40245.2148 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40089.9844 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 15ms/step - loss: 39972.5156 - accuracy: 0.0025\n","5/5 [==============================] - 1s 5ms/step - loss: 37930.0898 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 6s 12ms/step - loss: 53133.2109 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53133.1289 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53133.0352 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.9062 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7500 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7461 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.5195 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.4844 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 15ms/step - loss: 53132.4062 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.3008 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.1406 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.1055 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53132.0000 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.9219 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.8438 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6992 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.5586 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.4531 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.3750 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.2383 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.1133 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.0312 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.8359 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 15ms/step - loss: 53130.8906 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.6406 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.5312 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.4531 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.3164 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.3047 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.0508 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.9062 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.8867 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.6562 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.5508 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.3906 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53129.3047 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.1914 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.0234 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.8789 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.7695 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.5938 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.4766 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.3789 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53128.1719 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.0703 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.8594 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53127.8359 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53127.5898 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.5391 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.3203 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 55349.5898 - accuracy: 0.0050\n","Epoch 1/50\n","10/10 [==============================] - 5s 12ms/step - loss: 53132.1055 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.0469 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.9688 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.8711 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.7383 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6914 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.6211 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.5000 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.4141 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.3398 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.2617 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.1602 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.1484 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.9883 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.8945 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.8516 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 15ms/step - loss: 53130.7266 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.6641 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.5391 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.4062 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.2969 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.2305 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.2148 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.0508 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.9961 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53129.8203 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.8398 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.6406 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.5117 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.3594 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.3438 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53129.1992 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.1641 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.0117 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.8281 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.7617 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.6602 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.5312 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53127.6445 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53127.4219 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53127.2344 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53127.1797 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.0391 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 53917.5000 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 5s 12ms/step - loss: 53133.0547 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.9258 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.7109 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.6562 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.5703 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.5312 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.3555 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.3359 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53132.2109 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53132.1484 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.9766 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.8945 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.8281 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.6289 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53131.6602 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.5547 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.4297 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.3594 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53131.1133 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53131.0547 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.9492 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.8906 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.7539 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.5898 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.4336 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.4453 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53130.2305 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53130.2383 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53130.0664 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53129.9492 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.8281 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53129.6445 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 12ms/step - loss: 53129.5703 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.4688 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53129.3516 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.2344 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53129.0312 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.9297 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.8750 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.6758 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.4219 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.4141 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53128.2695 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.0703 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53128.0547 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.8711 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53127.7305 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.7266 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 13ms/step - loss: 53127.5391 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 14ms/step - loss: 53127.2656 - accuracy: 0.0025\n","5/5 [==============================] - 2s 6ms/step - loss: 50853.7969 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 5s 13ms/step - loss: 52893.8711 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 15ms/step - loss: 49970.0742 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48933.2695 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 12ms/step - loss: 48597.8398 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48397.9336 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48158.6562 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 15ms/step - loss: 47994.9102 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47756.7148 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47585.0547 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47374.0859 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 12ms/step - loss: 47200.6719 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47030.5742 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46802.3516 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46658.2500 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 15ms/step - loss: 46471.0703 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46236.9297 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46044.0938 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 12ms/step - loss: 45873.5156 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45724.8633 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45544.7852 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45379.5234 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45166.7383 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44965.7891 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44806.2500 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44681.8711 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44468.6953 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44290.1367 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44097.3359 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43908.0742 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 15ms/step - loss: 43697.1211 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43596.3906 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43368.4453 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43190.1484 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43063.5938 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42900.8047 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42739.8398 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42574.5664 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42424.5156 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42208.4648 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42007.2539 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41918.6367 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41726.9141 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41474.9961 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41321.1406 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41174.8867 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40941.8906 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40760.4414 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40601.9062 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40586.5312 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40353.0273 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 42252.4062 - accuracy: 0.0000e+00\n","Epoch 1/50\n","10/10 [==============================] - 6s 12ms/step - loss: 52804.4688 - accuracy: 0.0000e+00\n","Epoch 2/50\n","10/10 [==============================] - 0s 14ms/step - loss: 50246.6641 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 14ms/step - loss: 49436.2266 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 13ms/step - loss: 49129.1055 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 13ms/step - loss: 48942.0586 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48693.8984 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48542.4883 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 13ms/step - loss: 48306.4219 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 48101.4219 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47915.9883 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47755.8203 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47567.2617 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47378.4883 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47178.5508 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46947.5117 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46833.3750 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46590.7656 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46378.8633 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46276.1484 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46080.7148 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 15ms/step - loss: 45897.9062 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45756.0781 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45539.9766 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45409.2812 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45171.9414 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44988.8281 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44807.8594 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44588.0312 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44409.0000 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44286.9219 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44086.1562 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43886.6406 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43795.8555 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43601.5195 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43319.9531 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 15ms/step - loss: 43165.2148 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42958.3203 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42897.6016 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42683.4531 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42531.7695 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42353.9102 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42204.9062 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42053.6602 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41784.6719 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41619.8008 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41493.4766 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41399.5312 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41218.6914 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41045.9492 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 13ms/step - loss: 40896.5547 - accuracy: 0.0025\n","5/5 [==============================] - 2s 5ms/step - loss: 41355.9844 - accuracy: 0.0050\n","Epoch 1/50\n","10/10 [==============================] - 6s 14ms/step - loss: 52864.8359 - accuracy: 0.0025\n","Epoch 2/50\n","10/10 [==============================] - 0s 15ms/step - loss: 50102.4961 - accuracy: 0.0025\n","Epoch 3/50\n","10/10 [==============================] - 0s 14ms/step - loss: 49092.3906 - accuracy: 0.0025\n","Epoch 4/50\n","10/10 [==============================] - 0s 13ms/step - loss: 48797.2188 - accuracy: 0.0025\n","Epoch 5/50\n","10/10 [==============================] - 0s 14ms/step - loss: 48545.5117 - accuracy: 0.0025\n","Epoch 6/50\n","10/10 [==============================] - 0s 13ms/step - loss: 48327.3555 - accuracy: 0.0025\n","Epoch 7/50\n","10/10 [==============================] - 0s 15ms/step - loss: 48133.8047 - accuracy: 0.0025\n","Epoch 8/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47899.2031 - accuracy: 0.0025\n","Epoch 9/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47754.7695 - accuracy: 0.0025\n","Epoch 10/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47553.0547 - accuracy: 0.0025\n","Epoch 11/50\n","10/10 [==============================] - 0s 13ms/step - loss: 47372.1602 - accuracy: 0.0025\n","Epoch 12/50\n","10/10 [==============================] - 0s 14ms/step - loss: 47170.6758 - accuracy: 0.0025\n","Epoch 13/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46968.9102 - accuracy: 0.0025\n","Epoch 14/50\n","10/10 [==============================] - 0s 14ms/step - loss: 46836.4492 - accuracy: 0.0025\n","Epoch 15/50\n","10/10 [==============================] - 0s 12ms/step - loss: 46627.0508 - accuracy: 0.0025\n","Epoch 16/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46449.7500 - accuracy: 0.0025\n","Epoch 17/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46265.5938 - accuracy: 0.0025\n","Epoch 18/50\n","10/10 [==============================] - 0s 13ms/step - loss: 46077.7148 - accuracy: 0.0025\n","Epoch 19/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45831.4062 - accuracy: 0.0025\n","Epoch 20/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45706.0547 - accuracy: 0.0025\n","Epoch 21/50\n","10/10 [==============================] - 0s 14ms/step - loss: 45561.8867 - accuracy: 0.0025\n","Epoch 22/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45356.0039 - accuracy: 0.0025\n","Epoch 23/50\n","10/10 [==============================] - 0s 13ms/step - loss: 45132.0703 - accuracy: 0.0025\n","Epoch 24/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44966.9062 - accuracy: 0.0025\n","Epoch 25/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44847.4805 - accuracy: 0.0025\n","Epoch 26/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44624.6250 - accuracy: 0.0025\n","Epoch 27/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44484.4258 - accuracy: 0.0025\n","Epoch 28/50\n","10/10 [==============================] - 0s 14ms/step - loss: 44226.1211 - accuracy: 0.0025\n","Epoch 29/50\n","10/10 [==============================] - 0s 13ms/step - loss: 44129.4648 - accuracy: 0.0025\n","Epoch 30/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43981.1758 - accuracy: 0.0025\n","Epoch 31/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43750.6602 - accuracy: 0.0025\n","Epoch 32/50\n","10/10 [==============================] - 0s 14ms/step - loss: 43529.4688 - accuracy: 0.0025\n","Epoch 33/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43364.0156 - accuracy: 0.0025\n","Epoch 34/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43169.4961 - accuracy: 0.0025\n","Epoch 35/50\n","10/10 [==============================] - 0s 13ms/step - loss: 43062.6289 - accuracy: 0.0025\n","Epoch 36/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42868.2656 - accuracy: 0.0025\n","Epoch 37/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42704.7344 - accuracy: 0.0025\n","Epoch 38/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42489.8594 - accuracy: 0.0025\n","Epoch 39/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42322.4219 - accuracy: 0.0025\n","Epoch 40/50\n","10/10 [==============================] - 0s 13ms/step - loss: 42134.9414 - accuracy: 0.0025\n","Epoch 41/50\n","10/10 [==============================] - 0s 14ms/step - loss: 42036.6641 - accuracy: 0.0025\n","Epoch 42/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41877.9453 - accuracy: 0.0025\n","Epoch 43/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41739.5820 - accuracy: 0.0025\n","Epoch 44/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41575.0469 - accuracy: 0.0025\n","Epoch 45/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41350.8789 - accuracy: 0.0025\n","Epoch 46/50\n","10/10 [==============================] - 0s 13ms/step - loss: 41121.3398 - accuracy: 0.0025\n","Epoch 47/50\n","10/10 [==============================] - 0s 14ms/step - loss: 41024.0430 - accuracy: 0.0025\n","Epoch 48/50\n","10/10 [==============================] - 0s 15ms/step - loss: 40858.7266 - accuracy: 0.0025\n","Epoch 49/50\n","10/10 [==============================] - 0s 12ms/step - loss: 40646.4453 - accuracy: 0.0025\n","Epoch 50/50\n","10/10 [==============================] - 0s 14ms/step - loss: 40409.6133 - accuracy: 0.0025\n","5/5 [==============================] - 2s 6ms/step - loss: 38417.2383 - accuracy: 0.0000e+00\n","Epoch 1/8\n","30/30 [==============================] - 5s 11ms/step - loss: 119027.2969 - accuracy: 0.0033\n","Epoch 2/8\n","30/30 [==============================] - 0s 13ms/step - loss: 113926.9219 - accuracy: 0.0017\n","Epoch 3/8\n","30/30 [==============================] - 0s 11ms/step - loss: 112215.9453 - accuracy: 0.0017\n","Epoch 4/8\n","30/30 [==============================] - 0s 12ms/step - loss: 111176.1484 - accuracy: 0.0017\n","Epoch 5/8\n","30/30 [==============================] - 0s 12ms/step - loss: 110128.5234 - accuracy: 0.0017\n","Epoch 6/8\n","30/30 [==============================] - 0s 12ms/step - loss: 109201.0391 - accuracy: 0.0017\n","Epoch 7/8\n","30/30 [==============================] - 0s 12ms/step - loss: 108286.8516 - accuracy: 0.0017\n","Epoch 8/8\n","30/30 [==============================] - 0s 12ms/step - loss: 107387.0234 - accuracy: 0.0017\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=3,\n","             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f11e2e0c990>,\n","             param_grid={'batch_size': [20, 32, 40], 'epochs': [8, 30, 50],\n","                         'optimizer': ['adam', 'Adadelta', 'rmsprop']})"]},"metadata":{},"execution_count":20}],"source":["LSTM_grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVlUL3mi3M3S","outputId":"85bdac1d-aab0-4c97-f737-64f5d6445133"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best score across ALL searched params:\n"," 0.0016666666294137638\n","\n"," The best parameters across ALL searched params:\n"," {'batch_size': 20, 'epochs': 8, 'optimizer': 'adam'}\n"]}],"source":["print(\" Results from Grid Search \" )\n","# print(\"\\n The best estimator across ALL searched params:\\n\",grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",LSTM_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",LSTM_grid_search.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"KcrX4e4f3QiN"},"source":["with the test part of data set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puWE3iEbxg_7","outputId":"73ce806f-d6a7-4dd1-b620-0b06732b2472"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/8\n","14/14 [==============================] - 7s 12ms/step - loss: 23598.7930 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23231.5098 - accuracy: 0.0075\n","Epoch 3/8\n","14/14 [==============================] - 0s 14ms/step - loss: 21645.0664 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 12ms/step - loss: 20874.0645 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 14ms/step - loss: 20609.2617 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20395.9551 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20176.2461 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 14ms/step - loss: 19982.0566 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 22401.0254 - accuracy: 0.0000e+00\n","Epoch 1/8\n","14/14 [==============================] - 8s 12ms/step - loss: 23608.2715 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23339.3574 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 14ms/step - loss: 21962.9512 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 13ms/step - loss: 21168.1816 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 14ms/step - loss: 20886.0586 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20639.9434 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20423.2754 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 14ms/step - loss: 20243.9316 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 20239.9336 - accuracy: 0.0075\n","Epoch 1/8\n","14/14 [==============================] - 8s 12ms/step - loss: 23771.8867 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23366.6230 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 12ms/step - loss: 21722.7305 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 14ms/step - loss: 20946.8340 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20670.0059 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20427.1973 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20230.4609 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20035.5156 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 17688.0391 - accuracy: 0.0000e+00\n","Epoch 1/8\n","14/14 [==============================] - 7s 11ms/step - loss: 23629.1133 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.0977 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.9648 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.9453 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.8809 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.7793 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.7246 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.6680 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 26479.9961 - accuracy: 0.0000e+00\n","Epoch 1/8\n","14/14 [==============================] - 7s 12ms/step - loss: 23629.9551 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.8320 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.8164 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.6699 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.6250 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.5312 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.4570 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.3633 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 23738.2363 - accuracy: 0.0000e+00\n","Epoch 1/8\n","14/14 [==============================] - 7s 11ms/step - loss: 23807.7480 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.7031 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.6816 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.5879 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.5293 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.5000 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.4043 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.3516 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 21310.3574 - accuracy: 0.0075\n","Epoch 1/8\n","14/14 [==============================] - 8s 11ms/step - loss: 23117.5195 - accuracy: 0.0075\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 21144.8301 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20785.8301 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20605.3086 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20406.0820 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 12ms/step - loss: 20234.6641 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 12ms/step - loss: 20033.3750 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 12ms/step - loss: 19933.7363 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 22327.8633 - accuracy: 0.0000e+00\n","Epoch 1/8\n","14/14 [==============================] - 8s 12ms/step - loss: 23219.5273 - accuracy: 0.0037\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 21192.9043 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20779.9355 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20614.8262 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20391.1719 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20222.3359 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20020.8984 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 12ms/step - loss: 19894.9434 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 19942.7734 - accuracy: 0.0075\n","Epoch 1/8\n","14/14 [==============================] - 7s 13ms/step - loss: 23390.4492 - accuracy: 0.0000e+00\n","Epoch 2/8\n","14/14 [==============================] - 0s 12ms/step - loss: 21606.0898 - accuracy: 0.0037\n","Epoch 3/8\n","14/14 [==============================] - 0s 14ms/step - loss: 21234.6133 - accuracy: 0.0037\n","Epoch 4/8\n","14/14 [==============================] - 0s 13ms/step - loss: 21028.8359 - accuracy: 0.0037\n","Epoch 5/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20831.4297 - accuracy: 0.0037\n","Epoch 6/8\n","14/14 [==============================] - 0s 14ms/step - loss: 20678.2793 - accuracy: 0.0037\n","Epoch 7/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20486.6191 - accuracy: 0.0037\n","Epoch 8/8\n","14/14 [==============================] - 0s 13ms/step - loss: 20283.2969 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 17960.6328 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 8s 12ms/step - loss: 23602.2109 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23275.9648 - accuracy: 0.0000e+00\n","Epoch 3/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21835.8301 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 12ms/step - loss: 21052.0156 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20780.5254 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20540.6973 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20322.5918 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 12ms/step - loss: 20184.0566 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 12ms/step - loss: 19945.3301 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 12ms/step - loss: 19775.0996 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19573.8418 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19381.7891 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19225.7031 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19002.7695 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18884.7852 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18720.3770 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18564.6055 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18360.2363 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18280.5469 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18074.7988 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17980.2422 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17809.5898 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17629.0938 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17516.6777 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17367.5723 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17189.3438 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17111.0664 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16917.3359 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16714.1582 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 12ms/step - loss: 16585.2051 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 18772.9062 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 7s 12ms/step - loss: 23591.5293 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23144.9980 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21477.2188 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20815.9746 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20524.7773 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20311.4648 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20107.5898 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19877.7695 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19694.2129 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19495.7578 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19317.6289 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 12ms/step - loss: 19168.4141 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18998.5488 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18795.2109 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18700.6172 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18520.8809 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18270.5781 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18160.3164 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17984.8613 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17831.0293 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17762.6777 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 12ms/step - loss: 17586.3184 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17449.6328 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17243.2305 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17156.9414 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 12ms/step - loss: 17011.9512 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16879.6270 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16725.1113 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16534.8730 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16479.6113 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 16502.6113 - accuracy: 0.0075\n","Epoch 1/30\n","14/14 [==============================] - 8s 12ms/step - loss: 23779.9727 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23412.3379 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21771.7676 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20951.5547 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20639.2832 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20404.3008 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20221.8789 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19995.7969 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19784.7910 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19619.4629 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19429.1270 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19267.0820 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19068.0176 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18893.3594 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18738.4395 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18546.4023 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18386.6016 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18234.4941 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18099.5293 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17908.0645 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17738.7871 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17632.7656 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 12ms/step - loss: 17435.3711 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17310.1309 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17194.4395 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17092.0195 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16874.6719 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16700.8906 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16587.1953 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 15ms/step - loss: 16491.6777 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 14422.2803 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 7s 12ms/step - loss: 23629.7031 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.6035 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.5508 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.4766 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.4453 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.3418 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.2949 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.2246 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.1699 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.0684 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.9961 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.9199 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.8477 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.7949 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.7070 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.6328 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.5371 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.4844 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.3457 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.2891 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.2480 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.1562 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.0625 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.9570 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.8730 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.8223 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.6992 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.6387 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.5156 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.4629 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 26478.6816 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 7s 11ms/step - loss: 23630.0801 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23630.0664 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.9570 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.8184 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.7441 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.6543 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.5605 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.5039 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.4688 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.3633 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.2441 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23629.2207 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.1387 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23629.0742 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23628.8945 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.8711 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.7559 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.6133 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.6387 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.5000 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.3789 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.3379 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.2422 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.2031 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23628.0664 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.8906 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.8027 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23627.6992 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.6816 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.6055 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 23736.3926 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 8s 12ms/step - loss: 23808.4004 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23808.3164 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23808.2227 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23808.2070 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23808.0977 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.9883 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.9688 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.8730 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.8770 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.6855 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.6621 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.5879 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.5000 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23807.4004 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.3516 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23807.2227 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.1113 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.0391 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 12ms/step - loss: 23806.9688 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.8594 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.8320 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.7285 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.6738 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.5625 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.4883 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 15ms/step - loss: 23806.4238 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.3281 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.2090 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.1426 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 13ms/step - loss: 23806.0371 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 21309.1055 - accuracy: 0.0075\n","Epoch 1/30\n","14/14 [==============================] - 7s 12ms/step - loss: 23178.0176 - accuracy: 0.0075\n","Epoch 2/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21233.7402 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20852.8359 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20631.2949 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 12ms/step - loss: 20487.2090 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20273.3223 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20148.8184 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19955.7363 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19766.8164 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19614.9102 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 12ms/step - loss: 19445.9453 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19270.1855 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19150.8926 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18956.0898 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18743.0977 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18664.3828 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18480.1875 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18334.7891 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18219.8828 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18047.2793 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17917.3926 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17746.5312 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17532.1582 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17450.0371 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17297.9746 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17118.8066 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16953.4805 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16853.2949 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 15ms/step - loss: 16677.3613 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16637.2168 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 18696.9531 - accuracy: 0.0000e+00\n","Epoch 1/30\n","14/14 [==============================] - 7s 12ms/step - loss: 23180.2109 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21088.2500 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20746.4688 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 13ms/step - loss: 20586.4727 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20355.6914 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20210.9688 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20006.1836 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19925.8262 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19721.0488 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19490.3633 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19385.8496 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19202.3574 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19080.9863 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18913.3984 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18747.1211 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18550.0938 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18399.2324 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18255.7012 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18096.8477 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17984.3281 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17824.6387 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17638.0391 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17527.5859 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17356.1562 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 15ms/step - loss: 17216.8848 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17011.9219 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16941.3125 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16743.2695 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16609.9355 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 14ms/step - loss: 16489.4316 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 16563.8496 - accuracy: 0.0075\n","Epoch 1/30\n","14/14 [==============================] - 8s 13ms/step - loss: 23326.4785 - accuracy: 0.0037\n","Epoch 2/30\n","14/14 [==============================] - 0s 13ms/step - loss: 21254.6191 - accuracy: 0.0037\n","Epoch 3/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20851.0586 - accuracy: 0.0037\n","Epoch 4/30\n","14/14 [==============================] - 0s 12ms/step - loss: 20641.1855 - accuracy: 0.0037\n","Epoch 5/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20456.7695 - accuracy: 0.0037\n","Epoch 6/30\n","14/14 [==============================] - 0s 15ms/step - loss: 20292.1504 - accuracy: 0.0037\n","Epoch 7/30\n","14/14 [==============================] - 0s 14ms/step - loss: 20155.8242 - accuracy: 0.0037\n","Epoch 8/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19921.8828 - accuracy: 0.0037\n","Epoch 9/30\n","14/14 [==============================] - 0s 12ms/step - loss: 19775.6816 - accuracy: 0.0037\n","Epoch 10/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19611.8516 - accuracy: 0.0037\n","Epoch 11/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19451.6289 - accuracy: 0.0037\n","Epoch 12/30\n","14/14 [==============================] - 0s 14ms/step - loss: 19284.7168 - accuracy: 0.0037\n","Epoch 13/30\n","14/14 [==============================] - 0s 13ms/step - loss: 19129.3887 - accuracy: 0.0037\n","Epoch 14/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18933.5430 - accuracy: 0.0037\n","Epoch 15/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18827.3008 - accuracy: 0.0037\n","Epoch 16/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18670.8730 - accuracy: 0.0037\n","Epoch 17/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18524.8125 - accuracy: 0.0037\n","Epoch 18/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18357.2949 - accuracy: 0.0037\n","Epoch 19/30\n","14/14 [==============================] - 0s 13ms/step - loss: 18187.9375 - accuracy: 0.0037\n","Epoch 20/30\n","14/14 [==============================] - 0s 14ms/step - loss: 18016.3613 - accuracy: 0.0037\n","Epoch 21/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17915.5977 - accuracy: 0.0037\n","Epoch 22/30\n","14/14 [==============================] - 0s 13ms/step - loss: 17715.8027 - accuracy: 0.0037\n","Epoch 23/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17618.0449 - accuracy: 0.0037\n","Epoch 24/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17449.0547 - accuracy: 0.0037\n","Epoch 25/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17265.5410 - accuracy: 0.0037\n","Epoch 26/30\n","14/14 [==============================] - 0s 14ms/step - loss: 17144.0430 - accuracy: 0.0037\n","Epoch 27/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16996.4531 - accuracy: 0.0037\n","Epoch 28/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16873.9434 - accuracy: 0.0037\n","Epoch 29/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16735.6680 - accuracy: 0.0037\n","Epoch 30/30\n","14/14 [==============================] - 0s 13ms/step - loss: 16576.9375 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 14497.2686 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 7s 13ms/step - loss: 23592.1309 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23133.6465 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 15ms/step - loss: 21345.2910 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20630.0488 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20381.1680 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20066.1387 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19932.0156 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19694.3164 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19532.3828 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19371.5176 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19201.7676 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18977.3105 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 13ms/step - loss: 18805.1504 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18666.0742 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18489.9355 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18280.3965 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18123.0527 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18018.4414 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17868.5137 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 13ms/step - loss: 17719.4512 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17580.2344 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17373.7910 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17273.2188 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17113.2363 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16991.6387 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 13ms/step - loss: 16813.9980 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16666.5469 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16509.5195 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16382.2080 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16217.0615 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16165.0576 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 13ms/step - loss: 16009.5244 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15835.7861 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15735.8428 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15657.7910 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15511.0469 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15355.9648 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15251.6084 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15159.7695 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 13ms/step - loss: 14924.5098 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14847.5879 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14705.7881 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14592.9688 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14540.9805 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14455.1357 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14255.5781 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14153.2910 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14107.7939 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 14ms/step - loss: 13879.6484 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 13813.0225 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 15685.8975 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 8s 14ms/step - loss: 23603.1113 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 13ms/step - loss: 23307.4551 - accuracy: 0.0000e+00\n","Epoch 3/50\n","14/14 [==============================] - 0s 15ms/step - loss: 21817.5684 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 21056.0820 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20792.9766 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20567.7305 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20351.6309 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 13ms/step - loss: 20146.2637 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19974.7773 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19735.5645 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19569.8730 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 13ms/step - loss: 19354.3828 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 13ms/step - loss: 19242.9238 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19103.3574 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18836.1836 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18727.2617 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18585.4023 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18378.2383 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18216.7656 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18074.4336 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17897.3789 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17780.3535 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17592.3398 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17478.8496 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17362.7207 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 16ms/step - loss: 17170.1523 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17002.0898 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16917.4551 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16795.0234 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16696.5840 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16505.2324 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16366.0693 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16177.4648 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16066.6816 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15904.8066 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15820.9346 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15636.6250 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15519.3535 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15383.9990 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15322.9902 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15168.9443 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15060.7998 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14886.8174 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14804.5400 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14674.9902 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14522.6865 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14481.7480 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 17ms/step - loss: 14351.9277 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14287.4658 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14171.2754 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 14233.0234 - accuracy: 0.0075\n","Epoch 1/50\n","14/14 [==============================] - 8s 14ms/step - loss: 23774.1973 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23371.6797 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 13ms/step - loss: 21658.2891 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20838.6855 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20568.1035 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20297.8145 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20099.4258 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19918.6699 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19673.6699 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19523.3340 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19357.8281 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19159.6895 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18984.3086 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18805.6582 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18651.8555 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18511.8711 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18325.3086 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18162.2402 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17946.8945 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17868.3398 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17686.1367 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17556.2812 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17359.4023 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17288.1152 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17164.5645 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16955.3340 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16848.3359 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16687.2168 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16550.5508 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16398.7695 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16327.1533 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16201.8213 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15997.4424 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15930.5732 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15756.2334 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15569.5654 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15542.8613 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15385.3760 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15235.7686 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 14ms/step - loss: 15156.1426 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14998.0879 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14894.2012 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14787.0518 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14702.9766 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14495.7822 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14421.9463 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14278.7080 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14231.7705 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 15ms/step - loss: 13987.9150 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 13894.6748 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 12148.8623 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 7s 13ms/step - loss: 23629.2852 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.1211 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.0938 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.0586 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.9590 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.8242 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.8164 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.7539 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.6055 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.5469 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.4785 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.3867 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.3066 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.2188 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.0996 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.0488 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.9805 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.8887 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.7520 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.7500 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.5820 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.5527 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.3652 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.3574 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.2070 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.1191 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.0762 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.9707 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.9043 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.7871 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.6484 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.5566 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.5059 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.3691 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.2754 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.0527 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 13ms/step - loss: 23625.9766 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.9414 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.7910 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 13ms/step - loss: 23625.6602 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.5234 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.3887 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23625.3398 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23625.1426 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 13ms/step - loss: 23625.0977 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23624.9707 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23624.8145 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23624.6602 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23624.5352 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23624.5078 - accuracy: 0.0037\n","7/7 [==============================] - 2s 4ms/step - loss: 26475.3887 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 8s 14ms/step - loss: 23629.8066 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.8086 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.6621 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.6406 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.5898 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.4766 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.4082 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.3008 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23629.2754 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23629.1465 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.1055 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23629.0020 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.8555 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.7559 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.7109 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.6152 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.5586 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.4746 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.3652 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23628.2617 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23628.1484 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23628.1543 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.9941 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.9316 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.7988 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.7129 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.5898 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.4531 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.4590 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23627.3262 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.1777 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23627.0430 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23627.0312 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.8770 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.7773 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.6270 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23626.5723 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.4668 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.2871 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.2559 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23626.1191 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23625.9883 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.8652 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23625.7012 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.6211 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.4727 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23625.3867 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23625.2168 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23625.1719 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23625.0332 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 23733.8320 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 7s 14ms/step - loss: 23807.8125 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.7715 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23807.6543 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.5879 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23807.5352 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.4277 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.3750 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23807.2988 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23807.2266 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23807.1465 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23807.1035 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23806.9648 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.9258 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23806.8203 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.7871 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.6992 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23806.5996 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.5156 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23806.4082 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.3066 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.2578 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23806.1797 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23806.0410 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23805.9922 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.8496 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23805.7891 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.7773 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.6621 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.5566 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.5273 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23805.4336 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23805.2480 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.1797 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23805.1172 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23804.9570 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23804.8730 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23804.7949 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23804.6719 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23804.5508 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23804.4297 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23804.3418 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23804.2715 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23804.0996 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23803.9824 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23803.8613 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23803.7480 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 16ms/step - loss: 23803.6562 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23803.4570 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 14ms/step - loss: 23803.3965 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 15ms/step - loss: 23803.2559 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 21306.5781 - accuracy: 0.0075\n","Epoch 1/50\n","14/14 [==============================] - 9s 14ms/step - loss: 23158.7617 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 15ms/step - loss: 21204.9922 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20829.6035 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 16ms/step - loss: 20633.5781 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20444.9121 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20252.7871 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20123.9629 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19915.1211 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19801.0898 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19603.1484 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19470.2598 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19303.0957 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19140.3730 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18951.7188 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18815.1738 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 16ms/step - loss: 18663.6504 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18499.3262 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 17ms/step - loss: 18331.2012 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18225.5898 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18026.6816 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17952.1191 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 16ms/step - loss: 17748.8828 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17587.9297 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17464.6582 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17207.1875 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17127.7832 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 16ms/step - loss: 17033.1523 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16896.4531 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16686.2148 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16569.5469 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16476.7871 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16230.7021 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16147.6426 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15926.9141 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15888.3818 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15724.9795 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15563.7588 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15464.9062 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15361.5186 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15148.6670 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15085.8770 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14945.0771 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14802.6553 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14705.2471 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14539.3027 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14374.7373 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14327.6494 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14196.8184 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14140.0273 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 15ms/step - loss: 13898.7373 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 15762.4180 - accuracy: 0.0000e+00\n","Epoch 1/50\n","14/14 [==============================] - 8s 15ms/step - loss: 23087.4414 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20897.9316 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 16ms/step - loss: 20535.2988 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20318.4395 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20134.2344 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 17ms/step - loss: 19955.5039 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 17ms/step - loss: 19783.7949 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19639.1094 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19470.7188 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19311.2480 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19152.9512 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19005.2676 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18847.2051 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18713.8945 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 16ms/step - loss: 18482.3496 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18317.1543 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 16ms/step - loss: 18161.2305 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18019.5938 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17877.0234 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17736.8691 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17579.2734 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17373.8242 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17318.8477 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 17ms/step - loss: 17128.6816 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17026.1426 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16912.4629 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16730.7051 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16567.1133 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16496.1582 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16350.4102 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16139.1611 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16013.0840 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15882.1465 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15757.4814 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15605.1699 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 17ms/step - loss: 15443.7607 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15287.4102 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15206.4355 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15030.6826 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14982.6836 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14815.9473 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14697.8457 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14634.1211 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14384.7646 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14352.0068 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14185.9209 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14098.0840 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 15ms/step - loss: 13912.1758 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 15ms/step - loss: 13804.6777 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 15ms/step - loss: 13695.7021 - accuracy: 0.0037\n","7/7 [==============================] - 2s 6ms/step - loss: 13782.1562 - accuracy: 0.0075\n","Epoch 1/50\n","14/14 [==============================] - 7s 14ms/step - loss: 23324.1035 - accuracy: 0.0037\n","Epoch 2/50\n","14/14 [==============================] - 0s 14ms/step - loss: 21280.7617 - accuracy: 0.0037\n","Epoch 3/50\n","14/14 [==============================] - 0s 16ms/step - loss: 20922.3984 - accuracy: 0.0037\n","Epoch 4/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20718.1680 - accuracy: 0.0037\n","Epoch 5/50\n","14/14 [==============================] - 0s 14ms/step - loss: 20532.8340 - accuracy: 0.0037\n","Epoch 6/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20300.5371 - accuracy: 0.0037\n","Epoch 7/50\n","14/14 [==============================] - 0s 15ms/step - loss: 20211.1016 - accuracy: 0.0037\n","Epoch 8/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19995.7363 - accuracy: 0.0037\n","Epoch 9/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19855.4082 - accuracy: 0.0037\n","Epoch 10/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19606.7969 - accuracy: 0.0037\n","Epoch 11/50\n","14/14 [==============================] - 0s 14ms/step - loss: 19488.5898 - accuracy: 0.0037\n","Epoch 12/50\n","14/14 [==============================] - 0s 16ms/step - loss: 19311.7773 - accuracy: 0.0037\n","Epoch 13/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19178.6934 - accuracy: 0.0037\n","Epoch 14/50\n","14/14 [==============================] - 0s 15ms/step - loss: 19007.6543 - accuracy: 0.0037\n","Epoch 15/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18846.0195 - accuracy: 0.0037\n","Epoch 16/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18696.1055 - accuracy: 0.0037\n","Epoch 17/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18519.7559 - accuracy: 0.0037\n","Epoch 18/50\n","14/14 [==============================] - 0s 15ms/step - loss: 18427.2734 - accuracy: 0.0037\n","Epoch 19/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18261.3145 - accuracy: 0.0037\n","Epoch 20/50\n","14/14 [==============================] - 0s 14ms/step - loss: 18080.9863 - accuracy: 0.0037\n","Epoch 21/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17898.6934 - accuracy: 0.0037\n","Epoch 22/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17794.7441 - accuracy: 0.0037\n","Epoch 23/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17655.2676 - accuracy: 0.0037\n","Epoch 24/50\n","14/14 [==============================] - 0s 14ms/step - loss: 17519.5566 - accuracy: 0.0037\n","Epoch 25/50\n","14/14 [==============================] - 0s 16ms/step - loss: 17303.9219 - accuracy: 0.0037\n","Epoch 26/50\n","14/14 [==============================] - 0s 16ms/step - loss: 17202.9004 - accuracy: 0.0037\n","Epoch 27/50\n","14/14 [==============================] - 0s 15ms/step - loss: 17039.0352 - accuracy: 0.0037\n","Epoch 28/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16916.8750 - accuracy: 0.0037\n","Epoch 29/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16801.0176 - accuracy: 0.0037\n","Epoch 30/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16595.0840 - accuracy: 0.0037\n","Epoch 31/50\n","14/14 [==============================] - 0s 16ms/step - loss: 16484.4570 - accuracy: 0.0037\n","Epoch 32/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16324.8564 - accuracy: 0.0037\n","Epoch 33/50\n","14/14 [==============================] - 0s 15ms/step - loss: 16154.6582 - accuracy: 0.0037\n","Epoch 34/50\n","14/14 [==============================] - 0s 14ms/step - loss: 16067.1475 - accuracy: 0.0037\n","Epoch 35/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15931.9512 - accuracy: 0.0037\n","Epoch 36/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15835.7051 - accuracy: 0.0037\n","Epoch 37/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15699.1660 - accuracy: 0.0037\n","Epoch 38/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15462.8691 - accuracy: 0.0037\n","Epoch 39/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15370.1104 - accuracy: 0.0037\n","Epoch 40/50\n","14/14 [==============================] - 0s 16ms/step - loss: 15203.8584 - accuracy: 0.0037\n","Epoch 41/50\n","14/14 [==============================] - 0s 15ms/step - loss: 15146.2188 - accuracy: 0.0037\n","Epoch 42/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14930.6553 - accuracy: 0.0037\n","Epoch 43/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14843.9775 - accuracy: 0.0037\n","Epoch 44/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14695.0498 - accuracy: 0.0037\n","Epoch 45/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14540.0498 - accuracy: 0.0037\n","Epoch 46/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14436.6504 - accuracy: 0.0037\n","Epoch 47/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14449.9209 - accuracy: 0.0037\n","Epoch 48/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14307.1309 - accuracy: 0.0037\n","Epoch 49/50\n","14/14 [==============================] - 0s 15ms/step - loss: 14049.0273 - accuracy: 0.0037\n","Epoch 50/50\n","14/14 [==============================] - 0s 14ms/step - loss: 14077.1553 - accuracy: 0.0037\n","7/7 [==============================] - 2s 5ms/step - loss: 12133.3105 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 8s 18ms/step - loss: 23616.5645 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 15ms/step - loss: 23541.3574 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23185.0254 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 17ms/step - loss: 22061.6621 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 16ms/step - loss: 21161.2617 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20849.2461 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20681.6543 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20493.9297 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 22995.1523 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 7s 17ms/step - loss: 23613.3340 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23524.0840 - accuracy: 0.0075\n","Epoch 3/8\n","9/9 [==============================] - 0s 15ms/step - loss: 23117.4707 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 18ms/step - loss: 21922.0957 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 16ms/step - loss: 21028.6543 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20710.8281 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20524.7441 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20372.5176 - accuracy: 0.0037\n","5/5 [==============================] - 2s 7ms/step - loss: 20415.4883 - accuracy: 0.0075\n","Epoch 1/8\n","9/9 [==============================] - 7s 15ms/step - loss: 23791.8340 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23707.3770 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23291.0781 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 17ms/step - loss: 22284.9863 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21519.5859 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21267.3047 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21083.7891 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20926.3711 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 18523.8555 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 8s 15ms/step - loss: 23629.8340 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.7871 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.7285 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.7129 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 16ms/step - loss: 23629.5898 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.5977 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.5723 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.5215 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 26480.9980 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 7s 17ms/step - loss: 23629.6602 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 14ms/step - loss: 23629.6719 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.5703 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.5801 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.5566 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.4336 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.4336 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.3008 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 23738.1875 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 8s 16ms/step - loss: 23807.4473 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.4531 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 15ms/step - loss: 23807.3809 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.3418 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.3770 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 16ms/step - loss: 23807.2285 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 16ms/step - loss: 23807.2246 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 15ms/step - loss: 23807.1934 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 21310.2188 - accuracy: 0.0075\n","Epoch 1/8\n","9/9 [==============================] - 7s 17ms/step - loss: 23537.6680 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 16ms/step - loss: 22018.3320 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21120.7441 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20954.1270 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20765.3555 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 15ms/step - loss: 20645.5449 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20470.5781 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20389.9922 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 22909.0078 - accuracy: 0.0000e+00\n","Epoch 1/8\n","9/9 [==============================] - 7s 15ms/step - loss: 23536.5020 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 15ms/step - loss: 22111.7930 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 16ms/step - loss: 21318.3789 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 18ms/step - loss: 21132.4766 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20979.3555 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 16ms/step - loss: 20856.3711 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20763.6680 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 18ms/step - loss: 20624.2500 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 20685.7598 - accuracy: 0.0075\n","Epoch 1/8\n","9/9 [==============================] - 8s 15ms/step - loss: 23701.8691 - accuracy: 0.0037\n","Epoch 2/8\n","9/9 [==============================] - 0s 18ms/step - loss: 22190.2910 - accuracy: 0.0037\n","Epoch 3/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21385.8242 - accuracy: 0.0037\n","Epoch 4/8\n","9/9 [==============================] - 0s 19ms/step - loss: 21168.7383 - accuracy: 0.0037\n","Epoch 5/8\n","9/9 [==============================] - 0s 17ms/step - loss: 21060.4297 - accuracy: 0.0037\n","Epoch 6/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20902.2617 - accuracy: 0.0037\n","Epoch 7/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20757.4766 - accuracy: 0.0037\n","Epoch 8/8\n","9/9 [==============================] - 0s 17ms/step - loss: 20674.2852 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 18320.2793 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 7s 18ms/step - loss: 23616.5391 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23539.9805 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23201.6289 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 17ms/step - loss: 22111.2109 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 17ms/step - loss: 21373.3926 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 16ms/step - loss: 21073.5742 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20881.0488 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20749.1875 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20583.6211 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20489.1133 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20300.0117 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20201.6973 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20074.7285 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19962.9238 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19850.5234 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19722.3340 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19612.9004 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19438.1191 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19322.3086 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19214.0684 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19135.0449 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19003.1523 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 15ms/step - loss: 18937.2246 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 16ms/step - loss: 18842.6660 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18715.5781 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18603.3418 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18517.4043 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18367.1230 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 15ms/step - loss: 18301.9199 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18183.0684 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 20485.8301 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 8s 16ms/step - loss: 23614.8457 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23532.8066 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23142.9590 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 17ms/step - loss: 21994.8730 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 21223.6719 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20926.6543 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20733.9629 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20583.8750 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20440.6465 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20269.3477 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20134.2617 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20007.1348 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19919.7598 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19790.8770 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19642.1191 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19496.4961 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19450.2051 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19313.6660 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19169.2012 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19055.7207 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 16ms/step - loss: 18922.9238 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18846.4199 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18724.6660 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18639.0898 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18533.3164 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18377.9023 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18361.0449 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18237.0039 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 19ms/step - loss: 18077.5684 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18041.5840 - accuracy: 0.0037\n","5/5 [==============================] - 2s 7ms/step - loss: 18091.9688 - accuracy: 0.0075\n","Epoch 1/30\n","9/9 [==============================] - 7s 18ms/step - loss: 23793.7168 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23712.9375 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 19ms/step - loss: 23317.4297 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 17ms/step - loss: 22132.4805 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 19ms/step - loss: 21269.0352 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20953.9668 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20810.6797 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20629.9434 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20503.1367 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20325.3027 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20204.2227 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20072.2637 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19964.7832 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19844.5469 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19738.1680 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19579.3125 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19464.5508 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19349.7812 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19243.6270 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19158.1191 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19062.8789 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18970.3770 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18814.2285 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18660.8574 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18602.3887 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18470.1465 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 20ms/step - loss: 18378.9375 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18273.5684 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18158.2090 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18051.6445 - accuracy: 0.0037\n","5/5 [==============================] - 2s 7ms/step - loss: 15913.0850 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 7s 17ms/step - loss: 23629.1875 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.1250 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.0840 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.0098 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.0000 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.9043 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.8809 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.8281 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.7520 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.7070 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.6836 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.6133 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23628.5645 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.5234 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.4473 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.4199 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.3652 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.2988 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23628.2227 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.1758 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.1074 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.0410 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.0234 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.9766 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23627.8809 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.8320 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.7656 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.6875 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23627.6035 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.5703 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 26478.8203 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 8s 15ms/step - loss: 23629.4531 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23629.4062 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.3809 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.2910 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.1875 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.1914 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.1211 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.1270 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.0293 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.9941 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.9238 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.8672 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23628.8145 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.7500 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23628.7676 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.6406 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.5723 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.5566 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23628.5039 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.3906 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.3848 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.2871 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.2812 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.1816 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.1328 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.0664 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23628.0078 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.9121 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.9238 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.8359 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 23736.6777 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 7s 15ms/step - loss: 23807.6582 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23807.6035 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23807.5527 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23807.4746 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.4785 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23807.4141 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.2871 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23807.3125 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23807.2285 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23807.1797 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23807.0801 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 14ms/step - loss: 23807.0469 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.9785 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.8652 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23806.8125 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.7148 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.7812 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.5938 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.6230 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.5176 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.4629 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.3965 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.3242 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23806.3398 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.1914 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 15ms/step - loss: 23806.1152 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.0566 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23806.0215 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 16ms/step - loss: 23805.9727 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.8633 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 21309.0234 - accuracy: 0.0075\n","Epoch 1/30\n","9/9 [==============================] - 8s 17ms/step - loss: 23555.4922 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 19ms/step - loss: 22041.1035 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 17ms/step - loss: 21081.8418 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20855.9766 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20714.6602 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20592.3359 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20453.8477 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20374.7578 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20274.2812 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20098.7324 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20015.7637 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19875.8711 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19839.5625 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19703.6660 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19602.8164 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19511.2637 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19361.1504 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19285.6211 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19178.0723 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19103.8516 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18982.8730 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18866.3418 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18760.6191 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18635.8008 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18561.9004 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18453.6602 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18384.7988 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18278.5957 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18157.6328 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18041.6836 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 20360.1074 - accuracy: 0.0000e+00\n","Epoch 1/30\n","9/9 [==============================] - 7s 18ms/step - loss: 23519.8125 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 15ms/step - loss: 21896.7051 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 19ms/step - loss: 21039.0957 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20835.5840 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20685.6855 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20556.1230 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20445.9707 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20315.9434 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20206.7129 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20120.7383 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20017.8770 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19888.4844 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19746.4375 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19644.9297 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 16ms/step - loss: 19559.0234 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19469.8594 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 19ms/step - loss: 19346.7090 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19239.0742 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19132.6094 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19047.2969 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 15ms/step - loss: 18917.4922 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18855.7617 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 19ms/step - loss: 18732.9746 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18662.1895 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 16ms/step - loss: 18534.9746 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18437.2949 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18331.5215 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18278.2715 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 19ms/step - loss: 18121.8965 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18082.9258 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 18109.6094 - accuracy: 0.0075\n","Epoch 1/30\n","9/9 [==============================] - 7s 17ms/step - loss: 23717.3848 - accuracy: 0.0037\n","Epoch 2/30\n","9/9 [==============================] - 0s 15ms/step - loss: 22228.1543 - accuracy: 0.0037\n","Epoch 3/30\n","9/9 [==============================] - 0s 18ms/step - loss: 21387.5059 - accuracy: 0.0037\n","Epoch 4/30\n","9/9 [==============================] - 0s 16ms/step - loss: 21183.7051 - accuracy: 0.0037\n","Epoch 5/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20981.5488 - accuracy: 0.0037\n","Epoch 6/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20895.8047 - accuracy: 0.0037\n","Epoch 7/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20761.7930 - accuracy: 0.0037\n","Epoch 8/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20634.1484 - accuracy: 0.0037\n","Epoch 9/30\n","9/9 [==============================] - 0s 19ms/step - loss: 20547.8848 - accuracy: 0.0037\n","Epoch 10/30\n","9/9 [==============================] - 0s 18ms/step - loss: 20464.5879 - accuracy: 0.0037\n","Epoch 11/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20291.5332 - accuracy: 0.0037\n","Epoch 12/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20250.8887 - accuracy: 0.0037\n","Epoch 13/30\n","9/9 [==============================] - 0s 17ms/step - loss: 20079.2930 - accuracy: 0.0037\n","Epoch 14/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19981.7949 - accuracy: 0.0037\n","Epoch 15/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19903.2148 - accuracy: 0.0037\n","Epoch 16/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19772.8516 - accuracy: 0.0037\n","Epoch 17/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19629.2617 - accuracy: 0.0037\n","Epoch 18/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19602.1777 - accuracy: 0.0037\n","Epoch 19/30\n","9/9 [==============================] - 0s 17ms/step - loss: 19488.7129 - accuracy: 0.0037\n","Epoch 20/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19332.4160 - accuracy: 0.0037\n","Epoch 21/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19264.8730 - accuracy: 0.0037\n","Epoch 22/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19147.4883 - accuracy: 0.0037\n","Epoch 23/30\n","9/9 [==============================] - 0s 18ms/step - loss: 19025.5605 - accuracy: 0.0037\n","Epoch 24/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18892.5059 - accuracy: 0.0037\n","Epoch 25/30\n","9/9 [==============================] - 0s 16ms/step - loss: 18842.9297 - accuracy: 0.0037\n","Epoch 26/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18723.6582 - accuracy: 0.0037\n","Epoch 27/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18672.4941 - accuracy: 0.0037\n","Epoch 28/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18513.8965 - accuracy: 0.0037\n","Epoch 29/30\n","9/9 [==============================] - 0s 18ms/step - loss: 18434.1934 - accuracy: 0.0037\n","Epoch 30/30\n","9/9 [==============================] - 0s 17ms/step - loss: 18359.6934 - accuracy: 0.0037\n","5/5 [==============================] - 2s 7ms/step - loss: 16159.3066 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 8s 17ms/step - loss: 23614.5352 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23542.4902 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23166.8223 - accuracy: 0.0075\n","Epoch 4/50\n","9/9 [==============================] - 0s 19ms/step - loss: 21951.8594 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 17ms/step - loss: 21044.3320 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20752.9531 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20589.6797 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20428.1152 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 16ms/step - loss: 20259.1172 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20123.2461 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19998.0020 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19878.8203 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19729.5000 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19631.9551 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19484.2715 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19364.2285 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19211.3867 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19178.8086 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19019.2949 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18917.2363 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 16ms/step - loss: 18782.9473 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18738.7363 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18587.9395 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18468.4492 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18388.7168 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 16ms/step - loss: 18265.8613 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18253.0645 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18019.9668 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17960.7910 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17864.9180 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17781.6348 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17704.1895 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17619.7070 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17450.1016 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 21ms/step - loss: 17365.2344 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17283.7207 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 16ms/step - loss: 17184.7988 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17064.8848 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17014.3809 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 20ms/step - loss: 16934.3164 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 16ms/step - loss: 16837.4805 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16770.1113 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16659.3730 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16564.6562 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16445.0156 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16412.3145 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16235.6709 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16165.3291 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16091.6631 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16056.1895 - accuracy: 0.0037\n","5/5 [==============================] - 2s 9ms/step - loss: 18131.8105 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 7s 19ms/step - loss: 23614.8926 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23538.9590 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23178.5117 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 18ms/step - loss: 21978.7656 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20992.2051 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20687.3828 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20511.7695 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 16ms/step - loss: 20411.5938 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20270.8242 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20091.2266 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19974.6504 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19840.2500 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19695.2656 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19557.8535 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19479.2051 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19313.0664 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19225.2324 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19091.2207 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 20ms/step - loss: 19013.7305 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18910.6855 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18790.6582 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18644.2812 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18557.1016 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18459.4805 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18351.6016 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18259.6465 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 16ms/step - loss: 18159.3945 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18076.6934 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17906.7988 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17793.8633 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17730.4004 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17667.2930 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17505.8652 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17403.0273 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 16ms/step - loss: 17391.4766 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17286.1191 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 20ms/step - loss: 17211.1934 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 16ms/step - loss: 17106.0430 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16979.0918 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16925.5918 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16798.9922 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16731.4316 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16617.0293 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16488.7852 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16494.5684 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16303.8691 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16229.3164 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 16ms/step - loss: 16185.1328 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16120.3203 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 18ms/step - loss: 15986.0820 - accuracy: 0.0037\n","5/5 [==============================] - 2s 7ms/step - loss: 16102.0898 - accuracy: 0.0075\n","Epoch 1/50\n","9/9 [==============================] - 8s 16ms/step - loss: 23793.7852 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23718.7246 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23392.6895 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 20ms/step - loss: 22180.8789 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 18ms/step - loss: 21294.9961 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 20ms/step - loss: 21031.8770 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20829.0273 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20672.1270 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20528.4883 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20411.8711 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20272.6641 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20135.4473 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20021.8652 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19882.7051 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19767.6855 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 20ms/step - loss: 19664.9688 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 16ms/step - loss: 19551.1543 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19392.7402 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19306.0859 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19166.0703 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19051.1484 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18970.9316 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18852.8301 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18726.2324 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18631.7344 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18531.2441 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18417.7988 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18303.8203 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18240.9844 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18136.8984 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18005.3535 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17917.3906 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17785.9414 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 20ms/step - loss: 17742.5938 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17629.1602 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17536.4805 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17399.9336 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17356.9473 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17300.4492 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17121.5410 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17061.5215 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16990.2461 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16935.3242 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16760.6562 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16758.1660 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16653.1367 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16518.7168 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16390.6113 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16347.9062 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16243.0303 - accuracy: 0.0037\n","5/5 [==============================] - 2s 5ms/step - loss: 14251.6543 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 7s 19ms/step - loss: 23629.9453 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.8828 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23629.8418 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.8047 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.7129 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.7129 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.6172 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 15ms/step - loss: 23629.6777 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23629.5410 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.4570 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.4531 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.3535 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.3613 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.3320 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.2656 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23629.2598 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.1816 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.1426 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.0762 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.0156 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.9199 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.8398 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.8477 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.8242 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.6719 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23628.7227 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.5977 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.5508 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.4824 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23628.4316 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.3809 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23628.3379 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.2734 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.2051 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.1309 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.1152 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.0703 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.9629 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.9258 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.9102 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.8340 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.7812 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.7051 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23627.6250 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23627.5527 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.5059 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 15ms/step - loss: 23627.5078 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23627.3887 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.3125 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.2520 - accuracy: 0.0037\n","5/5 [==============================] - 2s 5ms/step - loss: 26478.4844 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 7s 17ms/step - loss: 23629.9766 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.8418 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.8281 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.8125 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.7637 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23629.6777 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.6484 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.5723 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.5156 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.4473 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.3945 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.3281 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.2500 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23629.2324 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.2500 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23629.1914 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.1348 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23629.0488 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.9570 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23628.9238 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.8379 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.8066 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.7266 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.7500 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.7051 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23628.5898 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.4766 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23628.4590 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.4316 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23628.3301 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.2969 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23628.2129 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23628.1836 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23628.1289 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.9902 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.9590 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.8418 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.8047 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.7891 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.6719 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.6582 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.5801 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.5547 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.4629 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23627.4219 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.2383 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.2695 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.2031 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23627.1406 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23627.0488 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 23735.8887 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 8s 18ms/step - loss: 23806.5977 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23806.5801 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.4570 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 15ms/step - loss: 23806.4492 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23806.4766 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.4316 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.3457 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.2598 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.3086 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23806.1641 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23806.1191 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23806.1055 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.0625 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23806.0469 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23805.9844 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.8809 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23805.9062 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.8027 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23805.7168 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23805.7383 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.5684 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23805.6309 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23805.5469 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23805.5156 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.5156 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23805.3809 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23805.3359 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 16ms/step - loss: 23805.2930 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23805.2656 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23805.1797 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.1016 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23805.0684 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23804.9395 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.9746 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.8691 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.7656 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.7168 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23804.7285 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.6660 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.6113 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 19ms/step - loss: 23804.4844 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.4238 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23804.4160 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.3379 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23804.3066 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 17ms/step - loss: 23804.2383 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 20ms/step - loss: 23804.1562 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.0762 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23804.0215 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 18ms/step - loss: 23803.9570 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 21307.2832 - accuracy: 0.0075\n","Epoch 1/50\n","9/9 [==============================] - 7s 16ms/step - loss: 23558.3867 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 18ms/step - loss: 22108.0840 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 18ms/step - loss: 21146.6484 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20932.8496 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20784.7852 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20679.1309 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20548.8379 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20406.9512 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20310.4395 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20203.6816 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 20ms/step - loss: 20095.5137 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19986.0312 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19892.0371 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19768.9512 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19659.3203 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19590.3613 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19497.5566 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19334.6914 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19258.7441 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19195.1797 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19068.0176 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18978.1895 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18812.2246 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18761.7324 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18636.7402 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18493.2852 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18447.8789 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18362.7559 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18264.4375 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18188.2832 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18048.1367 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17917.4082 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17815.2695 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17779.3516 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17620.2949 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17569.5078 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17478.2539 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17368.7598 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17230.5977 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17131.0781 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17120.5176 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16960.0703 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16883.9141 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 20ms/step - loss: 16779.4316 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16715.7051 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16605.1035 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16525.8867 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 20ms/step - loss: 16405.1055 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16362.5020 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16236.8506 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 18356.6309 - accuracy: 0.0000e+00\n","Epoch 1/50\n","9/9 [==============================] - 8s 20ms/step - loss: 23522.6816 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 18ms/step - loss: 21918.7695 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 19ms/step - loss: 21033.6094 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 17ms/step - loss: 20855.8652 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20693.3887 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 20ms/step - loss: 20553.3867 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20455.2285 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20307.7617 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20231.6816 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20082.9531 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20002.6211 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 20ms/step - loss: 19868.8398 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19751.0352 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19657.0703 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19543.3848 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 20ms/step - loss: 19433.1895 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19337.9023 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19232.9922 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19158.5176 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19026.3125 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18920.6914 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18824.7109 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18692.5332 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18599.3281 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18545.5625 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18417.8125 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 20ms/step - loss: 18310.4199 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18207.3926 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18104.7910 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18027.3223 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17936.5820 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17815.6738 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17679.5215 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17651.6504 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17516.9453 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17477.1211 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17380.5820 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17276.4805 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 20ms/step - loss: 17190.3555 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17043.9512 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16975.4375 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16897.3613 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16755.6895 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16681.9336 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16577.3027 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 20ms/step - loss: 16449.7578 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16378.6123 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16255.3818 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16264.4922 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16144.7021 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 16239.0596 - accuracy: 0.0075\n","Epoch 1/50\n","9/9 [==============================] - 8s 18ms/step - loss: 23683.2520 - accuracy: 0.0037\n","Epoch 2/50\n","9/9 [==============================] - 0s 19ms/step - loss: 21977.6484 - accuracy: 0.0037\n","Epoch 3/50\n","9/9 [==============================] - 0s 19ms/step - loss: 21215.6738 - accuracy: 0.0037\n","Epoch 4/50\n","9/9 [==============================] - 0s 18ms/step - loss: 21009.1426 - accuracy: 0.0037\n","Epoch 5/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20861.7812 - accuracy: 0.0037\n","Epoch 6/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20735.6855 - accuracy: 0.0037\n","Epoch 7/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20597.6875 - accuracy: 0.0037\n","Epoch 8/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20481.8555 - accuracy: 0.0037\n","Epoch 9/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20394.5000 - accuracy: 0.0037\n","Epoch 10/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20299.0391 - accuracy: 0.0037\n","Epoch 11/50\n","9/9 [==============================] - 0s 19ms/step - loss: 20127.7832 - accuracy: 0.0037\n","Epoch 12/50\n","9/9 [==============================] - 0s 18ms/step - loss: 20049.3691 - accuracy: 0.0037\n","Epoch 13/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19960.3828 - accuracy: 0.0037\n","Epoch 14/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19831.5625 - accuracy: 0.0037\n","Epoch 15/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19728.6445 - accuracy: 0.0037\n","Epoch 16/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19610.8125 - accuracy: 0.0037\n","Epoch 17/50\n","9/9 [==============================] - 0s 17ms/step - loss: 19570.6973 - accuracy: 0.0037\n","Epoch 18/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19421.6660 - accuracy: 0.0037\n","Epoch 19/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19280.8340 - accuracy: 0.0037\n","Epoch 20/50\n","9/9 [==============================] - 0s 18ms/step - loss: 19202.2480 - accuracy: 0.0037\n","Epoch 21/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19050.2246 - accuracy: 0.0037\n","Epoch 22/50\n","9/9 [==============================] - 0s 19ms/step - loss: 19009.8301 - accuracy: 0.0037\n","Epoch 23/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18852.3066 - accuracy: 0.0037\n","Epoch 24/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18792.4434 - accuracy: 0.0037\n","Epoch 25/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18701.0723 - accuracy: 0.0037\n","Epoch 26/50\n","9/9 [==============================] - 0s 19ms/step - loss: 18537.1973 - accuracy: 0.0037\n","Epoch 27/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18477.6172 - accuracy: 0.0037\n","Epoch 28/50\n","9/9 [==============================] - 0s 18ms/step - loss: 18390.7637 - accuracy: 0.0037\n","Epoch 29/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18319.0020 - accuracy: 0.0037\n","Epoch 30/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18203.8203 - accuracy: 0.0037\n","Epoch 31/50\n","9/9 [==============================] - 0s 17ms/step - loss: 18136.3340 - accuracy: 0.0037\n","Epoch 32/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17999.9805 - accuracy: 0.0037\n","Epoch 33/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17872.5312 - accuracy: 0.0037\n","Epoch 34/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17770.8008 - accuracy: 0.0037\n","Epoch 35/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17691.6250 - accuracy: 0.0037\n","Epoch 36/50\n","9/9 [==============================] - 0s 17ms/step - loss: 17607.8125 - accuracy: 0.0037\n","Epoch 37/50\n","9/9 [==============================] - 0s 21ms/step - loss: 17479.1680 - accuracy: 0.0037\n","Epoch 38/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17461.5273 - accuracy: 0.0037\n","Epoch 39/50\n","9/9 [==============================] - 0s 19ms/step - loss: 17327.8340 - accuracy: 0.0037\n","Epoch 40/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17268.1289 - accuracy: 0.0037\n","Epoch 41/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17134.0332 - accuracy: 0.0037\n","Epoch 42/50\n","9/9 [==============================] - 0s 18ms/step - loss: 17028.4102 - accuracy: 0.0037\n","Epoch 43/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16924.6484 - accuracy: 0.0037\n","Epoch 44/50\n","9/9 [==============================] - 0s 17ms/step - loss: 16817.6250 - accuracy: 0.0037\n","Epoch 45/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16810.3535 - accuracy: 0.0037\n","Epoch 46/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16690.4434 - accuracy: 0.0037\n","Epoch 47/50\n","9/9 [==============================] - 0s 18ms/step - loss: 16559.0332 - accuracy: 0.0037\n","Epoch 48/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16450.6270 - accuracy: 0.0037\n","Epoch 49/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16334.8018 - accuracy: 0.0037\n","Epoch 50/50\n","9/9 [==============================] - 0s 19ms/step - loss: 16315.7744 - accuracy: 0.0037\n","5/5 [==============================] - 2s 6ms/step - loss: 14272.2314 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 7s 19ms/step - loss: 23619.5469 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 19ms/step - loss: 23574.1230 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23444.1836 - accuracy: 0.0075\n","Epoch 4/8\n","7/7 [==============================] - 0s 22ms/step - loss: 22976.7031 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 21ms/step - loss: 21990.8496 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 20ms/step - loss: 21239.6172 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 20ms/step - loss: 20931.6582 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 20ms/step - loss: 20781.3535 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 23272.0996 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 8s 22ms/step - loss: 23618.1133 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 19ms/step - loss: 23566.0312 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23408.7500 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 20ms/step - loss: 22838.7207 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 22ms/step - loss: 21791.0840 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 21ms/step - loss: 21107.5684 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 22ms/step - loss: 20835.2031 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20686.2148 - accuracy: 0.0037\n","4/4 [==============================] - 2s 9ms/step - loss: 20714.0547 - accuracy: 0.0075\n","Epoch 1/8\n","7/7 [==============================] - 7s 18ms/step - loss: 23798.0059 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23757.3086 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23639.7930 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 23ms/step - loss: 23214.5020 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 20ms/step - loss: 22308.6016 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 20ms/step - loss: 21549.9746 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 21ms/step - loss: 21274.9629 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 23ms/step - loss: 21096.0449 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 18703.3398 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 8s 19ms/step - loss: 23630.0684 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23630.0312 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 23ms/step - loss: 23629.9707 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.9941 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.9355 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.8926 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.8516 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.8516 - accuracy: 0.0037\n","4/4 [==============================] - 2s 8ms/step - loss: 26481.3242 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 7s 19ms/step - loss: 23629.3711 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.3418 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.3047 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.2812 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2051 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.1738 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.1465 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1426 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 23737.9668 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 7s 20ms/step - loss: 23806.7383 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.6660 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.6172 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.5352 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.5625 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.5547 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 22ms/step - loss: 23806.3887 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.4336 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 21309.5820 - accuracy: 0.0075\n","Epoch 1/8\n","7/7 [==============================] - 8s 19ms/step - loss: 23579.6406 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 23ms/step - loss: 22667.4082 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 20ms/step - loss: 21407.6094 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 22ms/step - loss: 21107.8438 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 20ms/step - loss: 20946.8828 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20795.1758 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20719.7227 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 22ms/step - loss: 20623.5234 - accuracy: 0.0037\n","4/4 [==============================] - 2s 9ms/step - loss: 23163.3281 - accuracy: 0.0000e+00\n","Epoch 1/8\n","7/7 [==============================] - 7s 19ms/step - loss: 23568.1191 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 22ms/step - loss: 22543.6074 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 20ms/step - loss: 21103.8945 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20790.7109 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 20ms/step - loss: 20646.2578 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20541.0254 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 22ms/step - loss: 20443.2051 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20329.6699 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 20394.7500 - accuracy: 0.0075\n","Epoch 1/8\n","7/7 [==============================] - 8s 20ms/step - loss: 23744.0938 - accuracy: 0.0037\n","Epoch 2/8\n","7/7 [==============================] - 0s 20ms/step - loss: 22583.6035 - accuracy: 0.0037\n","Epoch 3/8\n","7/7 [==============================] - 0s 22ms/step - loss: 21089.0508 - accuracy: 0.0037\n","Epoch 4/8\n","7/7 [==============================] - 0s 22ms/step - loss: 20730.4590 - accuracy: 0.0037\n","Epoch 5/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20586.4297 - accuracy: 0.0037\n","Epoch 6/8\n","7/7 [==============================] - 0s 20ms/step - loss: 20489.9434 - accuracy: 0.0037\n","Epoch 7/8\n","7/7 [==============================] - 0s 22ms/step - loss: 20363.1074 - accuracy: 0.0037\n","Epoch 8/8\n","7/7 [==============================] - 0s 21ms/step - loss: 20297.2695 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 17970.0332 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 8s 19ms/step - loss: 23617.6621 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23564.9062 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23401.6602 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 22ms/step - loss: 22830.1953 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 23ms/step - loss: 21800.4316 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 21ms/step - loss: 21169.9277 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 23ms/step - loss: 20935.3594 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20765.8281 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20654.6055 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20519.0449 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20401.3125 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20275.4316 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20180.6445 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20032.0605 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20005.4668 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 23ms/step - loss: 19865.7500 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19778.3652 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19695.7773 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19595.7012 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19461.2305 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19402.7598 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19338.7949 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19267.6152 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19145.7578 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19087.2500 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 23ms/step - loss: 19003.5469 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 22ms/step - loss: 18898.4453 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 22ms/step - loss: 18859.9609 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 22ms/step - loss: 18691.9688 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 22ms/step - loss: 18664.8691 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 21013.1602 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 7s 19ms/step - loss: 23617.8223 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 24ms/step - loss: 23568.3418 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23412.9121 - accuracy: 0.0000e+00\n","Epoch 4/30\n","7/7 [==============================] - 0s 21ms/step - loss: 22883.8711 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21880.2461 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21208.4629 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20944.3477 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20782.3301 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20677.9316 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20544.8164 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20427.2344 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20332.4336 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20190.2305 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20080.3867 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19990.2852 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19931.5332 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19764.7676 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19736.3418 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19604.6387 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19499.6973 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19412.1621 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19388.5176 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19265.8594 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19166.1465 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19071.8105 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18987.6719 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18898.1973 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18806.4648 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 23ms/step - loss: 18727.0527 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18652.9141 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 18742.1895 - accuracy: 0.0075\n","Epoch 1/30\n","7/7 [==============================] - 8s 20ms/step - loss: 23794.5605 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 24ms/step - loss: 23740.6582 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23577.1582 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 20ms/step - loss: 22997.5918 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 22ms/step - loss: 21986.7051 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21377.1543 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21106.0684 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20951.1055 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20822.4219 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20701.9043 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20571.7969 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20440.7168 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20353.3340 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20259.1191 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 23ms/step - loss: 20174.3242 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 23ms/step - loss: 20026.7227 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19990.4004 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 24ms/step - loss: 19836.1621 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 24ms/step - loss: 19744.0527 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19685.2285 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 23ms/step - loss: 19589.5430 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19485.5781 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19418.4316 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19315.1875 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 23ms/step - loss: 19226.3535 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19177.8691 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19011.8965 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18961.9863 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18903.4688 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18776.8887 - accuracy: 0.0037\n","4/4 [==============================] - 2s 10ms/step - loss: 16589.1328 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 7s 20ms/step - loss: 23630.0703 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23630.0742 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 23ms/step - loss: 23630.0117 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.9883 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.9570 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.8984 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.8418 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.8633 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.7500 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.7871 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.6719 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.6738 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.6465 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.5762 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.5176 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4922 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.4102 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4141 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.3789 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 17ms/step - loss: 23629.2949 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2246 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.2090 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.1426 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.1035 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.0664 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.0254 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.9980 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.9277 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 18ms/step - loss: 23628.8926 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.8438 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 26480.2578 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 8s 20ms/step - loss: 23629.7129 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.6875 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.6445 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5801 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5723 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5430 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5430 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4805 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4004 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 23ms/step - loss: 23629.3867 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.3516 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.3438 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.3008 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.2637 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2070 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1719 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.1348 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.1484 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.1113 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.0859 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.9707 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.0020 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.9355 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.8789 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.8711 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.8301 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.7910 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.7383 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.7363 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.6699 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 23737.5215 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 7s 19ms/step - loss: 23807.4707 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.4863 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.4531 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.4082 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.3340 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.3145 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.2422 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.2754 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.2832 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.1816 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.1113 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23807.0859 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.0195 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.0254 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.0176 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23806.9551 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.8848 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 23ms/step - loss: 23806.8633 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.8242 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.8164 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.7852 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.7031 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23806.6641 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 22ms/step - loss: 23806.6094 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.5645 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.5176 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.4883 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.5293 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.4180 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.4160 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 21309.4863 - accuracy: 0.0075\n","Epoch 1/30\n","7/7 [==============================] - 7s 19ms/step - loss: 23568.3887 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 24ms/step - loss: 22565.6504 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21405.1230 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21123.3164 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20968.9023 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20864.9609 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20768.3438 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20654.6816 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20566.9355 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20468.0703 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20408.1250 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20337.0176 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20233.4082 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20149.0605 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20072.6895 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19964.8027 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19886.0234 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19826.8809 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19732.1914 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19603.9941 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19544.5840 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19490.0918 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19394.1309 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19266.2051 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19210.2363 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19096.1582 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 23ms/step - loss: 19091.0547 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19019.3672 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18872.4199 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 20ms/step - loss: 18787.6328 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 21204.5723 - accuracy: 0.0000e+00\n","Epoch 1/30\n","7/7 [==============================] - 8s 19ms/step - loss: 23579.3516 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 25ms/step - loss: 22667.2852 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 21ms/step - loss: 21507.8125 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 21ms/step - loss: 21236.1309 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 20ms/step - loss: 21078.5195 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20943.0098 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20829.8477 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20754.7188 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20653.6543 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20582.2656 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20475.8301 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20382.7285 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20307.8672 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 23ms/step - loss: 20182.4766 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20139.4102 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20023.7188 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20001.5039 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19923.2773 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19797.2031 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19728.8672 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 22ms/step - loss: 19628.6816 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19547.6992 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19473.8516 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19427.5801 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 24ms/step - loss: 19314.9336 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19247.0039 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 24ms/step - loss: 19102.0723 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 21ms/step - loss: 19077.2461 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 19ms/step - loss: 18959.8594 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 20ms/step - loss: 18840.6484 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 18993.2715 - accuracy: 0.0075\n","Epoch 1/30\n","7/7 [==============================] - 7s 24ms/step - loss: 23750.5078 - accuracy: 0.0037\n","Epoch 2/30\n","7/7 [==============================] - 0s 20ms/step - loss: 22709.4102 - accuracy: 0.0037\n","Epoch 3/30\n","7/7 [==============================] - 0s 22ms/step - loss: 21438.5703 - accuracy: 0.0037\n","Epoch 4/30\n","7/7 [==============================] - 0s 22ms/step - loss: 21160.9062 - accuracy: 0.0037\n","Epoch 5/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20992.4844 - accuracy: 0.0037\n","Epoch 6/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20883.6484 - accuracy: 0.0037\n","Epoch 7/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20789.6934 - accuracy: 0.0037\n","Epoch 8/30\n","7/7 [==============================] - 0s 22ms/step - loss: 20689.3906 - accuracy: 0.0037\n","Epoch 9/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20570.3789 - accuracy: 0.0037\n","Epoch 10/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20475.0332 - accuracy: 0.0037\n","Epoch 11/30\n","7/7 [==============================] - 0s 20ms/step - loss: 20395.3223 - accuracy: 0.0037\n","Epoch 12/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20298.9355 - accuracy: 0.0037\n","Epoch 13/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20256.6602 - accuracy: 0.0037\n","Epoch 14/30\n","7/7 [==============================] - 0s 21ms/step - loss: 20146.7246 - accuracy: 0.0037\n","Epoch 15/30\n","7/7 [==============================] - 0s 19ms/step - loss: 20095.5273 - accuracy: 0.0037\n","Epoch 16/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19977.4297 - accuracy: 0.0037\n","Epoch 17/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19867.0195 - accuracy: 0.0037\n","Epoch 18/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19800.4922 - accuracy: 0.0037\n","Epoch 19/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19725.1016 - accuracy: 0.0037\n","Epoch 20/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19613.4297 - accuracy: 0.0037\n","Epoch 21/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19579.8535 - accuracy: 0.0037\n","Epoch 22/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19489.6738 - accuracy: 0.0037\n","Epoch 23/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19387.1055 - accuracy: 0.0037\n","Epoch 24/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19305.4727 - accuracy: 0.0037\n","Epoch 25/30\n","7/7 [==============================] - 0s 20ms/step - loss: 19218.7969 - accuracy: 0.0037\n","Epoch 26/30\n","7/7 [==============================] - 0s 19ms/step - loss: 19144.8027 - accuracy: 0.0037\n","Epoch 27/30\n","7/7 [==============================] - 0s 18ms/step - loss: 19126.2617 - accuracy: 0.0037\n","Epoch 28/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18984.6348 - accuracy: 0.0037\n","Epoch 29/30\n","7/7 [==============================] - 0s 21ms/step - loss: 18919.9941 - accuracy: 0.0037\n","Epoch 30/30\n","7/7 [==============================] - 0s 19ms/step - loss: 18818.7871 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 16627.1992 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 8s 22ms/step - loss: 23621.2051 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23584.3164 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23482.6016 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23136.2207 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 21ms/step - loss: 22270.4102 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 21ms/step - loss: 21492.1152 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 22ms/step - loss: 21166.7070 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20989.4434 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20885.5742 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20781.6641 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20629.7266 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20503.6836 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20407.0156 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20305.5664 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20245.1582 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20136.1504 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20020.2559 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19926.1582 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19846.8926 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19763.8945 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 23ms/step - loss: 19655.7812 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19593.5938 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19452.2480 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19375.9551 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19291.2129 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19179.2754 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 23ms/step - loss: 19140.5195 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18986.0234 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18939.7383 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18882.0254 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18778.0137 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18741.6699 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18641.5430 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18565.9141 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18482.6777 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18345.2578 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18277.3594 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18216.5684 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18130.0859 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18073.1582 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18023.7480 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17919.0156 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17711.6074 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17715.0137 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17661.9668 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17581.8594 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 23ms/step - loss: 17530.2383 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17490.9824 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17432.8477 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17295.3223 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 19535.3164 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 7s 19ms/step - loss: 23618.1094 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23570.3945 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23422.5156 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 21ms/step - loss: 22919.8691 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 20ms/step - loss: 21944.3848 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 21ms/step - loss: 21276.6035 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 22ms/step - loss: 21031.3359 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 23ms/step - loss: 20841.8418 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20713.9590 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20590.3203 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 23ms/step - loss: 20494.1289 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20350.5195 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20241.4941 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20156.6484 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20019.9570 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19973.6562 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19860.1719 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19778.0723 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19692.6797 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19578.5195 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19461.9355 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19377.7285 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19346.7227 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19227.3691 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19174.3008 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19024.2754 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18950.3516 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18866.2070 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18813.0234 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18705.0645 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18669.2383 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18542.0684 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18438.9668 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18357.8965 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18255.2031 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18243.4277 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18176.1641 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18018.0059 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17978.5742 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17895.3105 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17804.4727 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 23ms/step - loss: 17774.1973 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17629.0039 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17559.2441 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17584.0234 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17390.4395 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17373.5938 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 23ms/step - loss: 17304.5039 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17207.6133 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17131.2090 - accuracy: 0.0037\n","4/4 [==============================] - 3s 9ms/step - loss: 17255.3809 - accuracy: 0.0075\n","Epoch 1/50\n","7/7 [==============================] - 7s 20ms/step - loss: 23795.3535 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23743.7422 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23594.4219 - accuracy: 0.0075\n","Epoch 4/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23113.2109 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 20ms/step - loss: 22140.9531 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 22ms/step - loss: 21478.4180 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 20ms/step - loss: 21216.0703 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 23ms/step - loss: 21029.7344 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20912.8965 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 23ms/step - loss: 20778.6934 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20669.6680 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20554.2070 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20463.7617 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20316.7891 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20260.6602 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 23ms/step - loss: 20129.5078 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20011.6758 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19952.6562 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19850.1328 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19731.0820 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19642.0957 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19550.0605 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19491.5625 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19407.2012 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19300.0254 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19241.0215 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19079.5566 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19062.7090 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18987.9102 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18849.1191 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18776.7715 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18741.1191 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18621.1211 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18579.1641 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18431.8008 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18379.6465 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18323.1504 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18240.3496 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18153.4883 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18085.8730 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18007.1445 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17855.5117 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17844.2129 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17732.4668 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17676.1914 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17600.4062 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17579.9961 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17460.7227 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17371.7031 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17279.9395 - accuracy: 0.0037\n","4/4 [==============================] - 2s 8ms/step - loss: 15229.9707 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 8s 20ms/step - loss: 23629.6230 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.6953 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.6465 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5488 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.5352 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.4863 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4375 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4004 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.4688 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.3496 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.2949 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.2422 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2129 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2617 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1680 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1660 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1445 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23629.1133 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.0684 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.0000 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.9688 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.9238 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.8594 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.8379 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.7949 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.7305 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.6367 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.7188 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.6328 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.5508 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.5039 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.5742 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 23ms/step - loss: 23628.4160 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.4316 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.3359 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.3516 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.2578 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.1797 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.1738 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.2031 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.1504 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23628.0820 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.9863 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.0254 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.9219 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.9766 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.8574 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23627.7949 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23627.7559 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.7207 - accuracy: 0.0037\n","4/4 [==============================] - 2s 11ms/step - loss: 26479.0215 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 7s 20ms/step - loss: 23629.9531 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.8867 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.8613 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.7891 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.7500 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.7520 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.6777 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.7031 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.6133 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.5723 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.5488 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.5020 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.4531 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.3945 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.3965 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.3477 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.3164 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.2930 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.2344 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23629.1973 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1543 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.1797 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23629.0762 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23629.0137 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.9766 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.9590 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.8574 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.8691 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.7910 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.7969 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.6934 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.6582 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.5684 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.5586 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23628.5215 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.4961 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.4258 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.4180 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.3320 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.3379 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.2266 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.2207 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23628.1641 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.0742 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23628.0254 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23627.9941 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23627.9434 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23627.9199 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23627.8672 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23627.8418 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 23736.6895 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 8s 22ms/step - loss: 23807.2695 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.2520 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 23ms/step - loss: 23807.1953 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23807.1035 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23807.1152 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.9375 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23807.0020 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.9727 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.9199 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.8965 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.8574 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.8164 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.7051 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.7383 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23806.6152 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 25ms/step - loss: 23806.5918 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.5586 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23806.5645 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23806.5820 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.4531 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.3887 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.3574 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.3633 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.2949 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.1953 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.1895 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.1387 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23806.0918 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23806.0664 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23806.0078 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.9785 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.8809 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 19ms/step - loss: 23805.8281 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.8223 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.7012 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.7539 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23805.6465 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 22ms/step - loss: 23805.6035 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.6387 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.5332 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.4336 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.4414 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.3984 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.3516 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 20ms/step - loss: 23805.2520 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.2461 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.2090 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.2070 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23805.0586 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 21ms/step - loss: 23804.9941 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 21308.2402 - accuracy: 0.0075\n","Epoch 1/50\n","7/7 [==============================] - 7s 20ms/step - loss: 23567.1816 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 21ms/step - loss: 22580.4434 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 20ms/step - loss: 21445.0645 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 20ms/step - loss: 21200.3125 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 19ms/step - loss: 21036.6914 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20941.2402 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20822.4766 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20741.1035 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20648.6074 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 19ms/step - loss: 20558.8789 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20456.8867 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20368.8320 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 19ms/step - loss: 20278.4961 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20200.7266 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20120.5117 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20045.7207 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19988.5840 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19869.9375 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19765.4863 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19739.4336 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19609.5449 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19536.6543 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19458.3516 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19369.0410 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19286.4805 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19204.6465 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19123.6035 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 19ms/step - loss: 19003.0449 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 19ms/step - loss: 18996.4316 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18854.0469 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18825.0840 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18697.4453 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18649.7109 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18567.7949 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18477.5566 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18405.6680 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18332.5703 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18299.3379 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18178.8984 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18082.0234 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17978.5391 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17943.0449 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17833.8066 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17805.8750 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17713.6602 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17557.4434 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17569.4414 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17489.0234 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17378.6934 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 22ms/step - loss: 17349.9355 - accuracy: 0.0037\n","4/4 [==============================] - 2s 6ms/step - loss: 19579.3516 - accuracy: 0.0000e+00\n","Epoch 1/50\n","7/7 [==============================] - 7s 20ms/step - loss: 23574.0371 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 22ms/step - loss: 22592.5488 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 21ms/step - loss: 21417.4863 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 20ms/step - loss: 21165.7402 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 21ms/step - loss: 21034.7754 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20898.1172 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20793.7520 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20711.8770 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20645.1367 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20500.6348 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20440.9219 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20322.7383 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20263.6621 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20186.3242 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 19ms/step - loss: 20077.4941 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19976.6543 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19938.8086 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19830.4473 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19747.0039 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19680.3164 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19573.4180 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19530.3203 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 24ms/step - loss: 19411.0625 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19328.0352 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19234.4062 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19172.2207 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 24ms/step - loss: 19144.0332 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 23ms/step - loss: 19026.9375 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18995.1660 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18837.9531 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18759.4805 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18699.6602 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18550.1328 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18573.2051 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18436.3262 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18368.4629 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18290.8965 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18244.1621 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18185.6621 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18074.6250 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17996.5527 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17913.9512 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17818.9883 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17743.7949 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17715.2988 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17602.2363 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17604.7910 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 19ms/step - loss: 17486.7305 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 19ms/step - loss: 17426.1152 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17325.7266 - accuracy: 0.0037\n","4/4 [==============================] - 2s 7ms/step - loss: 17410.4141 - accuracy: 0.0075\n","Epoch 1/50\n","7/7 [==============================] - 8s 20ms/step - loss: 23758.5488 - accuracy: 0.0037\n","Epoch 2/50\n","7/7 [==============================] - 0s 20ms/step - loss: 22835.1484 - accuracy: 0.0037\n","Epoch 3/50\n","7/7 [==============================] - 0s 21ms/step - loss: 21532.8418 - accuracy: 0.0037\n","Epoch 4/50\n","7/7 [==============================] - 0s 22ms/step - loss: 21213.6289 - accuracy: 0.0037\n","Epoch 5/50\n","7/7 [==============================] - 0s 22ms/step - loss: 21075.0527 - accuracy: 0.0037\n","Epoch 6/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20923.4707 - accuracy: 0.0037\n","Epoch 7/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20834.7812 - accuracy: 0.0037\n","Epoch 8/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20742.8730 - accuracy: 0.0037\n","Epoch 9/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20631.7285 - accuracy: 0.0037\n","Epoch 10/50\n","7/7 [==============================] - 0s 20ms/step - loss: 20584.3730 - accuracy: 0.0037\n","Epoch 11/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20504.3594 - accuracy: 0.0037\n","Epoch 12/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20360.4102 - accuracy: 0.0037\n","Epoch 13/50\n","7/7 [==============================] - 0s 23ms/step - loss: 20328.3633 - accuracy: 0.0037\n","Epoch 14/50\n","7/7 [==============================] - 0s 24ms/step - loss: 20235.9531 - accuracy: 0.0037\n","Epoch 15/50\n","7/7 [==============================] - 0s 21ms/step - loss: 20129.6230 - accuracy: 0.0037\n","Epoch 16/50\n","7/7 [==============================] - 0s 22ms/step - loss: 20081.8164 - accuracy: 0.0037\n","Epoch 17/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19938.2578 - accuracy: 0.0037\n","Epoch 18/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19901.3672 - accuracy: 0.0037\n","Epoch 19/50\n","7/7 [==============================] - 0s 23ms/step - loss: 19809.7695 - accuracy: 0.0037\n","Epoch 20/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19724.7539 - accuracy: 0.0037\n","Epoch 21/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19619.1582 - accuracy: 0.0037\n","Epoch 22/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19569.6035 - accuracy: 0.0037\n","Epoch 23/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19513.9082 - accuracy: 0.0037\n","Epoch 24/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19365.5098 - accuracy: 0.0037\n","Epoch 25/50\n","7/7 [==============================] - 0s 24ms/step - loss: 19289.9375 - accuracy: 0.0037\n","Epoch 26/50\n","7/7 [==============================] - 0s 21ms/step - loss: 19218.9414 - accuracy: 0.0037\n","Epoch 27/50\n","7/7 [==============================] - 0s 22ms/step - loss: 19142.7031 - accuracy: 0.0037\n","Epoch 28/50\n","7/7 [==============================] - 0s 20ms/step - loss: 19049.2754 - accuracy: 0.0037\n","Epoch 29/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18991.3340 - accuracy: 0.0037\n","Epoch 30/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18841.5254 - accuracy: 0.0037\n","Epoch 31/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18793.2891 - accuracy: 0.0037\n","Epoch 32/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18772.7812 - accuracy: 0.0037\n","Epoch 33/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18668.8730 - accuracy: 0.0037\n","Epoch 34/50\n","7/7 [==============================] - 0s 23ms/step - loss: 18570.0332 - accuracy: 0.0037\n","Epoch 35/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18482.6055 - accuracy: 0.0037\n","Epoch 36/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18444.2285 - accuracy: 0.0037\n","Epoch 37/50\n","7/7 [==============================] - 0s 22ms/step - loss: 18350.4297 - accuracy: 0.0037\n","Epoch 38/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18235.4375 - accuracy: 0.0037\n","Epoch 39/50\n","7/7 [==============================] - 0s 21ms/step - loss: 18147.3281 - accuracy: 0.0037\n","Epoch 40/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18141.6152 - accuracy: 0.0037\n","Epoch 41/50\n","7/7 [==============================] - 0s 20ms/step - loss: 18051.8828 - accuracy: 0.0037\n","Epoch 42/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17935.8145 - accuracy: 0.0037\n","Epoch 43/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17888.0781 - accuracy: 0.0037\n","Epoch 44/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17823.0098 - accuracy: 0.0037\n","Epoch 45/50\n","7/7 [==============================] - 0s 20ms/step - loss: 17723.1230 - accuracy: 0.0037\n","Epoch 46/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17633.2598 - accuracy: 0.0037\n","Epoch 47/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17521.8457 - accuracy: 0.0037\n","Epoch 48/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17490.7285 - accuracy: 0.0037\n","Epoch 49/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17464.2227 - accuracy: 0.0037\n","Epoch 50/50\n","7/7 [==============================] - 0s 21ms/step - loss: 17390.4473 - accuracy: 0.0037\n","4/4 [==============================] - 2s 10ms/step - loss: 15262.4912 - accuracy: 0.0000e+00\n","Epoch 1/8\n","21/21 [==============================] - 7s 17ms/step - loss: 53399.7539 - accuracy: 0.0025\n","Epoch 2/8\n","21/21 [==============================] - 0s 16ms/step - loss: 53399.5273 - accuracy: 0.0025\n","Epoch 3/8\n","21/21 [==============================] - 0s 18ms/step - loss: 53399.3711 - accuracy: 0.0025\n","Epoch 4/8\n","21/21 [==============================] - 0s 18ms/step - loss: 53399.2266 - accuracy: 0.0025\n","Epoch 5/8\n","21/21 [==============================] - 0s 17ms/step - loss: 53399.0156 - accuracy: 0.0025\n","Epoch 6/8\n","21/21 [==============================] - 0s 18ms/step - loss: 53398.9492 - accuracy: 0.0025\n","Epoch 7/8\n","21/21 [==============================] - 0s 17ms/step - loss: 53398.6445 - accuracy: 0.0025\n","Epoch 8/8\n","21/21 [==============================] - 0s 17ms/step - loss: 53398.5977 - accuracy: 0.0025\n"]},{"data":{"text/plain":["GridSearchCV(cv=3,\n","             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7faf95ee0690>,\n","             param_grid={'batch_size': [20, 32, 40], 'epochs': [8, 30, 50],\n","                         'optimizer': ['adam', 'Adadelta', 'rmsprop']})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["LSTM_grid_search.fit(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOt3tju2xsLR","outputId":"b0a17c82-7dac-478b-b274-9c1c0779ea56"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Results from Grid Search \n","\n"," The best score across ALL searched params:\n"," 0.0025062657271822295\n","\n"," The best parameters across ALL searched params:\n"," {'batch_size': 20, 'epochs': 8, 'optimizer': 'Adadelta'}\n"]}],"source":["print(\" Results from Grid Search \" )\n","# print(\"\\n The best estimator across ALL searched params:\\n\",grid_search.best_estimator_)\n","print(\"\\n The best score across ALL searched params:\\n\",LSTM_grid_search.best_score_)\n","print(\"\\n The best parameters across ALL searched params:\\n\",LSTM_grid_search.best_params_)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}